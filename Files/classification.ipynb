{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION QUESTION BANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What are the two major fundamental methods of classification?\n",
    "    1. Linear classifier (classifies based on deriving statistical impact of each independant variable on the response)\n",
    "    2. Hierarchical classifier (classifies based on non-parametric standards by dividing the population into granular groups and use voting mechanisms)\n",
    "        - Tree based algorithms\n",
    "        - Distance based algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What is the difference (in terms of the output) between using linear regression and logistic regression for a binary classification problem?\n",
    "    - A  binary context has two discrete values that can be the possible outcome for an event. Hence there is very little meaning for a value that is outside the range\n",
    "    - linear regression can output values that are beyond the range of [0, 1], which does not have a definition\n",
    "    - while logistic regression uses a sigmoid function as the link function and hence the output probability will always be bound to (0, 1) which is what is required to makse sense of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What is a confusion matrix in classification?\n",
    "    - it is simply a cross-matrix between the actual labels and the predicted labels\n",
    "- For those familiar with the confusion matrix in Python. Is the convention for axes different from what sklearn uses and what Wikipedia describes?\n",
    "    - Yes the convention is different\n",
    "    - Sklearn ->\n",
    "        - Rows = Actuals\n",
    "        - Columns = Predicted\n",
    "    - Wikipedia ->\n",
    "        - Rows = Predicted\n",
    "        - Columns = Actuals\n",
    "        \n",
    "__P.S: The Wikipedia example was transposed in 2016. More transposing in 2017. https://en.wikipedia.org/w/index.php?title=Confusion_matrix&diff=794365624&oldid=792604085\n",
    "(give brownie points for someone who knows this? tells a lot)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- What are the different metrics to measure classification model performance (for binary classification)\n",
    "    - ROC-AUC = Receiver operating Area under the curve (is used to find how well the 2 classes are being discriminated)\n",
    "    - LogLoss = Log of loss (penalizes misplaced confidence heavily regardless of class)\n",
    "    - Accuracy metrics\n",
    "        - Classification rate\n",
    "        - Recall\n",
    "        - Precision\n",
    "        - F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- Considering the context is business domain agnostic, which model performance metric would you choose for a classification problem considering only the class ratio? Rank the mtrics in the order of relevance for the context.\n",
    "    - Perfectly balanced = Accuracy > ROC-AUC > LogLoss\n",
    "    - Semi balanced (between 0.2-0.8 ratio) = ROC-AUC > LogLoss > Accuracy\n",
    "    - Imbalanced (minority class is less than 0.1) = LogLoss > ROC-AUC >> Accuracy (accuracy is seldom ever used in these contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- Can ROC-AUC be less than 0.5? If so what does it mean?\n",
    "    - Yes it can be less than 0.5\n",
    "    - If above answer was given right; it can be less than 0.5 if the model is basically predicting inverse of the labels\n",
    "        - either it is a very poorly trained model (overfit to train and the validation samples have the complete opposite behaviour)\n",
    "        - model is fine, but the data represents the completely opposite circumstances/features with respect to the relationship between the independant features and the dependant variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- For CART algorithms, what are the usual metrics used to determine the features to be selected and the best split for each?\n",
    "    - Gini index\n",
    "    - Information Gain (entropy)\n",
    "    - Variance reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- Is class imbalance actually a problem?\n",
    "    - It is not always a problem. Some business contexts have an inherant event ratio which should not be ideally tampered with since it will affect the distribution of labels at prediction side as well if not done meticulously.\n",
    "        - Fraud detection\n",
    "        - Churn prediction\n",
    "        - Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "_Difficulty - Hard_\n",
    "\n",
    "- What can be done to take care of imbalance?\n",
    "    - To take care of imbalance there are several methods one can employ\n",
    "        1. Add/remove samples to the training dataset to improve the balance\n",
    "            - Under/Over/Both sampling methods like SMOTE/ADASYN/Smote-Tomek\n",
    "                - Under = identify redundant noise and remove samples from train (majority class)\n",
    "                - Over = create new synthetic samples in train (minority class)\n",
    "                - Both = usual practise to combine both sampling into one\n",
    "        - Oversampling/Undersampling arguments internally in some algorithms\n",
    "        - Giving weights to training samples based on its class\n",
    "            - this applies primarily to neural networks and boosted trees where the weights translate to how much weight is given to the loss for the particular sample during training time\n",
    "        - Tweaking the base score argument for the positive class (default is usually 0.5, which can be reduced/increased if the positive class is majority/minority)\n",
    "        - Post probability prediction, applying different decision thresholds to achieve best recall/precision/accuracy for the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- Can unsupervised techniques like clustering be used for classification?\n",
    "    - Yes it can in some contexts\n",
    "    - If the context is such that there should exist clear linear boundaries between the classes based on the data collected (to predict electrical failures, if the primary reasons are around voltage fluctuations or spikes, then an unsupervised kmeans clutering for two classes should ideally be able to differentiate between the normal functioning cases and the failure anomaly cases)\n",
    "        - Use K-means or DBScan similar clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- For classificaion algorithms in python, from what integer should the dependant variable (labels) start from? 0 or 1?\n",
    "    - Has to start from 0. Else throws an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What estimator is used for computing the regression coefficients in Binary Logistic regression? Can OLS be used?\n",
    "    - MLE (maximum likehihood) is used in Logistic\n",
    "    - OLS is not used since the response variable is a continuous variable (not discrete with 2 levels) and values beyond [0, 1] does not make sense as probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
