{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "Long ago, in the distant, fragrant mists of time, there was a competition...\n",
    "It was not just any competition.\n",
    "\n",
    "It was a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samples... without overfitting.\n",
    "\n",
    "Data scientists ― including Kaggle's very own Will Cukierski ― competed by the hundreds. Legends were made. (Will took 5th place, and eventually ended up working at Kaggle!) People overfit like crazy. It was a Kaggle-y, data science-y madhouse.\n",
    "So... we're doing it again.\n",
    "\n",
    "Don't Overfit II: The Overfittening\n",
    "This is the next logical step in the evolution of weird competitions. Once again we have 20,000 rows of continuous variables, and a mere handful of training samples. Once again, we challenge you not to overfit. Do your best, model without overfitting, and add, perhaps, to your own legend.\n",
    "\n",
    "In addition to bragging rights, the winner also gets swag. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T17:44:32.968047Z",
     "start_time": "2019-02-12T17:44:32.958996Z"
    }
   },
   "source": [
    "## Data and Evaluation\n",
    "\n",
    "What am I predicting?\n",
    "You are predicting the binary target associated with each row, without overfitting to the minimal set of training examples provided.\n",
    "\n",
    "### Files\n",
    "- train.csv - the training set. 250 rows.\n",
    "- test.csv - the test set. 19,750 rows.\n",
    "- sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "### Columns\n",
    "- id- sample id\n",
    "- target- a binary target of mysterious origin.\n",
    "- 0-299- continuous variables.\n",
    "\n",
    "*Submissions are evaluated using AUCROC between the predicted target and the actual target value*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-12T18:14:42.911628Z",
     "start_time": "2019-02-12T18:14:42.843910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder files:  ['.ipynb_checkpoints', 'main.ipynb', 'sample_submission.csv', 'submission_1.csv', 'test.csv', 'train.csv'] \n",
      "\n",
      "envir variables: \n",
      "InteractiveShell\t MAX_EVALS\t np\t os\t pd\t randomseed\t sklearn\t sys\t train_test_split\t \n",
      "warnings\t \n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ignore warnings (only if you are the kind that would code when the world is burning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# some options\n",
    "MAX_EVALS=5\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# to display multiple outputs in a cell without usin print/display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# display wd files\n",
    "import os as os\n",
    "print('folder files: ', os.listdir(), '\\n')\n",
    "print('envir variables: ')\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "ytrain = train.target\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((250, 300), (19750, 300))"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.0    160\n",
       "0.0     90\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape\n",
    "ytrain.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 300), (50, 300))"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, ytrain, test_size=0.2, random_state=1, stratify=ytrain)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## pca\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=0.95)\n",
    "# pca.fit(X_train)\n",
    "# pca_train = pca.transform(X_train)\n",
    "# pca_valid = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=[0.07, 0.08, 0.09, 0.095, 0.099, 0.1, 0.11, 0.12],\n",
       "           class_weight='balanced', cv=5, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1.0, max_iter=100, multi_class='warn',\n",
       "           n_jobs=None, penalty='l1', random_state=None, refit=True,\n",
       "           scoring=None, solver='liblinear', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.095])"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "logmod = LogisticRegressionCV(cv=5, penalty='l1', class_weight='balanced', solver='liblinear', \n",
    "                              Cs=[0.07, 0.08, 0.09, 0.095, 0.099, 0.1, 0.11, 0.12])\n",
    "# logmod.fit(X=X_train, y=y_train)\n",
    "# logmod.score(X=X_test, y=y_test)\n",
    "# logmod.C_\n",
    "\n",
    "logmod.fit(X=pca_train, y=y_train)\n",
    "logmod.score(X=pca_valid, y=y_test)\n",
    "logmod.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=-1, penalty='l1', random_state=5,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=-1, penalty='l1', random_state=5,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodfull = LogisticRegression(C=0.1, class_weight='balanced', n_jobs=-1, random_state=5, solver='liblinear', penalty='l1')\n",
    "logmodfull.fit(X_train, y_train)\n",
    "logmodfull.score(X_test, y_test)\n",
    "logmodfull.fit(train, ytrain)\n",
    "pred=logmodfull.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X_train)\n",
    "XV = np.array(X_test)\n",
    "XX = np.array(train)\n",
    "XXV = np.array(test)\n",
    "\n",
    "Y = np_utils.to_categorical(y_train)\n",
    "YV = np_utils.to_categorical(y_test)\n",
    "YY = np_utils.to_categorical(ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 17s 86ms/step - loss: 0.9280 - acc: 0.4450\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5885 - acc: 0.6900\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4879 - acc: 0.7850\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4026 - acc: 0.8050\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3917 - acc: 0.8250\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3211 - acc: 0.9000\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2837 - acc: 0.9150\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2627 - acc: 0.9150\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2117 - acc: 0.9700\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.1842 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb3c030da0>"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 62.00%\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 0.7867 - acc: 0.5350\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5926 - acc: 0.7250\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5014 - acc: 0.8100\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4566 - acc: 0.8400\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4082 - acc: 0.9050\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3386 - acc: 0.9400\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3155 - acc: 0.9350\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2700 - acc: 0.9550\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2708 - acc: 0.9500\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2249 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb40847da0>"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.00%\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 19s 95ms/step - loss: 0.8146 - acc: 0.5050\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5975 - acc: 0.6900\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4764 - acc: 0.8050\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4503 - acc: 0.8150\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3786 - acc: 0.8900\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3563 - acc: 0.9250\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.3180 - acc: 0.9000\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2700 - acc: 0.9500\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2706 - acc: 0.9500\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2135 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb43863160>"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 66.00%\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 18s 91ms/step - loss: 0.8385 - acc: 0.5200\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.5776 - acc: 0.7200\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.5505 - acc: 0.7450\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4778 - acc: 0.8000\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4354 - acc: 0.8200\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.4009 - acc: 0.8850\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3343 - acc: 0.9050\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 5ms/step - loss: 0.2986 - acc: 0.9250\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2800 - acc: 0.9250\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2435 - acc: 0.9350\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb4a3554e0>"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 62.00%\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 19s 96ms/step - loss: 0.9508 - acc: 0.5000\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.6632 - acc: 0.6450\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.4896 - acc: 0.7550\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3919 - acc: 0.8300\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3807 - acc: 0.8450\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3190 - acc: 0.8650\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.3229 - acc: 0.8750\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2286 - acc: 0.9200\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.2293 - acc: 0.9200\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 6ms/step - loss: 0.1806 - acc: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb4d240b38>"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 60.00%\n",
      "62.80% (+/- 2.04%)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "class_weight = {0: 1., 1: 1.}\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# define 5-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(XX, ytrain):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_dim = input_dim , activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1000, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(20, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(5, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    model.fit(XX[train], ytrain[train], epochs=10, batch_size=10, verbose=1, class_weight=class_weight, shuffle=True)\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(XX[test], ytrain[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61.99999976158143,\n",
       " 64.00000095367432,\n",
       " 66.00000011920929,\n",
       " 61.99999976158143,\n",
       " 59.99999976158142]"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.80% (+/- 2.04%)\n"
     ]
    }
   ],
   "source": [
    "cvscores\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "200/200 [==============================] - 10s 51ms/step - loss: 0.8179 - acc: 0.5250\n",
      "Epoch 2/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.5550 - acc: 0.6950\n",
      "Epoch 3/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4765 - acc: 0.8250\n",
      "Epoch 4/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.4282 - acc: 0.8350\n",
      "Epoch 5/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3696 - acc: 0.9200\n",
      "Epoch 6/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3418 - acc: 0.9150\n",
      "Epoch 7/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.3203 - acc: 0.9050\n",
      "Epoch 8/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2930 - acc: 0.9600\n",
      "Epoch 9/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2719 - acc: 0.9600\n",
      "Epoch 10/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2523 - acc: 0.9750\n",
      "Epoch 11/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2539 - acc: 0.9500\n",
      "Epoch 12/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2186 - acc: 0.9700\n",
      "Epoch 13/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2054 - acc: 0.9800\n",
      "Epoch 14/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.2154 - acc: 0.9800\n",
      "Epoch 15/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1664 - acc: 0.9950\n",
      "Epoch 16/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1796 - acc: 0.9750\n",
      "Epoch 17/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1675 - acc: 0.9800\n",
      "Epoch 18/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1670 - acc: 0.9850\n",
      "Epoch 19/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1548 - acc: 0.9850\n",
      "Epoch 20/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1405 - acc: 0.9950\n",
      "Epoch 21/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1498 - acc: 0.9900\n",
      "Epoch 22/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1219 - acc: 1.0000\n",
      "Epoch 23/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1141 - acc: 0.9950\n",
      "Epoch 24/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1303 - acc: 0.9850\n",
      "Epoch 25/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1124 - acc: 1.0000\n",
      "Epoch 26/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1158 - acc: 1.0000\n",
      "Epoch 27/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1036 - acc: 1.0000\n",
      "Epoch 28/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1135 - acc: 0.9850\n",
      "Epoch 29/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0890 - acc: 1.0000\n",
      "Epoch 30/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.1123 - acc: 0.9800\n",
      "Epoch 31/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0789 - acc: 1.0000\n",
      "Epoch 32/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0726 - acc: 1.0000\n",
      "Epoch 33/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0738 - acc: 1.0000\n",
      "Epoch 34/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0808 - acc: 0.9800\n",
      "Epoch 35/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0894 - acc: 0.9850\n",
      "Epoch 36/100\n",
      "200/200 [==============================] - 0s 998us/step - loss: 0.0696 - acc: 0.9950\n",
      "Epoch 37/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0621 - acc: 1.0000\n",
      "Epoch 38/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0578 - acc: 1.0000\n",
      "Epoch 39/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0620 - acc: 0.9950\n",
      "Epoch 40/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0553 - acc: 1.0000\n",
      "Epoch 41/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0811 - acc: 0.9850\n",
      "Epoch 42/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0498 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0527 - acc: 0.9950\n",
      "Epoch 44/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0494 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0655 - acc: 0.9950\n",
      "Epoch 46/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0421 - acc: 1.0000\n",
      "Epoch 47/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0535 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0445 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0411 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0553 - acc: 0.9850\n",
      "Epoch 52/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0414 - acc: 0.9950\n",
      "Epoch 53/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0452 - acc: 0.9950\n",
      "Epoch 54/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0348 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0530 - acc: 0.9950\n",
      "Epoch 56/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0437 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0398 - acc: 0.9950\n",
      "Epoch 58/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0275 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0303 - acc: 0.9950\n",
      "Epoch 62/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0431 - acc: 0.9900\n",
      "Epoch 63/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0272 - acc: 0.9950\n",
      "Epoch 64/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0478 - acc: 0.9900\n",
      "Epoch 65/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0291 - acc: 0.9950\n",
      "Epoch 66/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0423 - acc: 0.9950\n",
      "Epoch 72/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0218 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0354 - acc: 0.9900\n",
      "Epoch 80/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0308 - acc: 0.9950\n",
      "Epoch 81/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0149 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0213 - acc: 0.9950\n",
      "Epoch 86/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0701 - acc: 0.9750\n",
      "Epoch 88/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0334 - acc: 0.9850\n",
      "Epoch 89/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0192 - acc: 0.9950\n",
      "Epoch 91/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0203 - acc: 0.9950\n",
      "Epoch 93/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0461 - acc: 0.9850\n",
      "Epoch 94/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0249 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0347 - acc: 0.9850\n",
      "Epoch 98/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0121 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0156 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "200/200 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1eb10cd7e10>"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 4s 76ms/step\n",
      "\n",
      "acc: 64.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6840277777777777"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict_proba(XV)\n",
    "sklearn.metrics.roc_auc_score(y_true=y_test, y_score=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random class\n",
    "\n",
    "class rand_mod():\n",
    "    \n",
    "    def __init__(self, train, ytrain, test, iter=10):\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.ytrain = ytrain\n",
    "        self.iter = iter\n",
    "        \n",
    "        self.main()\n",
    "        \n",
    "    def main(self):\n",
    "        \n",
    "        self.split(self.train, self.ytrain)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def split(self):\n",
    "        grid = {}\n",
    "        mod_grid = {}\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            x_train, x_test, y_train, y_test = train_test_split(self.train, self.ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model imports\n",
    "\n",
    "from sklearn.linear_model import RandomizedLogisticRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedLogisticRegression(C=1, fit_intercept=True, memory=None, n_jobs=-1,\n",
       "               n_resampling=500, normalize=True, pre_dispatch='3*n_jobs',\n",
       "               random_state=1, sample_fraction=0.8, scaling=0.5,\n",
       "               selection_threshold=0.01, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitmod = RandomizedLogisticRegression(n_jobs=-1, random_state=1, selection_threshold=0.01, \n",
    "                                        sample_fraction=0.8, n_resampling=500)\n",
    "logitmod.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   4,  16,  26,  33,  39,  43,  52,  53,  63,  65,  73,  80,\n",
       "        82,  89,  90,  91, 105, 108, 117, 119, 127, 129, 150, 151, 156,\n",
       "       164, 165, 168, 170, 176, 180, 189, 201, 209, 217, 220, 221, 228,\n",
       "       230, 237, 239, 240, 253, 272, 285, 295], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_cols = logitmod.get_support(indices=True)\n",
    "filtered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_train = X_train.ix[:, filtered_cols]\n",
    "XX_test = X_test.ix[:, filtered_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.5, eval_metric='auc', gamma=0,\n",
       "       learning_rate=0.001, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=5,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6507936507936508"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6565217391304348"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = xgb.XGBClassifier(learning_rate=0.001, n_estimators=1000, colsample_bytree=0.5, max_depth=5, subsample=0.9,\n",
    "                       eval_metric='auc', random_state=5)\n",
    "mod.fit(XX_train, y_train)\n",
    "mod.score(XX_test, y_test)\n",
    "pred=mod.predict_proba(XX_test)\n",
    "sklearn.metrics.roc_auc_score(y_true=y_test, y_score=pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.2, eval_metric='auc', gamma=0,\n",
       "       learning_rate=0.001, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=5,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.8)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6349206349206349"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7380434782608696"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = xgb.XGBClassifier(learning_rate=0.001, n_estimators=1000, colsample_bytree=0.2, max_depth=10, subsample=0.8,\n",
    "                       eval_metric='auc', random_state=5)\n",
    "mod.fit(pca_train, y_train)\n",
    "mod.score(pca_valid, y_test)\n",
    "pred=mod.predict_proba(pca_valid)\n",
    "sklearn.metrics.roc_auc_score(y_true=y_test, y_score=pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, eval_metric='auc', gamma=0, learning_rate=0.01,\n",
       "       max_delta_step=0, max_depth=2, min_child_weight=1, missing=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=5, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n",
       "       subsample=0.07)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "mod = xgb.XGBClassifier(learning_rate=0.01, n_estimators=1000, colsample_bytree=1, max_depth=2, subsample=0.07,\n",
    "                       eval_metric='auc', random_state=5)\n",
    "mod.fit(train, ytrain)\n",
    "pred=mod.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = pred[:,1]\n",
    "submission.to_csv('submission_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
