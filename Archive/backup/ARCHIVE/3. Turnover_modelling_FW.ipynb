{"cells":[{"cell_type":"markdown","source":["# Turnover_modelling_FW.ipynb\n> Project: **ABI Turnover**  \n> Turnover Process Phase: **3**  \n> Author: **Varun V**  \n> Location: **GCC**  \n> Team: **People Analytics**"],"metadata":{}},{"cell_type":"code","source":["## importing the relevant packages:\n\n# clear the workspace\n#%reset -f\n\n# print list of files in directory\nimport os\nprint(os.listdir())\n\n# the base packages\nimport collections # for the Counter function\nimport csv # for reading/writing csv files\nimport pandas as pd, numpy as np, time, gc, bisect, re\nfrom datetime import datetime as dt\n\n# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\nimport sklearn\nfrom sklearn import metrics, preprocessing\nimport sklearn.decomposition as decomposition\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.base import TransformerMixin\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import VarianceThreshold\nimport category_encoders as ce\nfrom scipy.stats import truncnorm\n\n# the modelling packages and related\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# hyperopt modules\n#from bayes_opt import BayesianOptimization\nfrom tqdm import tqdm\nfrom hyperopt import hp, tpe, STATUS_OK, Trials, space_eval, rand\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\n\nMAX_EVALS = 5\nrandomseed = 5 # the value for the random state used at various points in the pipeline\npd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\npd.options.display.max_columns = 200\n\n# to display multiple outputs in a cell without usin print/display\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;conf&#39;, &#39;ganglia&#39;, &#39;derby.log&#39;, &#39;logs&#39;, &#39;eventlogs&#39;]\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["## HELPER FUNCTIONS CLASS ##\n\nclass helper_funcs():\n    \n    def __init__():\n        \"\"\" helper functions used across the pipeline \"\"\"\n    \n    ## find and append multiple dataframes of the type specified in string\n    def append_datasets(cols_to_remove, string = ['TRAIN', 'VALID']):\n        files = dbutils.fs.ls('/mnt/datalake/OUTPUT/')\n        file_names = {}\n        \n        iter = 0\n        for i in files:\n          file_names[i.name] = i.path\n          iter = iter+1\n        file_names_keys = list(file_names.keys())\n        \n        # pass either train or valid as str argument\n        temp_files = [name for name in file_names_keys if name.startswith(string)]\n        temp_dict = {}\n        for i in temp_files:\n            df_name = re.sub(string=i, pattern='.csv', repl='')\n            print(df_name)\n            temp_dict[df_name] = pd.read_csv(('/dbfs/mnt/datalake/OUTPUT/') + i, na_values=['No Data', ' ', 'UNKNOWN', '', np.nan, np.inf])\n            temp_dict[df_name].columns = map(str.lower, temp_dict[df_name].columns)\n            temp_dict[df_name].drop(cols_to_remove, axis=1, inplace=True)\n            chars_to_remove = [' ', '.', '(', ')', '__', '-']\n            for i in chars_to_remove:\n                temp_dict[df_name].columns = temp_dict[df_name].columns.str.strip().str.lower().str.replace(i, '_')\n        temp_list = [v for k,v in temp_dict.items()]\n        temp = pd.concat(temp_list, axis=0, sort=True, ignore_index=True)\n        return temp\n    \n    ## datetime feature engineering\n    def datetime_feats(train, valid):\n        cols = [s for s in train.columns.values if 'date' in s]\n        print('datetime feature engineering is happening ...', '\\n')\n        # nested function to derive the various datetime features for a given date column\n        def dt_feats(df, col):\n            df[col] = pd.to_datetime(df[i])\n            #df[str(col+'_'+'day')] = df[col].dt.day\n            #df[str(col+'_'+'day_name')] = df[col].dt.day_name\n            #df[str(col+'_'+'dayofweek')] = df[col].dt.dayofweek\n            df[str(col+'_'+'dayofyear')] = df[col].dt.dayofyear\n            #df[str(col+'_'+'days_in_month')] = df[col].dt.days_in_month\n            df[str(col+'_'+'month')] = df[col].dt.month\n            #df[str(col+'_'+'month_name')] = df[col].dt.month_name\n            df[str(col+'_'+'quarter')] = df[col].dt.quarter\n            #df[str(col+'_'+'week')] = df[col].dt.week\n            #df[str(col+'_'+'weekday')] = df[col].dt.weekday\n            df[str(col+'_'+'year')] = df[col].dt.year\n            #df[col] = df[col].dt.date\n            df = df.drop([col], axis = 1)\n            return df\n        # loop function over all raw date columns\n        for i in cols:\n            train = dt_feats(train, i)\n            valid = dt_feats(valid, i)\n        return train, valid\n    \n    ## function to get frequency count of elements in a vector/list\n    def freq_count(input_vector):\n        return collections.Counter(input_vector)\n    \n    # removing near zero variance columns\n    def variance_threshold_selector(train, valid, threshold):\n        print('input data shape is: ', train.shape, '\\n')\n        selector = VarianceThreshold(threshold)\n        selector.fit(np.asanyarray(train))\n        X = train[train.columns[selector.get_support(indices=True)]]\n        Y = valid[valid.columns[selector.get_support(indices=True)]]\n        #display(pd.DataFrame(X.head(5)))\n        print('output data shape is: ', X.shape, '\\n')\n        return X, Y"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["## MISSING VALUE IMPUTATION CLASS ##\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        \"\"\"Impute missing values.\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n        Columns of other types are imputed with mean of column.\n        \"\"\"\n        \n    def fit(self, X, y=None):\n        X_temp = X.copy()\n        #self.fill = pd.Series([X_temp.groupby(['pay_grade_group_pa', 'global_job_om'])[c].value_counts().index[0] if X_temp[c].dtype == np.dtype('O') else X_temp.groupby(['pay_grade_group_pa', 'global_job_om'])[c].mean() for c in X_temp], index=X_temp.columns)\n        #X_temp = self.fill.copy().reset_index()\n        #self.fill = pd.Series([X_temp.groupby(['pay_grade_group_pa'])[c].value_counts().index[0] if X_temp[c].dtype == np.dtype('O') else X_temp.groupby(['pay_grade_group_pa'])[c].mean() for c in X_temp], index=X_temp.columns)\n        #X_temp = self.fill.copy().reset_index()\n        self.fill = pd.Series([X_temp[c].value_counts().index[0] if X_temp[c].dtype == np.dtype('O') else X_temp[c].mean() for c in X_temp], \n                              index=X_temp.columns)\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n    \n    def num_missing(self):\n        return sum(self.isnull())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["## CATEGORICAL ENCODERS CLASS ##\n\nclass categ_encoders(object):\n    def ce_encodings(self, train_df, valid_df, encoding):\n        print(str(encoding) + ' encoding is happening ...', '\\n')\n        if encoding=='bne':    \n            enc=ce.BaseNEncoder(base=4)\n        elif encoding=='be':\n            enc=ce.BinaryEncoder()\n        elif encoding=='he':\n            enc=ce.HashingEncoder(drop_invariant=True)\n        elif encoding=='oe':\n            enc=ce.OrdinalEncoder()\n        elif encoding=='ohe':\n            enc=ce.BaseNEncoder(base=1)\n        enc.fit(train_df)\n        train_enc=enc.transform(train_df)\n        valid_enc=enc.transform(valid_df)\n        print('category encoding completed', '\\n')\n        self.enc=enc\n        return train_enc, valid_enc"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["class main(object):\n    \n    def __init__(self):\n        \"\"\" random \"\"\"\n    \n    def prepare(self, cols_to_remove, response='label', id_col='employee_personnal_number_pa'):\n        # read in the train and validation datasets\n        # clean column names and remove unwanted columns\n        # append the (multiple?) train datasets into a single one (simple appending for now)\n        \n        print('1. Appending the multiple train/valid datasets in the working directory \\n')\n        train = helper_funcs.append_datasets(string='TRAIN_201801_1_', cols_to_remove=cols_to_remove)\n        valid = helper_funcs.append_datasets(string='VALID_201802_201808', cols_to_remove=cols_to_remove)\n        main.removed_cols = cols_to_remove ## attribute\n        \n        # reorder columns so that they are in the same order (impacts nothing but to be foolproof)\n        valid = valid[train.columns]\n        \n        # creating the datetime features from date columns (works only for cols with date in header, modify for other cases)\n        print('2. Datetime features are being created for the columns (which have \"date\" in their column name) \\n')\n        train, valid = helper_funcs.datetime_feats(train, valid)\n\n        # missing value threshold control (for both rows and columns)\n        mt = 0.6\n        print(train.shape, '\\n')\n        train.dropna(thresh=mt*(train.shape[0]), axis=1, inplace = True)\n        train.dropna(thresh=mt*(train.shape[1]), axis=0, inplace = True)\n        print(train.shape, '\\n')\n        valid = valid[train.columns]\n        valid.dropna(thresh=mt*(valid.shape[0]), axis=1, inplace = True)\n        train = train[valid.columns]\n        main.missing_threshold = mt ## attribute\n\n        # reset the index since inplace operations happened earlier\n        train.index = pd.RangeIndex(len(train.index))\n        valid.index = pd.RangeIndex(len(valid.index))\n        # save the global ids for mapping later (forward looking)\n        valid_ids = valid[[id_col, response]]\n        main.validation_labels = valid_ids ## attribute\n        valid_ids.to_csv('test_dfs.csv', index=False)\n        valid.drop(id_col, axis=1, inplace=True)\n        train.drop(id_col, axis=1, inplace=True)\n        train = pd.DataFrame(train)\n        valid = pd.DataFrame(valid)\n        # the class balance in the training dataset for the response\n        print(helper_funcs.freq_count(train[response]), '\\n')\n        # creating the response vector\n        y_train = train[response].values\n        y_valid = valid[response].values\n\n        # drop the response\n        train = train.drop([response], axis = 1)\n        valid = valid.drop([response], axis = 1)\n\n        #######################################################################################################\n        ## MISSING VALUE IMPUTATION ##\n        #######################################################################################################\n        # store all feature names\n        feat_names = train.columns.values\n        feat_names2 = valid.columns.values\n        \n        miss_enc = DataFrameImputer()\n        miss_enc.fit(X=train)\n        train_new = miss_enc.transform(train)\n        valid_new = miss_enc.transform(valid)\n        \n        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n        train = pd.DataFrame(data=train_new, columns=feat_names)\n        valid = pd.DataFrame(data=valid_new, columns=feat_names2)\n        print('missing value treatment completed ...', '\\n')\n        #######################################################################################################\n        \n        #######################################################################################################\n        ## ENCODING ##\n        #######################################################################################################\n        cat_columns = train.select_dtypes(include=['object']).columns.values\n        print(cat_columns)\n        train_cat = train[cat_columns]\n        num_cols = list(set(train.columns) - set(train_cat.columns))\n        train_num = train[num_cols]\n        valid_cat = valid[cat_columns]\n        valid_num = valid[num_cols]\n        \n        ce_ins = categ_encoders()\n        train_cat, valid_cat = ce_ins.ce_encodings(train_cat, valid_cat, encoding='oe')\n        self.enc = ce_ins\n\n        train = pd.concat([train_cat.reset_index(drop=True), train_num], axis=1)\n        valid = pd.concat([valid_cat.reset_index(drop=True), valid_num], axis=1)\n        \n        train = train.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n        valid = valid.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n        print('encoding completed ...', '\\n')\n        #need to fix below part (store the categorical classes for remapping during interpretation)\n        #main.categorical_dict = categorical_names ## attribute\n        #######################################################################################################\n\n        #######################################################################################################\n        ## VARIANCE THRESHOLD FEATURE SELECTION ##\n        #######################################################################################################\n        train, valid = helper_funcs.variance_threshold_selector(train=train, valid=valid, threshold=0.1)\n        #######################################################################################################\n    \n        #######################################################################################################\n        ## CORRELATION ANALYSIS ##\n        #######################################################################################################\n        # remove highly correlated features to reduce further computation time\n        print('correlation analysis is happening ...', '\\n')\n        # Create correlation matrix\n        corr_matrix = train.corr().abs()\n        # Select upper triangle of correlation matrix\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n        # Find index of feature columns with correlation greater than 0.9\n        to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]        \n        # Drop features\n        #print(to_drop, '\\n')\n        train.drop(to_drop, axis=1, inplace=True)\n        valid.drop(to_drop, axis=1, inplace=True)\n        print('correlation analysis completed ...', '\\n')\n        main.cor_dropped_vars = to_drop ## attribute\n        #######################################################################################################\n\n        return train, valid, y_train, y_valid"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["main_object = main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["train, valid, y_train, y_valid = main_object.prepare(cols_to_remove=['ohd', 'manager_s_ohd', 'employee_age', 'manager_s_age', 'yearmonth'], id_col='employee_personnal_number_pa')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">1. Appending the multiple train/valid datasets in the working directory \n\nTRAIN_201801_1_2\n/databricks/python/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2963: DtypeWarning: Columns (19,22) have mixed types. Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\nTRAIN_201801_1_3\nTRAIN_201801_1_4\nTRAIN_201801_1_5\nTRAIN_201801_1_6\nVALID_201802_201808\n2. Datetime features are being created for the columns (which have &#34;date&#34; in their column name) \n\ndatetime feature engineering is happening ... \n\n(48700, 105) \n\n(48287, 84) \n\nCounter({0: 34045, 1: 14242}) \n\nmissing value treatment completed ... \n\n[&#39;ab_inbev_entity_level_2_om&#39; &#39;company_code_om&#39; &#39;cost_center_id_om&#39;\n &#39;employee_subgroup_code_om&#39; &#39;functional_area_om&#39; &#39;global_job_om&#39;\n &#39;job_family_om&#39; &#39;macro_entity_level_3_om&#39; &#39;macro_entity_level_4_om&#39;\n &#39;manager_opr_bucket&#39; &#39;manager_opr_rating_scale&#39;\n &#39;manager_s_pay_grade_group_pa&#39; &#39;manager_s_position_band_om&#39;\n &#39;manager_s_position_name_om&#39; &#39;manager_s_position_name_om_l2&#39;\n &#39;manager_s_position_work_location_code_om&#39; &#39;opr_bucket&#39;\n &#39;opr_rating_scale&#39; &#39;pay_grade_group_pa&#39; &#39;physical_work_location_city_pa&#39;\n &#39;physical_work_location_code_pa&#39; &#39;physical_work_location_description_pa&#39;\n &#39;position_band_om&#39; &#39;position_name_om&#39; &#39;position_work_location_code_om&#39;\n &#39;position_work_location_country_om&#39;]\noe encoding is happening ... \n\ncategory encoding completed \n\nencoding completed ... \n\ninput data shape is:  (48287, 82) \n\noutput data shape is:  (48287, 68) \n\ncorrelation analysis is happening ... \n\ncorrelation analysis completed ... \n\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["# RF and GBC from sklearn"],"metadata":{}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n#model = RandomForestClassifier(n_estimators=300, n_jobs=-1, max_depth=10)\nmodel = GradientBoostingClassifier(max_depth=10, n_estimators=100, learning_rate=0.1, subsample=0.8)\nmodel.fit(train, y_train)\npred = model.predict_proba(valid)\n\npredict=np.where(pred[:,1]>0.1,1, 0)\nroc_auc_score(y_score=pred[:,1], y_true=y_valid)\nrecall_score(y_pred=predict, y_true=y_valid)\naccuracy_score(y_pred=predict, y_true=y_valid)\nprecision_score(y_pred=predict, y_true=y_valid)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[17]: \nGradientBoostingClassifier(criterion=&#39;friedman_mse&#39;, init=None,\n              learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=10,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=1, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=100,\n              n_iter_no_change=None, presort=&#39;auto&#39;, random_state=None,\n              subsample=0.8, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)\nOut[17]: 0.6006396765618077\nOut[17]: 0.22635135135135134\nOut[17]: 0.847952086553323\nOut[17]: 0.1072\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["# LOGIT"],"metadata":{}},{"cell_type":"code","source":["## SIMPLE LOGIT MODEL CLASS ##\n\nclass logit(object):\n  \n  def __init__(self):\n    \"\"\" class for implementing logistic regression \"\"\"\n    return None\n  \n  def logit_cv(self, train, y_train, scorer = 'roc_auc'):\n    modelCV = LogisticRegression(solver='liblinear')\n    # Create regularization penalty and intercept hyperparameter space\n    penalty = ['l1', 'l2']\n    C = np.logspace(0, 5, 5)\n    hyperparameters = dict(C=C, penalty=penalty)\n    \n    # Create grid search using 5-fold cross validation\n    clf = GridSearchCV(modelCV, hyperparameters, cv=5, verbose=1)\n    # Fit grid search\n    model_fit = clf.fit(train, y_train)\n\n    # View best hyperparameters\n    print('Best Penalty:', model_fit.best_estimator_.get_params()['penalty'])\n    print('Best C:', model_fit.best_estimator_.get_params()['C'])\n    scoring_metric = scorer # give recall/precision or f1 if needed\n    results = cross_val_score(modelCV, train, y_train, cv=5, scoring=scoring_metric)\n    print(\"5-fold cross validation average accuracy: %.3f\" % (results.mean()))\n    \n    self.penalty = model_fit.best_estimator_.get_params()['penalty']\n    self.C = model_fit.best_estimator_.get_params()['C']\n    self.score = results.mean()\n    \n    return None\n  \n  def main(self, train, y_train, valid, y_valid, thresh = 0, scorer = 'roc_auc'):\n    self.logit_cv(train, y_train, scorer)\n    model = LogisticRegression(penalty = self.penalty, C = self.C, solver = 'liblinear')\n    model.fit(train, y_train)\n    \n    self.model = model\n    self.valid = valid\n    self.y_valid = y_valid\n    self.predict(thresh)\n    return None\n  \n  def predict(self, thresh):\n    model = self.model\n    valid = self.valid\n    y_valid = self.y_valid\n    \n    pred = model.predict_proba(valid)\n    pred = pred[:, 1]\n    self.pred = pred\n    \n    self.thresh_predict(thresh)\n    thresh = self.thresh\n\n    predict = np.where(pred > thresh, 1, 0)\n    self.predict = predict\n    # print the various evaluation metrics\n    print('auc: ', roc_auc_score(y_score=pred, y_true=y_valid))\n    print('recall: ', recall_score(y_pred=predict, y_true=y_valid))\n    print('precision: ', precision_score(y_pred=predict, y_true=y_valid))\n    print('f1: ', f1_score(y_pred=predict, y_true=y_valid))\n    print('accuracy score: ', accuracy_score(y_pred=predict, y_true=y_valid))\n    \n    return None\n  \n  def get_truncated_normal(self, mean=0, sd=1, low=0, upp=10):\n    return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n\n  def best_thresh_score(self, yp, yt):\n    rc = recall_score(y_pred=yp, y_true=yt)\n    ac = accuracy_score(y_pred=yp, y_true=yt)\n    rc_ac_flag = 0\n    if (rc>0.6 and ac>0.6) : rc_ac_flag = 1\n    score = (rc_ac_flag)*(0.6*rc + 0.4*ac)\n    return score\n\n  def opt_thresh(self, pred):\n    X = self.get_truncated_normal(mean=0.1, sd=0.2, low=0, upp=0.4)\n    Y = list(X.rvs(1000))\n\n    cols = ['thresh', 'recall', 'precision', 'f1', 'acc', 'score'] #score = (0.6*recall + 0.4*acc)\n    thresh_grid = []\n    for i in Y:\n        predict=np.where(pred > i, 1, 0)\n        thresh_grid.append([i, recall_score(y_pred=predict, y_true=y_valid),\n                            precision_score(y_pred=predict, y_true=y_valid),\n                            f1_score(y_pred=predict, y_true=y_valid),\n                            accuracy_score(y_pred=predict, y_true=y_valid),\n                           self.best_thresh_score(yp=predict, yt=y_valid)])\n    thresh_grid = pd.DataFrame(thresh_grid, columns=cols)\n    thresh_grid.sort_values(by='score', ascending=False, inplace=True)\n    thresh = thresh_grid.reset_index(drop=True).iloc[0][0]\n    return thresh\n\n  def thresh_predict(self, thresh):\n    pred = self.pred\n    if (thresh==0):\n        thresh = self.opt_thresh(pred)\n    self.thresh = thresh\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["log_results = logit()\nlog_results.main(train, y_train, valid, y_valid)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fitting 5 folds for each of 10 candidates, totalling 50 fits\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   20.5s finished\nBest Penalty: l1\nBest C: 316.22776601683796\n5-fold cross validation average accuracy: 0.595\nauc:  0.6338218597696056\nrecall:  0.9797297297297297\nprecision:  0.06115563053563897\nf1:  0.11512504962286621\naccuracy score:  0.13871715610510046\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["#RANDOM FOREST"],"metadata":{}},{"cell_type":"code","source":["# random forest class for tuning\n\nclass rf_model:\n    \n    def __init__(self, train, y_train, valid, y_valid):\n      \"\"\" this class initializes some functions used in the random forest pipeline \"\"\"\n      self.train = train\n      self.y_train = y_train\n      self.valid = valid\n      self.y_valid = y_valid\n              \n    def rf_score(self, params):        \n        global ITERATION\n        ITERATION += 1\n\n        # Make sure parameters that need to be integers are integers\n        for parameter_name in ['max_depth', 'n_estimators']:\n            params[parameter_name] = int(params[parameter_name])\n                \n        rf_results = RandomForestClassifier(**params, random_state=randomseed)\n        #rf_results.fit(X_train, y_train)\n        rf_cv_scores = sklearn.model_selection.cross_val_predict(rf_results, self.train, self.y_train, cv=5, verbose=False)        \n        recall_score = sklearn.metrics.recall_score(y_pred=rf_cv_scores, y_true=self.y_train)\n        precision_score = sklearn.metrics.precision_score(y_pred=rf_cv_scores, y_true=self.y_train)\n        f1_score = sklearn.metrics.f1_score(y_pred=rf_cv_scores, y_true=self.y_train)\n        return {'loss': (1 - recall_score), 'status': STATUS_OK, 'params': params, 'iteration': ITERATION}\n    \n    def optimize(self):\n        # Keep track of evals\n        global ITERATION\n        ITERATION = 0\n        \n        global trials\n        trials = Trials()\n        space = {\n            'max_depth' : hp.quniform('max_depth', 5, 10, 1),\n            'max_features': hp.choice('max_features', range(2, int((self.train.shape[:][1])/5))),\n            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n            'n_estimators': hp.choice('n_estimators', np.arange(200, 1000))}\n        \n        # Run optimization\n        best = fmin(fn = self.rf_score, space = space, algo = tpe.suggest, \n            max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(randomseed))\n        best_params = space_eval(space, best)\n        #best_params = trials.best_trial['result']['params']\n        return best_params, trials\n    \n    def rf_train(self, best_params):\n        model = RandomForestClassifier(random_state = randomseed)\n        model.set_params(**best_params)\n        model.fit(self.train, self.y_train)\n        return model\n    \n    def rf_predict(self, X_test, y_test, model, mode = \"validate\"):\n        pred = model.predict_proba(self.valid)[:, 1]\n        predict = np.where(pred > 0.12, 1, 0)\n        \n        if mode == \"validate\":\n            recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=self.y_valid)\n            precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=self.y_valid)\n            f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=self.y_valid)\n            auc_score = roc_auc_score(self.y_valid, pred)\n            tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=self.y_valid).ravel()\n            print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=self.y_valid), '\\n')\n            print('recall score is: ', recall_score)\n            print('precision score is: ', precision_score)\n            print('f1_score is: ', f1_score)\n            print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=self.y_valid, y_pred=predict))\n            print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n            return pred, predict, tn, fp, fn, tp\n        else:\n            return pred\n        \n    def rf_cv(self, X_train, y_train, best):\n        model = RandomForestClassifier(**best, verbose=False)\n        rf_cv_scores = sklearn.model_selection.cross_val_predict(model, self.train, self.y_train, cv=5)\n        print('recall: ', sklearn.metrics.recall_score(y_pred=rf_cv_scores, y_true=self.y_train))\n        print('precision: ', sklearn.metrics.precision_score(y_pred=rf_cv_scores, y_true=self.y_train))\n        print('f1: ', sklearn.metrics.f1_score(y_pred=rf_cv_scores, y_true=self.y_train))\n        print('accuracy: ', sklearn.metrics.accuracy_score(y_pred=rf_cv_scores, y_true=self.y_train))\n        return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["rf = rf_model(train, y_train, valid, y_valid)\n\n# calling the randomforest function and returning the best model\nbest, trials = rf.optimize()\nprint(1 - trials.average_best_error(), '\\n')\n\nmodel = rf.rf_train(best)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.3323449468742279 \n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# cv results\nrf.rf_cv(train, y_train, best)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">recall:  0.33580429948109713\nprecision:  0.5220898962735305\nf1:  0.40872180451127815\naccuracy:  0.6362290683689518\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["# predicting using the best random forest model on the validation set\nrf_pred, rf_predict, tn, fp, fn, tp = rf.rf_predict(X_test=valid, model=model, y_test=y_valid, mode='validate')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[[ 722 4158]\n [   5  291]] \n\nrecall score is:  0.9831081081081081\nprecision score is:  0.06540795684423466\nf1_score is:  0.12265542676501578\naccuracy score:  0.19571097372488408\nThe final AUC after taking the best params and num_rounds when it stopped is 0.6499. \n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# print the various evaluation metrics\nprint('auc: ', roc_auc_score(y_score=pred[:,1], y_true=y_valid))\nprint('recall: ', recall_score(y_pred=predict, y_true=y_valid))\nprint('precision: ', precision_score(y_pred=predict, y_true=y_valid))\nprint('f1: ', f1_score(y_pred=predict, y_true=y_valid))\nprint('accuracy score: ', accuracy_score(y_pred=predict, y_true=y_valid))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">auc:  0.6496469317678334\nrecall:  0.9797297297297297\nprecision:  0.06471769694264673\nf1:  0.12141511408833995\naccuracy score:  0.18914219474497682\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["from treeinterpreter import treeinterpreter as ti\n\nprediction, bias, contributions = ti.predict(model, valid)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# explanation per instance\n\nprint(\"Prediction\", prediction[0][1])\nprint(\"Bias (trainset prior)\", bias[0][1])\nprint(\"Feature contributions:\")\nfor c, feature in zip(contributions[0][:,1], \n                             valid.columns.values):\n    print(feature, c)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Prediction 0.364349466894137\nBias (trainset prior) 0.3748449633564246\nFeature contributions:\nab_inbev_entity_level_2_om -0.0016420786640554358\ncompany_code_om 0.05482670148409118\nemployee_subgroup_code_om 0.004320453864677357\nfunctional_area_om -4.243337055178063e-05\nglobal_job_om 0.02196783516667973\njob_family_om 0.004406176706364121\nmacro_entity_level_3_om 0.00044007645032952434\nmacro_entity_level_4_om 0.006858459956108046\nmanager_s_pay_grade_group_pa -0.0003916484935577677\nmanager_s_position_name_om 0.01678510121732734\nmanager_s_position_name_om_l2 0.004056892784634718\nmanager_s_position_work_location_code_om 0.004951255561309194\nopr_bucket -0.00023034355391983834\nopr_rating_scale -0.001454755339926122\npay_grade_group_pa -0.0001338164761688994\nphysical_work_location_city_pa -0.11354908569212521\nphysical_work_location_code_pa 0.007289381758690345\nphysical_work_location_description_pa 0.0015918352129711698\nposition_band_om 0.0005549492474887864\nposition_name_om 0.04136139565589634\nposition_work_location_code_om -0.04718266142150298\nposition_work_location_country_om 0.0006491312288036904\nposition_tenure -0.02110218380565455\ntenure_diff 0.007071842192353278\npers_compgroup_year_comp_score_sum -0.0002223248807454026\nemployee_tenure 0.020757634180057778\nmanager_s_position_tenure -0.003317416590173052\nsalary -0.014862518775962083\nmanager_s_position_start_date_om_quarter 0.0\nteam_size -0.0006565815277363796\ncummean_employee_personnal_number_pa_net_target -0.0015606123621660908\npers_compgroup_year_comp_score_mean_leadership_competencies -0.0007016587814805702\npers_year_comp_score_sum -0.000605212792850344\nnet_target 0.00016497132198732838\nemployee_group_code_om 0.0004951792934363073\norgunitid_employee_isdifferentfrom_manager 5.469364072900989e-05\nbonus 0.0\nmanager_s_tenure -0.005266229237669104\nteam_l2_size -0.001173203208330295\nlocal_entity_code 0.0003722379536668493\nebm_level_of_the_job_om -0.00030073953121092237\ncummean_personnel_number_of_appraiser_net_target 0.0011700140570737182\nopr_value -0.001971268088057417\npers_compgroup_year_comp_score_sum_leadership_competencies -0.0004969722164093675\nage_diff 0.00622202941329002\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# function to bucket sparse levels in categorical features to the 'others' category as well as handle new values in the valid df\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom collections import defaultdict\n\nclass CategoryGrouper(BaseEstimator, TransformerMixin):  \n    \"\"\"A tranformer for combining low count observations for categorical features.\n    This transformer will preserve category values that are above a certain threshold, while bucketing together all the other values. This will fix issues where new data may have an unobserved category value that the training data did not have.\n    \"\"\"\n    \n    def __init__(self, threshold=0.05):\n        \"\"\" Initialize method.\n        Args: threshold (float): The threshold to apply the bucketing when categorical values drop below that threshold.\n        \"\"\"\n        self.d = defaultdict(list)\n        self.threshold = threshold\n\n    def transform(self, X, **transform_params):\n        \"\"\"Transforms X with new buckets.\n        Args: X (obj): The dataset to pass to the transformer.\n        Returns: The transformed X with grouped buckets.\n        \"\"\"\n        X_copy = X.copy()\n        for col in X_copy.columns:\n            X_copy[col] = X_copy[col].apply(lambda x: x if x in self.d[col] else 'others')\n        return X_copy\n\n    def fit(self, X, y=None, **fit_params):\n        \"\"\" Fits transformer over X.\n        Builds a dictionary of lists where the lists are category values of the\n        column key for preserving, since they meet the threshold.\n        \"\"\"\n        df_rows = len(X.index)\n        for col in X.columns:\n            calc_col = X.groupby(col)[col].agg(lambda x: (len(x) * 1.0) / df_rows)\n            self.d[col] = calc_col[calc_col >= self.threshold].index.tolist()\n        return self"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["# dfs with 100 elements in cat1 and cat2\n# note how df_test has elements 'g' and 't' in the respective categories (unknown values)\ndf_train = pd.DataFrame({'cat1': ['a'] * 20 + ['b'] * 30 + ['c'] * 40 + ['d'] * 3 + ['e'] * 4 + ['f'] * 3,\n                         'cat2': ['z'] * 25 + ['y'] * 25 + ['x'] * 25 + ['w'] * 20 +['v'] * 5})\ndf_test = pd.DataFrame({'cat1': ['a'] * 10 + ['b'] * 20 + ['c'] * 5 + ['d'] * 50 + ['e'] * 10 + ['g'] * 5,\n                        'cat2': ['z'] * 25 + ['y'] * 55 + ['x'] * 5 + ['w'] * 5 + ['t'] * 10})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["catgrouper = CategoryGrouper()\ncatgrouper.fit(df_train)\ndf_test_transformed = catgrouper.transform(df_test)\ndf_train_transformed = catgrouper.transform(df_train)\n\ndf_train_transformed"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[91]: CategoryGrouper(threshold=0.05)\nOut[91]: \n                    cat1 cat2\n0                      a    z\n1                      a    z\n2                      a    z\n3                      a    z\n4                      a    z\n5                      a    z\n6                      a    z\n7                      a    z\n8                      a    z\n9                      a    z\n10                     a    z\n11                     a    z\n12                     a    z\n13                     a    z\n14                     a    z\n15                     a    z\n16                     a    z\n17                     a    z\n18                     a    z\n19                     a    z\n20                     b    z\n21                     b    z\n22                     b    z\n23                     b    z\n24                     b    z\n25                     b    y\n26                     b    y\n27                     b    y\n28                     b    y\n29                     b    y\n30                     b    y\n31                     b    y\n32                     b    y\n33                     b    y\n34                     b    y\n35                     b    y\n36                     b    y\n37                     b    y\n38                     b    y\n39                     b    y\n40                     b    y\n41                     b    y\n42                     b    y\n43                     b    y\n44                     b    y\n45                     b    y\n46                     b    y\n47                     b    y\n48                     b    y\n49                     b    y\n50                     c    x\n51                     c    x\n52                     c    x\n53                     c    x\n54                     c    x\n55                     c    x\n56                     c    x\n57                     c    x\n58                     c    x\n59                     c    x\n60                     c    x\n61                     c    x\n62                     c    x\n63                     c    x\n64                     c    x\n65                     c    x\n66                     c    x\n67                     c    x\n68                     c    x\n69                     c    x\n70                     c    x\n71                     c    x\n72                     c    x\n73                     c    x\n74                     c    x\n75                     c    w\n76                     c    w\n77                     c    w\n78                     c    w\n79                     c    w\n80                     c    w\n81                     c    w\n82                     c    w\n83                     c    w\n84                     c    w\n85                     c    w\n86                     c    w\n87                     c    w\n88                     c    w\n89                     c    w\n90  CategoryGrouperOther    w\n91  CategoryGrouperOther    w\n92  CategoryGrouperOther    w\n93  CategoryGrouperOther    w\n94  CategoryGrouperOther    w\n95  CategoryGrouperOther    v\n96  CategoryGrouperOther    v\n97  CategoryGrouperOther    v\n98  CategoryGrouperOther    v\n99  CategoryGrouperOther    v\n</div>"]}}],"execution_count":25}],"metadata":{"name":"3. Turnover_modelling_FW","notebookId":1377202759692372},"nbformat":4,"nbformat_minor":0}
