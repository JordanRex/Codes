{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### _The AUTOML based generic framework for the AbInbev TURNOVER projects_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'april_17.csv', 'aug_16.csv', 'aug_17.csv', 'automl_new.ipynb', 'dec_16.csv', 'fasttext example.ipynb', 'feb_17.csv', 'I3_LighGBM_plus_Lime - Reduced Factors-v4.ipynb', 'jan_17.csv', 'july_16.csv', 'july_17.csv', 'JUNE18_TURNOVER.xlsx', 'june_16.csv', 'june_17.csv', 'MAIN_new.ipynb', 'mar_17.csv', 'may_17.csv', 'NEW.rar', 'nov_16.csv', 'nov_17.csv', 'oct_16.csv', 'oct_17.csv', 'plot_document_classification_20newsgroups.ipynb', 'sep_16.csv', 'sep_17.csv', 'test.txt', 'TRAIN.csv', 'train.txt', 'Turnover Report NAZ 2017 Year End.csv', 'turnover-2016-final.csv', 'VALID.csv']\n"
     ]
    }
   ],
   "source": [
    "## importing the relevant packages:\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# print/display all plots inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# the base packages\n",
    "import collections # for the Counter function\n",
    "import csv # for reading/writing csv files\n",
    "import pandas as pd, numpy as np, time, gc, bisect, re\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "import sklearn.decomposition as decomposition\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import sklearn.random_projection as rp\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials, space_eval\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# modelling/clustering algorithms\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# nlp modules\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "# Exporting packages for SHAP/LIME\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# missing value imputation\n",
    "from fancyimpute import KNN, MICE, NuclearNormMinimization\n",
    "\n",
    "# modules to handle 'if-else' and 'for' loop breaks\n",
    "import sys\n",
    "from io import StringIO\n",
    "from IPython import get_ipython\n",
    "\n",
    "# pickle modules\n",
    "import pickle\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 3 # number of iterations/parameter sets created towards tuning\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# to display multiple outputs in a cell without usin print/display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the train and validation dataset with the labels\n",
    "\n",
    "- The turnover files ('16 and '17)\n",
    "    - returns turnover dataframe with leavers and their termination date\n",
    "    - returns the unique list of leavers\n",
    "- The raw ads files (from June16 to Dec17 monthly files)\n",
    "    - all of them are aggregated\n",
    "    - split into two (active/leavers)\n",
    "    - 6 most recent records of leavers -> label=1\n",
    "    - 2 random records of active + leavers (records older than last 6) -> label=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_ads():\n",
    "    \n",
    "    # init function - returns nothing\n",
    "    def __init__():\n",
    "        \"\"\" this class creates the training dataframes and the labels from the raw monthly datasets \"\"\"\n",
    "        return None\n",
    "    \n",
    "    # turnover file creation function\n",
    "    def to_read_all():\n",
    "        # psg bands to filter\n",
    "        psg_bands = ['I-A', 'I-B', 'II-A', 'II-B', 'III-A', 'III-B', 'IV-A', 'IV-B', 'IX-A', 'IX-B', \n",
    "                     'V-A', 'V-B', 'VI-A', 'VI-B', 'VII-A', 'VII-B', 'VIII-A', 'VIII-B']\n",
    "\n",
    "        # read in the 2017 turnover file and filter\n",
    "        to_17=pd.read_csv('Turnover Report NAZ 2017 Year End.csv')\n",
    "        to_17=to_17[['Global ID', 'Termination Date', 'Name of action type', 'Name of reason for action', 'Pay Scale Group']]\n",
    "        to_17=to_17.loc[to_17['Name of reason for action'] == 'Resignation - Personal reason'].reset_index(drop=True)\n",
    "        to_17=to_17[to_17['Pay Scale Group'].isin(psg_bands)]\n",
    "        to_17['Termination Date'] = pd.to_datetime(to_17['Termination Date'])\n",
    "        to_17['term_year'] = to_17['Termination Date'].dt.year\n",
    "        to_17['term_month'] = to_17['Termination Date'].dt.month_name()\n",
    "        to_17['term_monthid'] = to_17['Termination Date'].dt.month\n",
    "\n",
    "        # read in the 2016 turnover file and filter\n",
    "        to_16=pd.read_csv('turnover-2016-final.csv')\n",
    "        to_16=to_16[['Global ID', 'Termination Date', 'Name of action type', 'Name of reason for action', 'Pay Scale Group']]\n",
    "        to_16=to_16.loc[to_16['Name of reason for action'] == 'Resignation - Personal reason'].reset_index(drop=True)\n",
    "        to_16=to_16[to_16['Pay Scale Group'].isin(psg_bands)]\n",
    "        to_16['Termination Date'] = pd.to_datetime(to_16['Termination Date'])\n",
    "        to_16['term_year'] = to_16['Termination Date'].dt.year\n",
    "        to_16['term_month'] = to_16['Termination Date'].dt.month_name()\n",
    "        to_16['term_monthid'] = to_16['Termination Date'].dt.month\n",
    "        to_16=to_16.loc[to_16['term_monthid'] > 5].reset_index(drop=True)\n",
    "\n",
    "        # append the two sets of people\n",
    "        to_all = pd.concat([to_16.reset_index(drop=True), to_17], axis=0)\n",
    "        to_all.drop_duplicates(inplace=True)\n",
    "        to_all.sort_values(inplace=True, by=['Global ID', 'term_year'])\n",
    "        to_all.reset_index(drop=True, inplace=True)\n",
    "        to_all.drop_duplicates(inplace=True, subset='Global ID')\n",
    "        to_ids = to_all['Global ID'].unique()\n",
    "        return to_all, to_ids\n",
    "\n",
    "    # final ads creation function to aggregate all the monthly files\n",
    "    # n = number of random records to be taken from the pool of records with label=0\n",
    "    # m = number of recent records to be taken from the pool of records of leavers and given label=1\n",
    "    def read_all(to_ids, n, m):\n",
    "        ## reading in the multiple month level files and mapping the labels based on the to files\n",
    "        dataset_files = os.listdir()\n",
    "        df16 = [x for x in dataset_files if re.search(pattern='_16', string=x)]\n",
    "        df17 = [x for x in dataset_files if re.search(pattern='_17', string=x)]\n",
    "        dict16_keys = [s.replace('.csv', '') for s in df16]\n",
    "        dict17_keys = [s.replace('.csv', '') for s in df17]\n",
    "\n",
    "        # the dict to store the individual datasets (per year)\n",
    "        dict16 = {}\n",
    "        dict17 = {}\n",
    "        \n",
    "        # the dictionary for months and month index mapping\n",
    "        dict_of_months = {'jan': 'January', 'feb': 'February', 'mar': 'March', 'april': 'April', 'may': 'May', 'june': 'June',\n",
    "                     'july': 'July', 'aug': 'August', 'sep': 'September', 'oct': 'October', 'nov': 'Novembor', 'dec': 'December'}\n",
    "        dict_of_months_ids = {'jan': 1, 'feb': 2, 'mar': 3, 'april': 4, 'may': 5, 'june': 6,\n",
    "                         'july': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "        # append the (multiple) datasets into one\n",
    "        iter=0\n",
    "        for i in df16:\n",
    "            dict16[dict16_keys[iter]] = pd.read_csv(str(i))\n",
    "            dict16[dict16_keys[iter]]['month_file'] = dict16_keys[iter]\n",
    "            dict16[dict16_keys[iter]]['term_year'] = 2016\n",
    "            dict16[dict16_keys[iter]]['term_month_temp'] = dict16[dict16_keys[iter]]['month_file'].apply(lambda s:s.split('_')[0])\n",
    "            dict16[dict16_keys[iter]]['term_month'] = dict16[dict16_keys[iter]]['term_month_temp'].map(dict_of_months)\n",
    "            dict16[dict16_keys[iter]]['term_monthid'] = dict16[dict16_keys[iter]]['term_month_temp'].map(dict_of_months_ids)\n",
    "            iter = iter+1\n",
    "\n",
    "        iter=0\n",
    "        for i in df17:\n",
    "            dict17[dict17_keys[iter]] = pd.read_csv(str(i))\n",
    "            dict17[dict17_keys[iter]]['month_file'] = dict17_keys[iter]\n",
    "            dict17[dict17_keys[iter]]['term_year'] = 2017\n",
    "            dict17[dict17_keys[iter]]['term_month_temp'] = dict17[dict17_keys[iter]]['month_file'].apply(lambda s:s.split('_')[0])\n",
    "            dict17[dict17_keys[iter]]['term_month'] = dict17[dict17_keys[iter]]['term_month_temp'].map(dict_of_months)\n",
    "            dict17[dict17_keys[iter]]['term_monthid'] = dict17[dict17_keys[iter]]['term_month_temp'].map(dict_of_months_ids)\n",
    "            iter = iter+1\n",
    "\n",
    "        df_16 = pd.concat(dict16.values(), ignore_index=True)\n",
    "        df_17 = pd.concat(dict17.values(), ignore_index=True)\n",
    "\n",
    "        df_all = pd.concat([df_16.reset_index(drop=True), df_17], axis=0)\n",
    "        df_all['global id'] = df_all['global id'].astype(int)\n",
    "        df_all['original id'] = df_all['original id'].astype(int)\n",
    "\n",
    "        df_all_temp=df_all.copy()\n",
    "        df_all_temp['flag_1'] = df_all_temp['global id'].isin(to_ids).astype(int)\n",
    "        df_all_temp['flag_2'] = df_all_temp['original id'].isin(to_ids).astype(int)\n",
    "        df_all_temp['flag'] = np.where((df_all_temp['flag_1'] == 1) | (df_all_temp['flag_2'] == 1), 1, 0)\n",
    "\n",
    "        # taking top m=3 recent records of leavers to flag as labels=1\n",
    "        x=df_all_temp.sort_values(ascending=False, by=['term_year', 'term_monthid']).groupby('global id').head(m)\n",
    "        # taking only The most recent record of leavers for label=1\n",
    "        #x=df_all_temp.sort_values(ascending=False, by=['global id', 'term_year', 'term_monthid']).drop_duplicates(['global id'])\n",
    "        x=x.loc[x['flag'] == 1].reset_index(drop=True)\n",
    "        x=x[['global id', 'term_year', 'term_monthid', 'flag']].reset_index(drop=True)\n",
    "        x.rename(columns={'flag': 'label'}, inplace=True)\n",
    "\n",
    "        df_all.drop(['label'], inplace=True, axis=1)\n",
    "        df_all_new=pd.merge(df_all, x, how='left', on=['global id', 'term_year', 'term_monthid'])\n",
    "        df_all_new['label'].fillna(0, inplace=True)\n",
    "        df_all_new['label'] = df_all_new['label'].astype(int)\n",
    "        df_all_new['term_monthid'] = df_all_new['term_monthid'].astype(int)\n",
    "        df_all_new.drop(['month_file', 'term_month_temp', 'term_month'], axis=1, inplace=True)\n",
    "        df_all_new.sort_values(by=['global id', 'term_year', 'term_monthid'], inplace=True)\n",
    "\n",
    "        mask = df_all_new['label'] == 1\n",
    "        df_pos_labels = df_all_new[mask]\n",
    "        df_neg_labels = df_all_new[~mask]\n",
    "\n",
    "        df_neg_labels=df_neg_labels.groupby('global id').apply(lambda x: x.sample(n, replace=True)).reset_index(drop=True)\n",
    "#         df_neg_labels['groupid'] = df_neg_labels.groupby(['global id']).cumcount()+1\n",
    "#         df_neg_labels['sample_flag'] = np.where((df_neg_labels['groupid'] % n == 0), 1, 0)\n",
    "#         df_neg_labels = df_neg_labels.loc[df_neg_labels['sample_flag'] == 1].reset_index(drop=True)\n",
    "#         df_neg_labels.drop(['groupid', 'sample_flag'], axis=1, inplace=True)\n",
    "\n",
    "        df_all_complete = pd.concat([df_pos_labels.reset_index(drop=True), df_neg_labels], axis=0)\n",
    "        df_all_complete.drop(['term_year', 'term_monthid'], axis=1, inplace=True)\n",
    "\n",
    "        df_final = df_all_complete.loc[df_all_complete['global id'].isin(to_ids)]\n",
    "        df_final.shape\n",
    "\n",
    "        return df_all_complete\n",
    "    \n",
    "    # main function that is called, prepares the datasets, and writes the TRAIN file into the working dir\n",
    "    # default values for n and m are given (6 recent records of leavers -> label=1, 2 random records of active -> label=0)\n",
    "    def main(n=1, m=6):\n",
    "        to_all, to_ids = prepare_ads.to_read_all()\n",
    "        df_all = prepare_ads.read_all(to_ids, n, m)\n",
    "\n",
    "        df_all.to_csv('TRAIN.csv', index=False)\n",
    "        #one-time exercise below so uncomment for the fresh beginning run\n",
    "        #os.replace('dec_17.csv', 'VALID.csv')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 6,
        "hidden": false,
        "row": 132,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### HELPER FUNCTIONS #####\n",
    "- Append datasets (train/valid)\n",
    "    - only used in the cases you have multiple train/validation files (for some reason?)\n",
    "    - appends on the basis of files found in folder starting with the string 'train' and 'valid'\n",
    "- DateTime feature engineering (year, quarter, season, month, week, day -> their corresponding string/numeric representations)\n",
    "    - this is done to bring the different representations of the date dimension as features into the model\n",
    "    - in most contexts where a time-series based approach is not employed, it is still a good practise to represent the date dimension in the form of numerical/categorical features that can help the model find (if any) date level information\n",
    "- Frequency count of a vector\n",
    "    - a helper function to get frequency count of different values in a list/vector\n",
    "- Deviation encoding of categorical features (not part of feature engineering since implemented before encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## HELPER FUNCTIONS CLASS ##\n",
    "\n",
    "class helper_funcs():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" helper functions used across the pipeline \"\"\"\n",
    "        return None\n",
    "    \n",
    "    ## find and append multiple dataframes of the type specified in string\n",
    "    def append_datasets(cols_to_remove, string = ['TRAIN', 'VALID']):\n",
    "        # pass either train or valid as str argument\n",
    "        temp_files = [name for name in os.listdir() if name.startswith(string)]\n",
    "        temp_dict = {}\n",
    "        for i in temp_files:\n",
    "            df_name = re.sub(string=i, pattern='.csv', repl='')\n",
    "            print(df_name)\n",
    "            temp_dict[df_name] = pd.read_csv(i, na_values=['No Data', ' ', 'UNKNOWN', ''])\n",
    "            temp_dict[df_name].columns = map(str.lower, temp_dict[df_name].columns)\n",
    "            temp_dict[df_name].drop(cols_to_remove, axis = 1, inplace = True)\n",
    "            chars_to_remove = [' ', '.', '(', ')', '__', '-']\n",
    "            for i in chars_to_remove:\n",
    "                temp_dict[df_name].columns = temp_dict[df_name].columns.str.strip().str.lower().str.replace(i, '_')\n",
    "        temp_list = [v for k,v in temp_dict.items()]\n",
    "        temp = pd.concat(temp_list, axis=0, sort=True, ignore_index=True)\n",
    "        return temp\n",
    "    \n",
    "    ## datetime feature engineering\n",
    "    def datetime_feats(train, valid):\n",
    "        cols = [s for s in train.columns.values if 'date' in s]\n",
    "        print('datetime feature engineering is happening ...', '\\n')\n",
    "        # nested function to derive the various datetime features for a given date column\n",
    "        def dt_feats(df, col):\n",
    "            df[col] = pd.to_datetime(df[i])\n",
    "            #df[str(col+'_'+'day')] = df[col].dt.day\n",
    "            #df[str(col+'_'+'day_name')] = df[col].dt.day_name\n",
    "            #df[str(col+'_'+'dayofweek')] = df[col].dt.dayofweek\n",
    "            df[str(col+'_'+'dayofyear')] = df[col].dt.dayofyear\n",
    "            #df[str(col+'_'+'days_in_month')] = df[col].dt.days_in_month\n",
    "            df[str(col+'_'+'month')] = df[col].dt.month\n",
    "            #df[str(col+'_'+'month_name')] = df[col].dt.month_name\n",
    "            df[str(col+'_'+'quarter')] = df[col].dt.quarter\n",
    "            #df[str(col+'_'+'week')] = df[col].dt.week\n",
    "            #df[str(col+'_'+'weekday')] = df[col].dt.weekday\n",
    "            df[str(col+'_'+'year')] = df[col].dt.year\n",
    "            #df[col] = df[col].dt.date\n",
    "            df = df.drop([col], axis = 1)\n",
    "            return df\n",
    "        # loop function over all raw date columns\n",
    "        for i in cols:\n",
    "            train = dt_feats(train, i)\n",
    "            valid = dt_feats(valid, i)\n",
    "        return train, valid\n",
    "    \n",
    "    ## function to get frequency count of elements in a vector/list\n",
    "    def freq_count(input_vector):\n",
    "        return collections.Counter(input_vector)\n",
    "    \n",
    "    ## function to make deviation encoding features\n",
    "    def categ_feat_eng(train_df, valid_df, cat_columns):\n",
    "        print('categorical feature engineering is happening ...', '\\n')\n",
    "        global iter\n",
    "        iter = 0\n",
    "        for i in tqdm(cat_columns):\n",
    "            grouped_df = pd.DataFrame(train_df.groupby([i])['label'].agg(['mean', 'count'])).reset_index()\n",
    "            grouped_df.rename(columns={'mean': str('mean_' + cat_columns[iter]),\n",
    "                                       'count': str('count_' + cat_columns[iter])}, inplace=True)\n",
    "            train_df = pd.merge(train_df, grouped_df, how='left')\n",
    "            valid_df = pd.merge(valid_df, grouped_df, how='left')\n",
    "            iter += 1\n",
    "        return train_df, valid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 8,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### LOOP BREAK FUNCTION ####\n",
    "\n",
    "*To allow early exit of loops or conditional statements to handle exceptions/errors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Allows exit() to work if script is invoked with IPython without raising NameError Exception. Keeps kernel alive. '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Allows exit() to work if script is invoked with IPython without raising NameError Exception. Keeps kernel alive. \"\"\"\n",
    "\n",
    "class IpyExit(SystemExit):\n",
    "    \"\"\"Exit Exception for IPython.\n",
    "\n",
    "    Exception temporarily redirects stderr to buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # print(\"exiting\")  # optionally print some message to stdout, too\n",
    "        # ... or do other stuff before exit\n",
    "        sys.stderr = StringIO()\n",
    "\n",
    "    def __del__(self):\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = sys.__stderr__  # restore from backup\n",
    "\n",
    "def ipy_exit():\n",
    "    raise IpyExit\n",
    "\n",
    "if get_ipython():    # ...run with IPython\n",
    "    exit = ipy_exit  # rebind to custom exit\n",
    "else:\n",
    "    exit = exit      # just make exit importable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 138,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### The Missing value imputation class ####\n",
    "\n",
    "- implements multiple methods to perform missing value treatement\n",
    "- simple methods (mean/median/mode) imputation for categorical and numerical features.\n",
    "    - slightly tweak for adding grouped level if needed\n",
    "    - currently built to perform imputation consistent to below process (after encoding)\n",
    "- fancy methods (NNM/KNN/MICE) imputation for numerical features only\n",
    "    - requires encoding to be done prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## MISSING VALUE IMPUTATION CLASS ##\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "#         X.groupby(['pay_scale_group', 'abinbev_entity2'])\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "    def num_missing(self):\n",
    "        return sum(self.isnull())\n",
    "    \n",
    "    def imputer_method(self, column, method=['mean', 'median', 'most_frequent']):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = method, axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def fancy_impute(X, which_method):\n",
    "        \"\"\" currently supported algorithms are KNN, NNM and MICE from the fancyimpute package\n",
    "        which_method = ['KNN', 'NNM', 'MICE']\n",
    "        \"\"\"\n",
    "        print(which_method, ' based missing value imputation is happening ...', '\\n')\n",
    "        \n",
    "        if which_method == 'NNM': X = NuclearNormMinimization().complete(X) # NNM method\n",
    "        if which_method == 'KNN': X = KNN(k=3, verbose=False).complete(X) # KNN method\n",
    "        if which_method == 'MICE':\n",
    "            X_complete_df = X.copy()\n",
    "            mice = MICE(verbose=False)\n",
    "            X_complete = mice.complete(np.asarray(X.values, dtype=float))\n",
    "            X_complete_df.loc[:, X.columns] = X_complete[:][:]\n",
    "            X = X_complete_df\n",
    "        print('missing value imputation completed', '\\n')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 8,
        "hidden": false,
        "row": 145,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### ENCODING CLASS ####\n",
    "- various types of encoding here\n",
    "    - label encoder\n",
    "    - binary encoder\n",
    "    - base encoder\n",
    "    - hashing encoder\n",
    "    - ordinal encoder (similar to label, different implementation)\n",
    "    - one-hot encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## ENCODERS CLASS ##\n",
    "\n",
    "class categ_encoders():\n",
    "    \n",
    "    def encoding(train, valid, y_train, y_valid, which=['le', 'be', 'bne', 'ohe', 'he', 'oe']):\n",
    "        if which=='le':\n",
    "            train, valid, categorical_names = categ_encoders.labelEncoder(train, valid)\n",
    "        elif which in ['be', 'bne', 'ohe', 'he', 'oe']:\n",
    "            train, valid, categorical_names = categ_encoders.ce_encodings(train, valid, y_train, y_valid, which)\n",
    "        else :\n",
    "            print('Not supported. Use one of [be, bne, he, oe, ohe]', '\\n')\n",
    "            exit()            \n",
    "        return train, valid, categorical_names\n",
    "        \n",
    "    def labelEncoder(train_df, valid_df):\n",
    "        print('label encoding is happening ...', '\\n')\n",
    "        cat_columns = train_df.select_dtypes(include=['object']).columns.values\n",
    "        categorical_names = {}\n",
    "        for feature in tqdm(cat_columns):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(train_df[feature].astype(str))\n",
    "            train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "            valid_df[feature] = valid_df[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "            le_classes = le.classes_.tolist()\n",
    "            bisect.insort_left(le_classes, 'No Data')\n",
    "            le.classes_ = le_classes\n",
    "            valid_df[feature] = le.transform(valid_df[feature].astype(str))\n",
    "            categorical_names[feature] = le.classes_\n",
    "        print('label encoding completed', '\\n')\n",
    "        return train_df, valid_df, categorical_names\n",
    "        \n",
    "    def ce_encodings(train_df, valid_df, y_train, y_valid, encoding):\n",
    "        print(str(encoding) + ' encoding is happening ...', '\\n')\n",
    "        if encoding=='bne':    \n",
    "            enc=ce.BaseNEncoder(base=4)\n",
    "        elif encoding=='be':\n",
    "            enc=ce.BinaryEncoder()\n",
    "        elif encoding=='he':\n",
    "            enc=ce.HashingEncoder(drop_invariant=True)\n",
    "        elif encoding=='oe':\n",
    "            enc=ce.OrdinalEncoder()\n",
    "        elif encoding=='ohe':\n",
    "            enc=ce.BaseNEncoder(base=1)\n",
    "        enc.fit(train_df)\n",
    "        train_enc=enc.transform(train_df)\n",
    "        valid_enc=enc.transform(valid_df)\n",
    "        print('category encoding completed', '\\n')\n",
    "        categorical_names = {}\n",
    "        return train_enc, valid_enc, categorical_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 20,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### FEATURE ENGINEERING MODULE ####\n",
    "\n",
    "- Decomposition features\n",
    "    - PCA\n",
    "    - ICA\n",
    "    - TSVD\n",
    "    - GRP\n",
    "    - SRP\n",
    "    - ...\n",
    "- Clustering output feaatures\n",
    "    - KMeans\n",
    "    - ...\n",
    "- Deterministic features\n",
    "    - Binning\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "class feat_eng(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" this module contains several functions for creating new features. find below a brief description of each \"\"\"\n",
    "    \n",
    "    def scalers(self, train, valid, which_method):\n",
    "        if which_method == 'ss':\n",
    "            sc = StandardScaler()\n",
    "            sc.fit(train)\n",
    "            train_new = pd.DataFrame(sc.transform(train), columns=train.columns.values)\n",
    "            valid_new = pd.DataFrame(sc.transform(valid), columns=valid.columns.values)\n",
    "            return train_new, valid_new # scale all variables to zero mean and unit variance, required for PCA and related\n",
    "        if which_method == 'mm':\n",
    "            mm = MinMaxScaler()\n",
    "            mm.fit(train)\n",
    "            train_new = pd.DataFrame(mm.transform(train), columns=train.columns.values)\n",
    "            valid_new = pd.DataFrame(mm.transform(valid), columns=valid.columns.values)\n",
    "            return train_new, valid_new # use this method to iterate\n",
    "    \n",
    "    def decomp_various(self, train, valid, n, which_method):\n",
    "        global decomp_dfs\n",
    "        decomp_dfs = {}\n",
    "        decomp_methods = ['PCA', 'FastICA', 'TruncatedSVD', 'GaussianRandomProjection', 'SparseRandomProjection']\n",
    "        \n",
    "        for i in decomp_methods:\n",
    "            if i == 'PCA':\n",
    "                decomp_obj = getattr(decomposition, i)\n",
    "                decomp_obj = decomp_obj(n_components=n)\n",
    "            elif i in ['FastICA', 'TruncatedSVD']:\n",
    "                decomp_obj = getattr(decomposition, i)\n",
    "                decomp_obj = decomp_obj(n_components=n)\n",
    "            else :\n",
    "                decomp_obj = getattr(rp, i)\n",
    "                decomp_obj = decomp_obj(n_components=n, eps=0.3)\n",
    "            \n",
    "            # perform the multiple decomposition techniques\n",
    "            train, valid = feat_eng.scalers(self, train, valid, which_method)\n",
    "\n",
    "            decomp_obj.fit(train)\n",
    "            decomp_train = pd.DataFrame(decomp_obj.transform(train))\n",
    "            decomp_valid = pd.DataFrame(decomp_obj.transform(valid))\n",
    "            cols = list(set(list(decomp_train)))\n",
    "            cols = [str(i) + '_' + str(s) for s in cols]\n",
    "            decomp_train.columns = cols\n",
    "            decomp_valid.columns = cols\n",
    "\n",
    "            decomp_dfs[i + '_train'] = decomp_train\n",
    "            decomp_dfs[i + '_valid'] = decomp_valid\n",
    "            \n",
    "        feat_eng.df = decomp_dfs\n",
    "        return None\n",
    "        \n",
    "    def return_combined(self, train, valid):        \n",
    "        for i in list(self.df.keys()):\n",
    "            if bool(re.search('train', i)):\n",
    "                train = pd.concat([train.reset_index(drop=True), self.df[i]], axis=1)\n",
    "            else :\n",
    "                valid = pd.concat([valid.reset_index(drop=True), self.df[i]], axis=1)\n",
    "        return train, valid\n",
    "    \n",
    "    def kmeans_clusterer(train_df, valid_df, n):\n",
    "        clusterer = KMeans(n, random_state=1, init='k-means++')\n",
    "        # fit the clusterer\n",
    "        clusterer.fit(train_df)\n",
    "        train_clusters = clusterer.predict(train_df)\n",
    "        valid_clusters = clusterer.predict(valid_df)\n",
    "        return train_clusters, valid_clusters\n",
    "    \n",
    "    def kmeans_feats(train_df, valid_df, m=5):\n",
    "        print('m is ', m, '\\n')\n",
    "        for i in range(2, m):\n",
    "            t, v = feat_eng.kmeans_clusterer(train_df, valid_df, n=i)\n",
    "            col_name = str('kmeans_'+ str(i))\n",
    "            t = pd.DataFrame({col_name: t})\n",
    "            v = pd.DataFrame({col_name: v})\n",
    "            train_df = pd.concat([train_df.reset_index(drop=True), t], axis=1)\n",
    "            valid_df = pd.concat([valid_df.reset_index(drop=True), v], axis=1)\n",
    "        return train_df, valid_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 153,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### FEATURE SELECTION ####\n",
    "- near zero variance columns are removed (threshold=0.1)\n",
    "- rf based rfecv with depth=7, column_sampling=0.25, estimators=100 (optional=True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## FEATURE SELECTION\n",
    "\n",
    "class feat_selection():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module is for dynamic feature selection after all the processing and feat engineering phases. ideally this\n",
    "        module is followed by the modelling phase immediately \"\"\"\n",
    "\n",
    "    # removing near zero variance columns\n",
    "    def variance_threshold_selector(train, valid, threshold):\n",
    "        print('input data shape is: ', train.shape, '\\n')\n",
    "        selector = VarianceThreshold(threshold)\n",
    "        selector.fit(train)\n",
    "        X = train[train.columns[selector.get_support(indices=True)]]\n",
    "        Y = valid[valid.columns[selector.get_support(indices=True)]]\n",
    "        #display(pd.DataFrame(X.head(5)))\n",
    "        print('output data shape is: ', X.shape, '\\n')\n",
    "        return X, Y\n",
    "\n",
    "    # using RFECV\n",
    "    def rfecv(train, valid, y_train):\n",
    "        # Create the RFE object and compute a cross-validated score.\n",
    "        model = LogisticRegression(C=0.1, penalty='l1')\n",
    "        #model = RandomForestClassifier(max_depth=10, max_features=0.3, n_estimators=200, n_jobs=-1)\n",
    "        rfecv = RFECV(estimator=model, step=1, scoring='roc_auc', verbose=True)\n",
    "        rfecv.fit(train, y_train)\n",
    "        print(\"Optimal number of features : %d\" % rfecv.n_features_, '\\n')\n",
    "\n",
    "        # Plot number of features VS. cross-validation scores\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of features selected\")\n",
    "        plt.ylabel(\"Cross validation score (roc-auc)\")\n",
    "        plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "        plt.show()\n",
    "\n",
    "        features = [f for f,s in zip(train.columns, rfecv.support_) if s]\n",
    "        train = train[features]\n",
    "        valid = valid[features]\n",
    "        return train, valid\n",
    "    \n",
    "    def feat_selection(train, valid, y_train, t=0.3):\n",
    "        # read in the train, valid and y_train objects\n",
    "        X, Y = feat_selection.variance_threshold_selector(train, valid, threshold=t)\n",
    "        X, Y = feat_selection.rfecv(train=X, valid=Y, y_train=y_train)\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 157,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### SAMPLING ####\n",
    "- Oversampling (ADASYN, SMOTE)\n",
    "- Undersampling (ENN, RENN, AllKNN)\n",
    "- Oversampling and then Undersampling (SMOTE and ENN/TOMEK)\n",
    "\n",
    "*it's okay if you have no idea what the above mean. the only thing that is important is to understand why over/undersampling\n",
    "is done and why or what ratio between*\n",
    "    - why over/under sampling is done in a classification context\n",
    "    - what ratio between the 2 classes is important to You in your context\n",
    "    - how much information loss (or gain) are you willing to tolerate? (create More data than what you have at hand?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Explicitly doing sampling. Use with care if going ahead with the CV based approach. Keep ratio low if so (recommended)\\n\\noversampling the minority class using techniques from SMOTE (for oversampling) and ENN/Tomek (for undersampling/cleaning)\\nENN worked out better than Tomek\\nadded support for undersampling with ENN/RENN/AllKNN '"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Explicitly doing sampling. Use with care if going ahead with the CV based approach. Keep ratio low if so (recommended)\n",
    "\n",
    "oversampling the minority class using techniques from SMOTE (for oversampling) and ENN/Tomek (for undersampling/cleaning)\n",
    "ENN worked out better than Tomek\n",
    "added support for undersampling with ENN/RENN/AllKNN \"\"\"\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.under_sampling import AllKNN, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "def sampler(X_train, y_train, which='smote_enn', frac=0.75):\n",
    "    \"\"\" which = ['adasyn', smote_tomek', 'smote_enn', 'enn', 'renn', 'allknn'] \"\"\"\n",
    "    \n",
    "    feat_names = X_train.columns.values\n",
    "    \n",
    "    ### OVERSAMPLING (ADASYN) ###\n",
    "    if which=='adasyn':\n",
    "        # Apply ADASYN\n",
    "        ada = ADASYN(random_state=0)\n",
    "        X_train, y_train = ada.fit_sample(X_train, y_train)\n",
    "\n",
    "    ### OVERSAMPLING (SMOTE) AND THEN UNDERSAMPLING (ENN/Tomek) ###\n",
    "    if which=='smote_tomek':\n",
    "        # Apply SMOTE + Tomek links\n",
    "        sm = SMOTETomek(random_state=0, ratio=frac)\n",
    "        X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    if which=='smote_enn':\n",
    "        # Apply SMOTE + ENN\n",
    "        smote_enn = SMOTEENN(random_state=0, ratio=frac)\n",
    "        X_train, y_train = smote_enn.fit_sample(X_train, y_train)\n",
    "\n",
    "    ### UNDERSAMPLING (ENN/RENN/AllKNN) ###\n",
    "    if which=='enn':\n",
    "        # Apply ENN\n",
    "        enn = EditedNearestNeighbours(random_state=0)\n",
    "        X_train, y_train = enn.fit_sample(X_train, y_train)\n",
    "    if which=='renn':\n",
    "        # Apply RENN\n",
    "        renn = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "        X_train, y_train = renn.fit_sample(X_train, y_train)\n",
    "    if which=='allknn':\n",
    "        # Apply AllKNN\n",
    "        allknn = AllKNN(random_state=0)\n",
    "        X_train, y_train = allknn.fit_sample(X_train, y_train)\n",
    "\n",
    "    X_train = pd.DataFrame(data=X_train,columns=feat_names)\n",
    "    print(X_train.shape, y_train.shape, collections.Counter(y_train))\n",
    "    \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### h2o AUTOML ####\n",
    "- automated ML framework by H2O\n",
    "- https://www.h2o.ai/products/h2o/\n",
    "- tuning performed for the sampling parameters as well as whether to balance classes post sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## h2o AUTO_ML grid search framework\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "class h2o_automl():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" module to invoke h2o auto ml and tune on the sampling parameters. \n",
    "        will need to incorporate to super pipeline later \"\"\"\n",
    "        \n",
    "    def automl(os, us, bc, iter, response='response'):\n",
    "        # initializing the h2o cluster\n",
    "        h2o.init()\n",
    "        # Import a sample binary outcome train/test set into H2O\n",
    "        h2o_train = h2o.import_file(str(str(iter) + '_t_h2o.csv'), header=1)\n",
    "        h2o_valid = h2o.import_file(str(str(iter) + '_v_h2o.csv'), header=1)\n",
    "        # Identify the response and set of predictors\n",
    "        x = list(h2o_train.columns)  #if x is defined as all columns except the response, then x is not required\n",
    "        x.remove(response)\n",
    "        # For binary classification, response should be a factor\n",
    "        h2o_train[response] = h2o_train[response].asfactor()\n",
    "        h2o_valid[response] = h2o_valid[response].asfactor()\n",
    "\n",
    "        randomseed = 1\n",
    "        aml = H2OAutoML(max_runtime_secs = 60, stopping_metric='mean_per_class_error', sort_metric='mean_per_class_error',\n",
    "                        class_sampling_factors=[os, us], balance_classes = bc)\n",
    "        aml.train(y = 'response', training_frame = h2o_train)\n",
    "        \n",
    "        # Print Leaderboard (ranked by xval metrics)\n",
    "        print(aml.leaderboard)\n",
    "        # Evaluate performance on a test set\n",
    "        perf = aml.leader.model_performance(h2o_valid)\n",
    "        print('The validation performance (auc) is ', perf.auc())\n",
    "        return aml, h2o_valid\n",
    "        \n",
    "    def get_score(aml, h2o_valid, y_valid, threshold = 0.1):\n",
    "        pred2 = aml.predict(h2o_valid)[:,2]\n",
    "        pred = pred2.as_data_frame().as_matrix()\n",
    "        predict = np.where(pred > threshold, 1, 0)\n",
    "        y_test=y_valid\n",
    "\n",
    "        recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "        precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "        f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "        auc_score = roc_auc_score(y_test, pred)\n",
    "        tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "        print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "        print('recall score is: ', recall_score)\n",
    "        print('precision score is: ', precision_score)\n",
    "        print('f1_score is: ', f1_score)\n",
    "        print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "        print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n",
    "        \n",
    "        return pred2, predict, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 166,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### MOTHER OF ALL PIPELINES ####\n",
    "- hyperopt based\n",
    "- the process flow is as below\n",
    "    - prepare data\n",
    "    - pre-processing\n",
    "        - missing value imputation\n",
    "        - categorical encoding\n",
    "    - feature engineering\n",
    "    - feature selection\n",
    "    - automl (modelling pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## MOTHER OF ALL PIPELINES ##\n",
    "# hyperopt based ##\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" random \"\"\"\n",
    "        return None    \n",
    "    \n",
    "    def prepare(self, cols_to_remove, response='label'):\n",
    "        \"\"\" checks first if backup pickles exist in folder already. \n",
    "        if so skips the major computation segment that is already back up \"\"\"\n",
    "        \n",
    "        pickle_files = [x for x in os.listdir() if re.search(pattern='.pickle', string=x)]\n",
    "        trials = {'backup_1': None}\n",
    "        \n",
    "        if len(pickle_files) > 0 :\n",
    "            # opening the first pickle only for now (will later add a loop for all the pickle files)\n",
    "            \n",
    "            print('backups available...hence using them stead of reinventing the wheel \\n')\n",
    "            f = open(pickle_files[0], \"rb\")\n",
    "            train = pickle.load(f)\n",
    "            valid = pickle.load(f)\n",
    "            y_train = pickle.load(f)\n",
    "            y_valid = pickle.load(f)\n",
    "            backup_md = pickle.load(f)\n",
    "            f.close()\n",
    "            print(backup_md)\n",
    "            \n",
    "            trials['backup_1'] = main.backup_optimize(train=train, valid=valid, y_train=y_train, \n",
    "                                                      y_valid=y_valid, backup_md=backup_md)\n",
    "        \n",
    "        else :\n",
    "            # read in the train and validation datasets\n",
    "            # clean column names and remove unwanted columns\n",
    "            # append the (multiple?) train datasets into a single one (simple appending for now)\n",
    "\n",
    "            print('1. Appending the multiple train/valid datasets in the working directory \\n')\n",
    "            train = helper_funcs.append_datasets(string='TRAIN', cols_to_remove=cols_to_remove)\n",
    "            valid = helper_funcs.append_datasets(string='VALID', cols_to_remove=cols_to_remove)\n",
    "            main.removed_cols = cols_to_remove ## attribute\n",
    "\n",
    "            # creating the datetime features from date columns (works only for cols with date in header, modify for other cases)\n",
    "            print('2. Datetime features are being created for the columns (which have \"date\" in their column name) \\n')\n",
    "            train, valid = helper_funcs.datetime_feats(train, valid)\n",
    "\n",
    "            # missing value threshold control (for both rows and columns)\n",
    "            mt = 0.6\n",
    "            print(train.shape, '\\n')\n",
    "            train.dropna(thresh=mt*(train.shape[0]), axis=1, inplace = True)\n",
    "            train.dropna(thresh=mt*(train.shape[1]), axis=0, inplace = True)\n",
    "            print(train.shape, '\\n')\n",
    "            valid = valid[train.columns]\n",
    "            valid.dropna(thresh=mt*(valid.shape[0]), axis=1, inplace = True)\n",
    "            train = train[valid.columns]\n",
    "            main.missing_threshold = mt ## attribute\n",
    "\n",
    "            # reset the index since inplace operations happened earlier\n",
    "            train.index = pd.RangeIndex(len(train.index))\n",
    "            valid.index = pd.RangeIndex(len(valid.index))\n",
    "            # save the global ids for mapping later (forward looking)\n",
    "            valid_ids = valid[['original_id', response]]\n",
    "            main.validation_labels = valid_ids ## attribute\n",
    "            valid_ids.to_csv('test_dfs.csv', index=False)\n",
    "            valid.drop('original_id', axis=1, inplace=True)\n",
    "            train.drop('original_id', axis=1, inplace=True)\n",
    "            X_train = pd.DataFrame(train)\n",
    "            X_valid = pd.DataFrame(valid)\n",
    "            # the class balance in the training dataset for the response\n",
    "            print(helper_funcs.freq_count(X_train[response]), '\\n')\n",
    "            # creating the response vector\n",
    "            y_train = X_train[response].values\n",
    "            y_valid = X_valid[response].values\n",
    "\n",
    "            # categorical columns (names, indices and dtypes)\n",
    "            x = list(X_train.dtypes)\n",
    "            x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "            categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "            # Get feature names and their values for categorical data (needed for LIME)\n",
    "            cat_columns = X_train.select_dtypes(include=['object']).columns.values\n",
    "            X_train, X_valid = helper_funcs.categ_feat_eng(X_train, X_valid, cat_columns)\n",
    "\n",
    "            # drop the response\n",
    "            X_train = X_train.drop([response], axis = 1)\n",
    "            X_valid = X_valid.drop([response], axis = 1)\n",
    "\n",
    "            # call the main optimize function that does the whole tuning (inside the nested score function)\n",
    "            trials = main.optimize(train=X_train, valid=X_valid, y_train=y_train, y_valid=y_valid)\n",
    "        return trials\n",
    "    \n",
    "    # function to be minimized and sent to the optimize function of hyperopt\n",
    "    def score(params):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        \n",
    "        train=main.train\n",
    "        valid=main.valid\n",
    "        y_train=main.y_train\n",
    "        y_valid=main.y_valid\n",
    "        \n",
    "        print('\\n', params, '\\n')\n",
    "        #######################################################################################################\n",
    "        ## ENCODING ##\n",
    "        #######################################################################################################\n",
    "        cat_columns = train.select_dtypes(include=['object']).columns.values\n",
    "        train_cat = train[cat_columns]\n",
    "        num_cols = list(set(train.columns)-set(train_cat.columns))\n",
    "        train_num = train[num_cols]\n",
    "        valid_cat = valid[cat_columns]\n",
    "        valid_num = valid[num_cols]\n",
    "        \n",
    "        train_cat, valid_cat, categorical_names = categ_encoders.encoding(train_cat, valid_cat, y_train, y_valid,\n",
    "                                                                            which=params['encoder'])\n",
    "        train = pd.concat([train_cat.reset_index(drop=True), train_num], axis=1)\n",
    "        valid = pd.concat([valid_cat.reset_index(drop=True), valid_num], axis=1)\n",
    "        print('encoding completed ...', '\\n')\n",
    "        main.categorical_dict = categorical_names ## attribute\n",
    "        #######################################################################################################\n",
    "\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## CORRELATION ANALYSIS ##\n",
    "        #######################################################################################################\n",
    "        # remove highly correlated features to reduce further computation time\n",
    "        print('correlation analysis is happening ...', '\\n')\n",
    "        # Create correlation matrix\n",
    "        corr_matrix = train.corr().abs()\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        # Find index of feature columns with correlation greater than 0.9\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]        \n",
    "        # Drop features\n",
    "        #print(to_drop, '\\n')\n",
    "        train.drop(to_drop, axis=1, inplace=True)\n",
    "        valid.drop(to_drop, axis=1, inplace=True)\n",
    "        print('correlation analysis completed ...', '\\n')\n",
    "        main.cor_dropped_vars = to_drop ## attribute\n",
    "        #######################################################################################################\n",
    "    \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## MISSING VALUE IMPUTATION ##\n",
    "        #######################################################################################################\n",
    "        # store all feature names\n",
    "        feat_names = train.columns.values\n",
    "        feat_names2 = valid.columns.values\n",
    "        \n",
    "        if params['miss_treatment'] == 'simple':\n",
    "            miss_enc = DataFrameImputer()\n",
    "            miss_enc.fit(X=train)\n",
    "            train_new = miss_enc.transform(train)\n",
    "            valid_new = miss_enc.transform(valid)\n",
    "        elif params['miss_treatment'] in ['KNN', 'MICE']:\n",
    "            train_new = DataFrameImputer.fancy_impute(train, which_method=params['miss_treatment'])\n",
    "            valid_new = DataFrameImputer.fancy_impute(valid, which_method=params['miss_treatment'])\n",
    "\n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        train = pd.DataFrame(data=train_new, columns=feat_names)\n",
    "        valid = pd.DataFrame(data=valid_new, columns=feat_names2)\n",
    "        print('missing value treatment completed ...', '\\n')\n",
    "        #######################################################################################################\n",
    "        \n",
    "\n",
    "        #######################################################################################################\n",
    "        ## STATUS REPORT ##\n",
    "        #######################################################################################################\n",
    "        print('STATUS REPORT \\n')\n",
    "        print(train.shape)\n",
    "        print(valid.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_valid.shape)\n",
    "        print(collections.Counter(y_train))\n",
    "        print(collections.Counter(y_valid))\n",
    "        #######################################################################################################\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## FEATURE ENGINEERING ##\n",
    "        #######################################################################################################\n",
    "        \"\"\" the feature engineering module \n",
    "            - 1. PCA/ICA/TSVD/GRP/SRP\n",
    "            - 2. KMEANS \"\"\"\n",
    "        \n",
    "        ## 1.\n",
    "        feat_eng_instance = feat_eng()\n",
    "        feat_eng_instance.decomp_various(train, valid, n=int(params['decomp_feats']), which_method=params['scaler'])\n",
    "        train, valid = feat_eng_instance.return_combined(train, valid)\n",
    "        \n",
    "        ## 2.\n",
    "        train, valid = feat_eng.kmeans_feats(train_df=train, valid_df=valid, m=int(params['kmeans_n']))\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## FEATURE SELECTION ##\n",
    "        #######################################################################################################\n",
    "        train, valid = feat_selection.variance_threshold_selector(train=train, valid=valid, threshold=0.1)\n",
    "        \n",
    "        if params['feat_selection'] == 'true':\n",
    "            train, valid = feat_selection.rfecv(train=train, valid=valid, y_train=y_train)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAMPLING ##\n",
    "        #######################################################################################################\n",
    "        \"\"\" oversampling or undersampling or oversampling with undersampling \"\"\"\n",
    "        \n",
    "        if params['sampler']['choice'] == 'yes':\n",
    "            train, y_train = sampler(X_train=train, y_train=y_train, \n",
    "                                     which=params['sampler']['which_method'], \n",
    "                                     frac=params['sampler']['frac'])\n",
    "        else :\n",
    "            print('no sampling done in this pipeline', '\\n')\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## BACKUP ##\n",
    "        #######################################################################################################\n",
    "        backup = str(str(ITERATION) + str(dt.now().strftime('_%H_%M_%d_%m_%Y.pickle')))\n",
    "        f = open(backup, \"wb\")\n",
    "        pickle.dump(train, f)\n",
    "        pickle.dump(valid, f)\n",
    "        pickle.dump(y_train, f)\n",
    "        pickle.dump(y_valid, f)\n",
    "        \n",
    "        backup_md = {'params': params, 'pickle_name': backup, 'randomseed': randomseed}\n",
    "        pickle.dump(backup_md, f)\n",
    "        \n",
    "        f.close()\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAVE AS FLATFILES ##\n",
    "        #######################################################################################################\n",
    "        train['response'] = y_train\n",
    "        valid['response'] = y_valid\n",
    "\n",
    "        train.to_csv(str(str(ITERATION) + '_t_h2o.csv'), index=False)\n",
    "        valid.to_csv(str(str(ITERATION) + '_v_h2o.csv'), index=False)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## H2O AUTOML ##\n",
    "        #######################################################################################################\n",
    "        h2o_os = params['h2o_automl_params']['oversampling']\n",
    "        h2o_us = params['h2o_automl_params']['undersampling']\n",
    "        h2o_bc = params['h2o_automl_params']['balance_classes']\n",
    "        \n",
    "        aml, h2o_valid = h2o_automl.automl(os=h2o_os, bc=h2o_bc, us=h2o_us, iter=ITERATION)\n",
    "        \n",
    "        pred, predict, score = h2o_automl.get_score(aml=aml, h2o_valid=h2o_valid, y_valid=y_valid,\n",
    "                                                    threshold=aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        \n",
    "        setattr(main, str('aml_' + str(ITERATION)), aml)\n",
    "        setattr(main, str('pred_' + str(ITERATION)), pred)\n",
    "        setattr(main, str('predict_' + str(ITERATION)), predict)\n",
    "        setattr(main, str('score_' + str(ITERATION)), score)\n",
    "        setattr(main, str('threshold_' + str(ITERATION)), aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        setattr(main, str('h2o_valid_' + str(ITERATION)), h2o_valid)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        loss = 1 - score\n",
    "        end_time = time.time()\n",
    "        time_taken = timedelta(seconds = round(end_time - start_time))\n",
    "        print(\"Execution took: %s secs (Wall clock time)\" % time_taken)\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK, 'params': params, 'auc': score, 'eval_time': time_taken}\n",
    "    \n",
    "    # function to do hyperparameter tuning with hyperopt (bayesian based method)\n",
    "    def optimize(train, valid, y_train, y_valid):\n",
    "                \n",
    "        main.train = train\n",
    "        main.valid = valid\n",
    "        main.y_train = y_train\n",
    "        main.y_valid= y_valid        \n",
    "        \n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        \n",
    "        # space to be traversed for the hyperopt function\n",
    "        space = {\n",
    "            'encoder': hp.choice('encoder', ['he', 'le']),\n",
    "            'eval_time': time.time(),\n",
    "            'miss_treatment': hp.choice('missing', ['simple']),\n",
    "            'decomp_feats': hp.quniform('n', 2, 10, 1),\n",
    "            'scaler': hp.choice('scaler', ['ss', 'mm']),\n",
    "            'kmeans_n': hp.quniform('m', 2, 5, 1),\n",
    "            'feat_selection': hp.choice('rfecv', ['false', 'false']),\n",
    "            'sampler': hp.choice('sampler', [\n",
    "                {\n",
    "                    'choice': 'no',\n",
    "                    'which_method': hp.choice('sampling', ['smote_enn']),\n",
    "                    'frac': hp.quniform('frac', 0.85, 1, 0.05)\n",
    "                },\n",
    "                {\n",
    "                    'choice': 'no'\n",
    "                }\n",
    "            ]),\n",
    "            'h2o_automl_params': hp.choice('sampling_params', [\n",
    "                {\n",
    "                    'undersampling': hp.uniform('us', 0.1, 1),\n",
    "                    'oversampling': hp.uniform('os', 2, 5),\n",
    "                    'balance_classes': hp.choice('bc', ['False', 'False'])\n",
    "                }\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        best = fmin(main.score, space, algo=tpe.suggest, trials=trials, max_evals=MAX_EVALS,\n",
    "                    rstate=np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        \n",
    "        main.best = space_eval(space, trials.argmin)\n",
    "        main.trials = trials\n",
    "        \n",
    "        return trials # results of all the iterations, the best params\n",
    "    \n",
    "    def backup_optimize(train, valid, y_train, y_valid, backup_md):\n",
    "        main.train = train\n",
    "        main.valid = valid\n",
    "        main.y_train = y_train\n",
    "        main.y_valid= y_valid\n",
    "        main.md = backup_md\n",
    "        params = backup_md['params']\n",
    "        \n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = backup_md['pickle_name'][0]\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAVE AS FLATFILES ##\n",
    "        #######################################################################################################\n",
    "        train['response'] = y_train\n",
    "        valid['response'] = y_valid\n",
    "\n",
    "        train.to_csv(str(str(ITERATION) + '_t_h2o.csv'), index=False)\n",
    "        valid.to_csv(str(str(ITERATION) + '_v_h2o.csv'), index=False)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## H2O AUTOML ##\n",
    "        #######################################################################################################\n",
    "        h2o_os = params['h2o_automl_params']['oversampling']\n",
    "        h2o_us = params['h2o_automl_params']['undersampling']\n",
    "        h2o_bc = params['h2o_automl_params']['balance_classes']\n",
    "        \n",
    "        aml, h2o_valid = h2o_automl.automl(os=h2o_os, bc=h2o_bc, us=h2o_us, iter=ITERATION)\n",
    "        \n",
    "        pred, predict, score = h2o_automl.get_score(aml=aml, h2o_valid=h2o_valid, y_valid=y_valid,\n",
    "                                                    threshold=aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        \n",
    "        setattr(main, str('aml_' + str(ITERATION)), aml)\n",
    "        setattr(main, str('pred_' + str(ITERATION)), pred)\n",
    "        setattr(main, str('predict_' + str(ITERATION)), predict)\n",
    "        setattr(main, str('score_' + str(ITERATION)), score)\n",
    "        setattr(main, str('threshold_' + str(ITERATION)), aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        setattr(main, str('h2o_valid_' + str(ITERATION)), h2o_valid)\n",
    "        #######################################################################################################\n",
    "        trials = score\n",
    "        \n",
    "        return trials        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_ads.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Appending the multiple train/valid datasets in the working directory \n",
      "\n",
      "TRAIN\n",
      "VALID\n",
      "2. Datetime features are being created for the columns (which have \"date\" in their column name) \n",
      "\n",
      "datetime feature engineering is happening ... \n",
      "\n",
      "(12429, 101) \n",
      "\n",
      "(11362, 67) \n",
      "\n",
      "Counter({0: 6824, 1: 4538}) \n",
      "\n",
      "categorical feature engineering is happening ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:04<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'decomp_feats': 7.0, 'encoder': 'he', 'eval_time': 1540452110.239333, 'feat_selection': 'false', 'h2o_automl_params': {'balance_classes': 'False', 'oversampling': 2.701652699333679, 'undersampling': 0.315595564665503}, 'kmeans_n': 4.0, 'miss_treatment': 'simple', 'sampler': {'choice': 'no', 'frac': 0.9, 'which_method': 'smote_enn'}, 'scaler': 'ss'} \n",
      "\n",
      "he encoding is happening ... \n",
      "\n",
      "category encoding completed \n",
      "\n",
      "encoding completed ... \n",
      "\n",
      "correlation analysis is happening ... \n",
      "\n",
      "correlation analysis completed ... \n",
      "\n",
      "missing value treatment completed ... \n",
      "\n",
      "STATUS REPORT \n",
      "\n",
      "(11362, 89)\n",
      "(5734, 89)\n",
      "(11362,)\n",
      "(5734,)\n",
      "Counter({0: 6824, 1: 4538})\n",
      "Counter({0: 5453, 1: 281})\n",
      "m is  4 \n",
      "\n",
      "input data shape is:  (11362, 126) \n",
      "\n",
      "output data shape is:  (11362, 93) \n",
      "\n",
      "no sampling done in this pipeline \n",
      "\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n",
      "Warning: Your H2O cluster version is too old (3 months and 14 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>19 hours 55 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>3 months and 14 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Lenovo_6jriv6</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.140 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         19 hours 55 mins\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.3\n",
       "H2O cluster version age:    3 months and 14 days !!!\n",
       "H2O cluster name:           H2O_from_python_Lenovo_6jriv6\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.140 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20181025_125214</td><td style=\"text-align: right;\">0.950912</td><td style=\"text-align: right;\"> 0.275046</td><td style=\"text-align: right;\">              0.112152</td><td style=\"text-align: right;\">0.285611</td><td style=\"text-align: right;\">0.0815739</td></tr>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20181025_125214   </td><td style=\"text-align: right;\">0.950912</td><td style=\"text-align: right;\"> 0.275046</td><td style=\"text-align: right;\">              0.112152</td><td style=\"text-align: right;\">0.285611</td><td style=\"text-align: right;\">0.0815739</td></tr>\n",
       "<tr><td>DRF_0_AutoML_20181025_125214                         </td><td style=\"text-align: right;\">0.943767</td><td style=\"text-align: right;\"> 0.363034</td><td style=\"text-align: right;\">              0.117987</td><td style=\"text-align: right;\">0.322979</td><td style=\"text-align: right;\">0.104315 </td></tr>\n",
       "<tr><td>XRT_0_AutoML_20181025_125214                         </td><td style=\"text-align: right;\">0.941892</td><td style=\"text-align: right;\"> 0.368168</td><td style=\"text-align: right;\">              0.123851</td><td style=\"text-align: right;\">0.324091</td><td style=\"text-align: right;\">0.105035 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181025_125214_model_0            </td><td style=\"text-align: right;\">0.924979</td><td style=\"text-align: right;\"> 0.357887</td><td style=\"text-align: right;\">              0.146588</td><td style=\"text-align: right;\">0.333298</td><td style=\"text-align: right;\">0.111088 </td></tr>\n",
       "<tr><td>GLM_grid_0_AutoML_20181025_125214_model_0            </td><td style=\"text-align: right;\">0.833173</td><td style=\"text-align: right;\"> 0.485691</td><td style=\"text-align: right;\">              0.244199</td><td style=\"text-align: right;\">0.404174</td><td style=\"text-align: right;\">0.163356 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The validation performance (auc) is  0.7393468481550199\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5252  201]\n",
      " [ 199   82]] \n",
      "\n",
      "recall score is:  0.2918149466192171\n",
      "precision score is:  0.28975265017667845\n",
      "f1_score is:  0.2907801418439716\n",
      "accuracy score:  0.930240669689571\n",
      "The final AUC after taking the best params and num_rounds when it stopped is 0.7393. \n",
      "\n",
      "Execution took: 0:01:30 secs (Wall clock time)\n",
      "\n",
      " {'decomp_feats': 7.0, 'encoder': 'le', 'eval_time': 1540452110.239333, 'feat_selection': 'false', 'h2o_automl_params': {'balance_classes': 'False', 'oversampling': 2.965137127316968, 'undersampling': 0.7323804931999229}, 'kmeans_n': 4.0, 'miss_treatment': 'simple', 'sampler': {'choice': 'no'}, 'scaler': 'mm'} \n",
      "\n",
      "label encoding is happening ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:06<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding completed \n",
      "\n",
      "encoding completed ... \n",
      "\n",
      "correlation analysis is happening ... \n",
      "\n",
      "correlation analysis completed ... \n",
      "\n",
      "missing value treatment completed ... \n",
      "\n",
      "STATUS REPORT \n",
      "\n",
      "(11362, 104)\n",
      "(5734, 104)\n",
      "(11362,)\n",
      "(5734,)\n",
      "Counter({0: 6824, 1: 4538})\n",
      "Counter({0: 5453, 1: 281})\n",
      "m is  4 \n",
      "\n",
      "input data shape is:  (11362, 141) \n",
      "\n",
      "output data shape is:  (11362, 110) \n",
      "\n",
      "no sampling done in this pipeline \n",
      "\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n",
      "Warning: Your H2O cluster version is too old (3 months and 14 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>19 hours 56 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>3 months and 14 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Lenovo_6jriv6</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.073 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         19 hours 56 mins\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.3\n",
       "H2O cluster version age:    3 months and 14 days !!!\n",
       "H2O cluster name:           H2O_from_python_Lenovo_6jriv6\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.073 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20181025_125334   </td><td style=\"text-align: right;\">0.950518</td><td style=\"text-align: right;\"> 0.275586</td><td style=\"text-align: right;\">              0.114727</td><td style=\"text-align: right;\">0.286299</td><td style=\"text-align: right;\">0.0819671</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20181025_125334</td><td style=\"text-align: right;\">0.950518</td><td style=\"text-align: right;\"> 0.275586</td><td style=\"text-align: right;\">              0.114727</td><td style=\"text-align: right;\">0.286299</td><td style=\"text-align: right;\">0.0819671</td></tr>\n",
       "<tr><td>DRF_0_AutoML_20181025_125334                         </td><td style=\"text-align: right;\">0.943687</td><td style=\"text-align: right;\"> 0.36549 </td><td style=\"text-align: right;\">              0.120346</td><td style=\"text-align: right;\">0.324814</td><td style=\"text-align: right;\">0.105504 </td></tr>\n",
       "<tr><td>XRT_0_AutoML_20181025_125334                         </td><td style=\"text-align: right;\">0.943048</td><td style=\"text-align: right;\"> 0.370093</td><td style=\"text-align: right;\">              0.126594</td><td style=\"text-align: right;\">0.32873 </td><td style=\"text-align: right;\">0.108064 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181025_125334_model_0            </td><td style=\"text-align: right;\">0.874458</td><td style=\"text-align: right;\"> 0.483887</td><td style=\"text-align: right;\">              0.207944</td><td style=\"text-align: right;\">0.396537</td><td style=\"text-align: right;\">0.157242 </td></tr>\n",
       "<tr><td>GLM_grid_0_AutoML_20181025_125334_model_0            </td><td style=\"text-align: right;\">0.841046</td><td style=\"text-align: right;\"> 0.475734</td><td style=\"text-align: right;\">              0.242297</td><td style=\"text-align: right;\">0.399385</td><td style=\"text-align: right;\">0.159509 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The validation performance (auc) is  0.7172583833509649\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "[[5424   29]\n",
      " [ 255   26]] \n",
      "\n",
      "recall score is:  0.09252669039145907\n",
      "precision score is:  0.4727272727272727\n",
      "f1_score is:  0.15476190476190474\n",
      "accuracy score:  0.9504708754795954\n",
      "The final AUC after taking the best params and num_rounds when it stopped is 0.7173. \n",
      "\n",
      "Execution took: 0:01:20 secs (Wall clock time)\n",
      "\n",
      " {'decomp_feats': 6.0, 'encoder': 'he', 'eval_time': 1540452110.239333, 'feat_selection': 'false', 'h2o_automl_params': {'balance_classes': 'False', 'oversampling': 4.288486132803, 'undersampling': 0.7152016958868622}, 'kmeans_n': 3.0, 'miss_treatment': 'simple', 'sampler': {'choice': 'no'}, 'scaler': 'mm'} \n",
      "\n",
      "he encoding is happening ... \n",
      "\n",
      "category encoding completed \n",
      "\n",
      "encoding completed ... \n",
      "\n",
      "correlation analysis is happening ... \n",
      "\n",
      "correlation analysis completed ... \n",
      "\n",
      "missing value treatment completed ... \n",
      "\n",
      "STATUS REPORT \n",
      "\n",
      "(11362, 89)\n",
      "(5734, 89)\n",
      "(11362,)\n",
      "(5734,)\n",
      "Counter({0: 6824, 1: 4538})\n",
      "Counter({0: 5453, 1: 281})\n",
      "m is  3 \n",
      "\n",
      "input data shape is:  (11362, 120) \n",
      "\n",
      "output data shape is:  (11362, 88) \n",
      "\n",
      "no sampling done in this pipeline \n",
      "\n",
      "Checking whether there is an H2O instance running at http://localhost:54321. connected.\n",
      "Warning: Your H2O cluster version is too old (3 months and 14 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>19 hours 58 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Kolkata</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.20.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>3 months and 14 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_Lenovo_6jriv6</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.031 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         19 hours 58 mins\n",
       "H2O cluster timezone:       Asia/Kolkata\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.20.0.3\n",
       "H2O cluster version age:    3 months and 14 days !!!\n",
       "H2O cluster name:           H2O_from_python_Lenovo_6jriv6\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.031 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "AutoML progress: |████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                             </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_0_AutoML_20181025_125503   </td><td style=\"text-align: right;\">0.95043 </td><td style=\"text-align: right;\"> 0.27542 </td><td style=\"text-align: right;\">              0.111152</td><td style=\"text-align: right;\">0.285628</td><td style=\"text-align: right;\">0.0815836</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_0_AutoML_20181025_125503</td><td style=\"text-align: right;\">0.95043 </td><td style=\"text-align: right;\"> 0.27542 </td><td style=\"text-align: right;\">              0.111152</td><td style=\"text-align: right;\">0.285628</td><td style=\"text-align: right;\">0.0815836</td></tr>\n",
       "<tr><td>XRT_0_AutoML_20181025_125503                         </td><td style=\"text-align: right;\">0.941986</td><td style=\"text-align: right;\"> 0.364149</td><td style=\"text-align: right;\">              0.123185</td><td style=\"text-align: right;\">0.325229</td><td style=\"text-align: right;\">0.105774 </td></tr>\n",
       "<tr><td>DRF_0_AutoML_20181025_125503                         </td><td style=\"text-align: right;\">0.94196 </td><td style=\"text-align: right;\"> 0.362516</td><td style=\"text-align: right;\">              0.123897</td><td style=\"text-align: right;\">0.324062</td><td style=\"text-align: right;\">0.105016 </td></tr>\n",
       "<tr><td>GBM_grid_0_AutoML_20181025_125503_model_0            </td><td style=\"text-align: right;\">0.927655</td><td style=\"text-align: right;\"> 0.356135</td><td style=\"text-align: right;\">              0.142918</td><td style=\"text-align: right;\">0.331799</td><td style=\"text-align: right;\">0.110091 </td></tr>\n",
       "<tr><td>GLM_grid_0_AutoML_20181025_125503_model_0            </td><td style=\"text-align: right;\">0.841437</td><td style=\"text-align: right;\"> 0.474241</td><td style=\"text-align: right;\">              0.236762</td><td style=\"text-align: right;\">0.399462</td><td style=\"text-align: right;\">0.15957  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The validation performance (auc) is  0.7088298386796781\n",
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n",
      "[[5300  153]\n",
      " [ 221   60]] \n",
      "\n",
      "recall score is:  0.21352313167259787\n",
      "precision score is:  0.28169014084507044\n",
      "f1_score is:  0.242914979757085\n",
      "accuracy score:  0.9347750261597488\n",
      "The final AUC after taking the best params and num_rounds when it stopped is 0.7088. \n",
      "\n",
      "Execution took: 0:01:29 secs (Wall clock time)\n"
     ]
    }
   ],
   "source": [
    "trials = x.prepare(cols_to_remove=['global id', 'manager global id', 'personnel number manager', \n",
    "                                                  'short text of organizational unit', 'position text', \n",
    "                                                  'physical work location-description', 'manager position desc', \n",
    "                                   'costcenter description', 'local entity description', 'appraiser id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:38: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3284 2169]\n",
      " [  77  204]] \n",
      "\n",
      "recall score is:  0.7259786476868327\n",
      "precision score is:  0.08596713021491782\n",
      "f1_score is:  0.15373021853805574\n",
      "accuracy score:  0.608301360306941\n",
      "The final AUC after taking the best params and num_rounds when it stopped is 0.7393. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w1,y1,z1=h2o_automl.get_score(aml=x.aml_1, h2o_valid=x.h2o_valid_1, y_valid=x.y_valid, threshold=0.045)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP Class\n",
    "\n",
    "class nlp_feats(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return None\n",
    "    \n",
    "    def prepare(self, train_path = 'TRAIN.csv', test_path = 'VALID.csv'):\n",
    "        train=pd.read_csv(train_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        test=pd.read_csv(test_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        text_cols = ['short text of organizational unit', 'position text', 'opr change', 'm_opr change',\n",
    "                    'physical work location-description', 'physical work location-city',\n",
    "                    'manager position desc', 'costcenter description', 'global job', \n",
    "                    'local entity description', 'pers. subarea text', 'group']\n",
    "        train_text = train[text_cols]\n",
    "        test_text = test[text_cols]\n",
    "        train_text['string_all'] = train_text[text_cols].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "        test_text['string_all'] = test_text[text_cols].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "        train_text = train_text[['string_all']]\n",
    "        test_text = test_text[['string_all']]\n",
    "        y_train = train[['label']]\n",
    "        y_test = test[['label']]\n",
    "        return train_text, test_text, y_train, y_test\n",
    "    \n",
    "    def make_feats(self, train_text, test_text):\n",
    "        # nlp feats\n",
    "\n",
    "        ## HASHING ##\n",
    "        # create the transform\n",
    "        vectorizer = HashingVectorizer(n_features=500, ngram_range=(1,4), stop_words='english',\n",
    "                                       strip_accents='unicode', analyzer='char', norm='l1')\n",
    "        vectorizer.fit(train_text)\n",
    "        train_new1 = pd.DataFrame(vectorizer.transform(train_text.string_all).todense())\n",
    "        test_new1 = pd.DataFrame(vectorizer.transform(test_text.string_all).todense())\n",
    "\n",
    "        ## COUNTVECTORIZER ##\n",
    "        # create the transform\n",
    "        vectorizer = CountVectorizer(strip_accents='unicode', ngram_range=(1,5), stop_words='english', max_features=100)\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(train_text.string_all)\n",
    "        # summarize\n",
    "        #print(vectorizer.vocabulary_)\n",
    "        train_new2 = pd.DataFrame(vectorizer.transform(train_text.string_all).todense(), columns=vectorizer.get_feature_names())\n",
    "        test_new2 = pd.DataFrame(vectorizer.transform(test_text.string_all).todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "        ## TFIDVECTORIZER ##\n",
    "        # create the transform\n",
    "        vectorizer = TfidfVectorizer(strip_accents='unicode', ngram_range=(1,4), stop_words='english', max_features=100)\n",
    "        # tokenize and build vocab\n",
    "        vectorizer.fit(train_text.string_all)\n",
    "        train_new3 = pd.DataFrame(vectorizer.transform(train_text.string_all).todense(), columns=vectorizer.get_feature_names())\n",
    "        test_new3 = pd.DataFrame(vectorizer.transform(test_text.string_all).todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "        train_new = pd.concat([train_new1, train_new2, train_new3], ignore_index=True, axis=1)\n",
    "        test_new = pd.concat([test_new1, test_new2, test_new3], ignore_index=True, axis=1)\n",
    "        return train_new, test_new\n",
    "    \n",
    "    def model(self, train_new, test_new, y_train, y_valid, which='rf'):\n",
    "        if which == 'rf':\n",
    "            # rf classifier\n",
    "            clf=RandomForestClassifier(max_depth=15, n_estimators=300, random_state=1)\n",
    "            clf.fit(train_new, y_train)\n",
    "        elif which == 'sgd':\n",
    "            # sgd classifier\n",
    "            clf=sklearn.linear_model.SGDClassifier(loss='log', penalty='l1', random_state=6)\n",
    "            clf.fit(train_new, y_train)\n",
    "        elif which == 'xgb':\n",
    "            # xgb classifier\n",
    "            clf=xgb.XGBClassifier(learning_rate=0.02, n_estimators=100, colsample_bytree=0.9, \n",
    "                              subsample=0.9, scale_pos_weight=1, max_depth=10)\n",
    "            clf.fit(train_new, y_train)        \n",
    "        return clf\n",
    "    \n",
    "    def main(self, w1):\n",
    "        print('preparing ... \\n')\n",
    "        train_text, test_text, y_train, y_valid = self.prepare()\n",
    "        \n",
    "        print('making the features ... \\n')\n",
    "        train_new, test_new = self.make_feats(train_text, test_text)\n",
    "        \n",
    "        print('making the predictions ... \\n')\n",
    "        clf = self.model(train_new, test_new, y_train, y_valid)\n",
    "        \n",
    "        # get the h2o model predictions and make the simple weighted ensemble predictions\n",
    "        h2o_model_predictions = w1.as_data_frame()\n",
    "\n",
    "        return clf, test_new, y_valid, h2o_model_predictions\n",
    "    \n",
    "    def get_truncated_normal(self, mean=0, sd=1, low=0, upp=10):\n",
    "        return truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "    def best_thresh_score(self, yp, yt):\n",
    "        rc = recall_score(y_pred=yp, y_true=yt)\n",
    "        ac = accuracy_score(y_pred=yp, y_true=yt)\n",
    "        rc_ac_flag = 0\n",
    "        if (rc>0.6 and ac>0.6) : rc_ac_flag = 1\n",
    "        score = (rc_ac_flag)*(0.6*rc + 0.4*ac)\n",
    "        return score\n",
    "    \n",
    "    def opt_thresh(self, h2o_pred, nlp_pred, ens_pred):\n",
    "        X = self.get_truncated_normal(mean=0.1, sd=0.2, low=0, upp=0.4)\n",
    "        Y = list(X.rvs(1000))\n",
    "        \n",
    "        cols = ['thresh', 'recall', 'precision', 'f1', 'acc', 'score'] #score = (0.6*recall + 0.4*acc)\n",
    "        thresh_grid_h2o, thresh_grid_nlp, thresh_grid_ens = [], [], []\n",
    "\n",
    "        for i in Y:\n",
    "            # for h2o\n",
    "            h2o_predict=np.where(h2o_pred > i, 1, 0)\n",
    "            thresh_grid_h2o.append([i, recall_score(y_pred=h2o_predict, y_true=y_valid),\n",
    "                                precision_score(y_pred=h2o_predict, y_true=y_valid),\n",
    "                                f1_score(y_pred=h2o_predict, y_true=y_valid),\n",
    "                                accuracy_score(y_pred=h2o_predict, y_true=y_valid),\n",
    "                               self.best_thresh_score(yp=h2o_predict, yt=y_valid)])\n",
    "            \n",
    "            # for nlp\n",
    "            nlp_predict=np.where(nlp_pred > i, 1, 0)\n",
    "            thresh_grid_nlp.append([i, recall_score(y_pred=nlp_predict, y_true=y_valid),\n",
    "                                precision_score(y_pred=nlp_predict, y_true=y_valid),\n",
    "                                f1_score(y_pred=nlp_predict, y_true=y_valid),\n",
    "                                accuracy_score(y_pred=nlp_predict, y_true=y_valid),\n",
    "                               self.best_thresh_score(yp=nlp_predict, yt=y_valid)])\n",
    "            \n",
    "            # for ensemble\n",
    "            ens_predict=np.where(ens_pred > i, 1, 0)\n",
    "            thresh_grid_ens.append([i, recall_score(y_pred=ens_predict, y_true=y_valid),\n",
    "                                precision_score(y_pred=ens_predict, y_true=y_valid),\n",
    "                                f1_score(y_pred=ens_predict, y_true=y_valid),\n",
    "                                accuracy_score(y_pred=ens_predict, y_true=y_valid),\n",
    "                               self.best_thresh_score(yp=ens_predict, yt=y_valid)])\n",
    "            \n",
    "        thresh_grid_h2o = pd.DataFrame(thresh_grid_h2o, columns=cols)\n",
    "        thresh_grid_nlp = pd.DataFrame(thresh_grid_nlp, columns=cols)\n",
    "        thresh_grid_ens = pd.DataFrame(thresh_grid_ens, columns=cols)\n",
    "        \n",
    "        thresh_grid_h2o.sort_values(by='score', ascending=False, inplace=True)\n",
    "        thresh_grid_nlp.sort_values(by='score', ascending=False, inplace=True)\n",
    "        thresh_grid_ens.sort_values(by='score', ascending=False, inplace=True)\n",
    "\n",
    "        h2o_thresh = thresh_grid_h2o.reset_index(drop=True).iloc[0][0]\n",
    "        nlp_thresh = thresh_grid_nlp.reset_index(drop=True).iloc[0][0]\n",
    "        ens_thresh = thresh_grid_ens.reset_index(drop=True).iloc[0][0]\n",
    "        \n",
    "        return h2o_thresh, nlp_thresh, ens_thresh\n",
    "        \n",
    "    def predict(self, clf, test_new, y_valid, h2o_model_predictions, \n",
    "                h2o_thresh=0, nlp_thresh=0, ens_thresh=0, ens_weightage=0.9):\n",
    "        h2o_pred = h2o_model_predictions.p1\n",
    "        nlp_pred = clf.predict_proba(test_new)[:,1]\n",
    "        ens_pred = (((1 - ens_weightage) * nlp_pred) + (ens_weightage * h2o_pred))\n",
    "        \n",
    "        if (h2o_thresh==0 and nlp_thresh==0 and ens_thresh==0):\n",
    "            h2o_thresh, nlp_thresh, ens_thresh = self.opt_thresh(h2o_pred, nlp_pred, ens_pred)\n",
    "        \n",
    "        # h2o predictions summary\n",
    "        print('h2o predictions summary \\n')\n",
    "        h2o_predict=np.where(h2o_pred > h2o_thresh, 1, 0)\n",
    "        print('h2o model auc is: ', sklearn.metrics.roc_auc_score(y_score=h2o_pred, y_true=y_valid), '\\n')\n",
    "        print('h2o model recall is: ', sklearn.metrics.recall_score(y_pred=h2o_predict, y_true=y_valid), '\\n')\n",
    "        print('h2o model accuracy is: ', sklearn.metrics.accuracy_score(y_pred=h2o_predict, y_true=y_valid), '\\n')\n",
    "        print('h2o model precision is: ', sklearn.metrics.precision_score(y_pred=h2o_predict, y_true=y_valid), '\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        # nlp predictions summary\n",
    "        print('nlp predictions summary \\n')\n",
    "        nlp_predict=np.where(nlp_pred > nlp_thresh, 1, 0)\n",
    "        print('nlp model auc is: ', sklearn.metrics.roc_auc_score(y_score=nlp_pred, y_true=y_valid), '\\n')\n",
    "        print('nlp model recall is: ', sklearn.metrics.recall_score(y_pred=nlp_predict, y_true=y_valid), '\\n')\n",
    "        print('nlp model accuracy is: ', sklearn.metrics.accuracy_score(y_pred=nlp_predict, y_true=y_valid), '\\n')\n",
    "        print('nlp model precision is: ', sklearn.metrics.precision_score(y_pred=nlp_predict, y_true=y_valid), '\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        # ensemble predictions summary\n",
    "        print('ensemble predictions summary \\n')\n",
    "        ens_predict=np.where(ens_pred > ens_thresh, 1, 0)\n",
    "        print('ensemble model auc is: ', sklearn.metrics.roc_auc_score(y_score=ens_pred, y_true=y_valid), '\\n')\n",
    "        print('ensemble model recall is: ', sklearn.metrics.recall_score(y_pred=ens_predict, y_true=y_valid), '\\n')\n",
    "        print('ensemble model accuracy is: ', sklearn.metrics.accuracy_score(y_pred=ens_predict, y_true=y_valid), '\\n')\n",
    "        print('ensemble model precision is: ', sklearn.metrics.precision_score(y_pred=ens_predict, y_true=y_valid), '\\n')\n",
    "        \n",
    "        return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=nlp_feats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making the features ... \n",
      "\n",
      "making the predictions ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:62: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "clf, test_new, y_valid, h2o_model_predictions = y.main(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2o predictions summary \n",
      "\n",
      "h2o model auc is:  0.7393259644206428 \n",
      "\n",
      "h2o model recall is:  0.7259786476868327 \n",
      "\n",
      "h2o model accuracy is:  0.6220788280432508 \n",
      "\n",
      "h2o model precision is:  0.08892763731473409 \n",
      "\n",
      "\n",
      "\n",
      "nlp predictions summary \n",
      "\n",
      "nlp model auc is:  0.6951663291550636 \n",
      "\n",
      "nlp model recall is:  0.6441281138790036 \n",
      "\n",
      "nlp model accuracy is:  0.6194628531566096 \n",
      "\n",
      "nlp model precision is:  0.07998232434821034 \n",
      "\n",
      "\n",
      "\n",
      "ensemble predictions summary \n",
      "\n",
      "ensemble model auc is:  0.7388247547955906 \n",
      "\n",
      "ensemble model recall is:  0.7330960854092526 \n",
      "\n",
      "ensemble model accuracy is:  0.6014998256016743 \n",
      "\n",
      "ensemble model precision is:  0.08526490066225166 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "ens_pred = y.predict(clf, test_new, y_valid, h2o_model_predictions, ens_weightage=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataFrameImputer at 0x258281ca160>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 10, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 28, 29, 30, 31, 35, 36, 37, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█████▏                                                                             | 2/32 [00:00<00:03,  9.26it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████▊                                                                           | 3/32 [00:00<00:03,  9.49it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▉                                                                      | 5/32 [00:00<00:02, 10.59it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██████████████████▏                                                                | 7/32 [00:00<00:02, 11.01it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████▎                                                           | 9/32 [00:00<00:02, 11.42it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|████████████████████████████▏                                                     | 11/32 [00:00<00:01, 11.55it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████████████████████▎                                                | 13/32 [00:01<00:01, 11.82it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████████████████████████▍                                           | 15/32 [00:01<00:01, 11.83it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|███████████████████████████████████████████▌                                      | 17/32 [00:01<00:01, 11.94it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|████████████████████████████████████████████████▋                                 | 19/32 [00:01<00:01, 11.96it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████████████████████████████████▊                            | 21/32 [00:01<00:00, 11.99it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████████████████████████████████████▉                       | 23/32 [00:01<00:00, 12.03it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|████████████████████████████████████████████████████████████████                  | 25/32 [00:02<00:00, 12.08it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|█████████████████████████████████████████████████████████████████████▏            | 27/32 [00:02<00:00, 12.15it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████████████████████▎       | 29/32 [00:02<00:00, 12.17it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████████▍  | 31/32 [00:02<00:00, 12.17it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 12.20it/s]\n"
     ]
    }
   ],
   "source": [
    "## create the interpreter dataframe with the features to be attributed to :\n",
    "\n",
    "train_df = pd.read_csv('TRAIN.csv', na_values=['NO DATA', '', ' ', 'UNKNOWN'])\n",
    "valid_for_interpretation = pd.read_csv('VALID.csv', na_values=['NO DATA', '', ' ', 'UNKNOWN'])\n",
    "interpreter_cols = ['pay scale group', 'service years', 'age', 'group',\n",
    "                    'manager service years', 'manager age', 'age diff', 'manager position',\n",
    "                    'ebm level of the job', 'months in position', \n",
    "                    'manager pay scale', 'psg diff', 'opr_1', 'opr_2', 'opr_3', 'opr change', 'm_opr_1', 'm_opr_2', \n",
    "                    'm_opr_3', 'm_opr change', 'fc_recent', 'lc_recent', 'cr', 'no of positions', \n",
    "                    'no of band changes', 'time in band', 'label']\n",
    "train_df = train_df[interpreter_cols]\n",
    "valid_for_interpretation = valid_for_interpretation[interpreter_cols]\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_valid = valid_for_interpretation['label']\n",
    "train_df.drop(['label'], inplace=True, axis=1)\n",
    "valid_for_interpretation.drop(['label'], inplace=True, axis=1)\n",
    "\n",
    "DFI = DataFrameImputer()\n",
    "DFI.fit(train_df)\n",
    "train_df = DFI.transform(train_df)\n",
    "valid_for_interpretation = DFI.transform(valid_for_interpretation)\n",
    "\n",
    "feat_names = train_df.columns\n",
    "cat_columns = train_df.select_dtypes(include=['object']).columns.values\n",
    "x = list(train_df.dtypes)\n",
    "x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "print(categorical_idx)\n",
    "\n",
    "categorical_names = {}\n",
    "for feature in tqdm(cat_columns):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(train_df[feature].astype(str))\n",
    "    train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "    valid_for_interpretation[feature] = valid_for_interpretation[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "    le_classes = le.classes_.tolist()\n",
    "    bisect.insort_left(le_classes, 'No Data')\n",
    "    le.classes_ = le_classes\n",
    "    valid_for_interpretation[feature] = le.transform(valid_for_interpretation[feature].astype(str))\n",
    "    categorical_names[feature] = le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df)\n",
    "train_df = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns.values)\n",
    "valid_for_interpretation = pd.DataFrame(scaler.transform(valid_for_interpretation), \n",
    "                                        columns=valid_for_interpretation.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=9, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=1, silent=True,\n",
       "       subsample=1)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7204725205949514"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_model=xgb.XGBClassifier(seed=1, max_depth=9)\n",
    "lime_model.fit(train_df, y_train)\n",
    "\n",
    "lime_model_pred = lime_model.predict_proba(valid_for_interpretation)\n",
    "roc_auc_score(y_score=lime_model_pred[:,1], y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6168468782699686"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.5836298932384342"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.07308377896613191"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_pred=np.where(lime_model_pred[:, 1] > 0.4, 1, 0)\n",
    "accuracy_score(y_pred=lime_pred, y_true=y_valid)\n",
    "recall_score(y_pred=lime_pred, y_true=y_valid)\n",
    "precision_score(y_pred=lime_pred, y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25828e67518>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(booster=lime_model, max_num_features=20, show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=1, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6462533601602305"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_model=LogisticRegression(C=100, penalty='l1', random_state=1)\n",
    "lime_model.fit(train_df, y_train)\n",
    "\n",
    "lime_model_pred = lime_model.predict_proba(valid_for_interpretation)\n",
    "roc_auc_score(y_score=lime_model_pred[:,1], y_true=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_model_coeffs = pd.DataFrame({'columns': train_df.columns,\n",
    "                                 'beta': lime_model.coef_.T.ravel()})\n",
    "lime_model_coeffs['rank'] = lime_model_coeffs['beta'].rank(method='dense', ascending=False).astype('int')\n",
    "lime_model_coeffs.sort_values('rank', ascending=True, inplace=True)\n",
    "lime_model_coeffs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>beta</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ebm level of the job</td>\n",
       "      <td>0.322586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cr_percentage_team_x</td>\n",
       "      <td>0.310930</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as_min</td>\n",
       "      <td>0.304760</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>group</td>\n",
       "      <td>0.266390</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bt_percentage_team_x</td>\n",
       "      <td>0.254814</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>manager age</td>\n",
       "      <td>0.234963</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gt_percentage_team_y</td>\n",
       "      <td>0.179876</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bt_max</td>\n",
       "      <td>0.172255</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>as_max</td>\n",
       "      <td>0.140559</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>months in position</td>\n",
       "      <td>0.137823</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cr_percentage</td>\n",
       "      <td>0.128560</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bonus_comparison_team_y</td>\n",
       "      <td>0.124590</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>as_s</td>\n",
       "      <td>0.116717</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>incentive flag</td>\n",
       "      <td>0.109174</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>target by me_band</td>\n",
       "      <td>0.090810</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>g_total_comparison</td>\n",
       "      <td>0.090523</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bt_s</td>\n",
       "      <td>0.084981</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>net target</td>\n",
       "      <td>0.068453</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>manager position</td>\n",
       "      <td>0.063813</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gt_skew</td>\n",
       "      <td>0.057885</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>target by macro entity</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>cr_team_y</td>\n",
       "      <td>0.054381</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>target by functional area</td>\n",
       "      <td>0.053793</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>target by cc_band</td>\n",
       "      <td>0.053170</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>target by band</td>\n",
       "      <td>0.045066</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>no of band changes</td>\n",
       "      <td>0.044459</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>time in band</td>\n",
       "      <td>0.035159</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>count_sal_team</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>m no of band changes</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>as_m</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>m_opr_1</td>\n",
       "      <td>0.012473</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>target by org id</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cr_team_x</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>target by oid_band</td>\n",
       "      <td>0.009431</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>bonus total</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>m_opr_2</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>target by aid_band</td>\n",
       "      <td>0.007963</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>g_total_comparison_team_y</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>target by cost center</td>\n",
       "      <td>-0.008831</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>grand total</td>\n",
       "      <td>-0.010648</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>m_opr_3</td>\n",
       "      <td>-0.010654</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>manager service years</td>\n",
       "      <td>-0.015341</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>lc_recent</td>\n",
       "      <td>-0.020283</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>g_total_comparison_team_x</td>\n",
       "      <td>-0.023417</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>bonus_comparison</td>\n",
       "      <td>-0.024562</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>cr</td>\n",
       "      <td>-0.029671</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>annual salary</td>\n",
       "      <td>-0.029703</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>m time in band</td>\n",
       "      <td>-0.033534</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>bt_skew</td>\n",
       "      <td>-0.034802</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>bonus_comparison_team_x</td>\n",
       "      <td>-0.045082</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>target by appraiser</td>\n",
       "      <td>-0.045916</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>opr_2</td>\n",
       "      <td>-0.047029</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>bt_percentage_team_y</td>\n",
       "      <td>-0.047212</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>psg diff</td>\n",
       "      <td>-0.047612</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>no of positions</td>\n",
       "      <td>-0.050785</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>target by fa_band</td>\n",
       "      <td>-0.057249</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>opr_3</td>\n",
       "      <td>-0.066020</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>gt_min</td>\n",
       "      <td>-0.072057</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>bt_min</td>\n",
       "      <td>-0.072642</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>bt_percentage</td>\n",
       "      <td>-0.072826</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>opr_1</td>\n",
       "      <td>-0.074346</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>as_skew</td>\n",
       "      <td>-0.083750</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>fc_recent</td>\n",
       "      <td>-0.084682</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>appraiser id</td>\n",
       "      <td>-0.086126</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>cr_percentage_team_y</td>\n",
       "      <td>-0.093122</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>gt_m</td>\n",
       "      <td>-0.095753</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>pay scale group</td>\n",
       "      <td>-0.107307</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>m_opr change</td>\n",
       "      <td>-0.112255</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>manager pay scale</td>\n",
       "      <td>-0.114854</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>opr change</td>\n",
       "      <td>-0.141334</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>gt_s</td>\n",
       "      <td>-0.154918</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>gt_max</td>\n",
       "      <td>-0.176928</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>bt_m</td>\n",
       "      <td>-0.186044</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>gt_percentage</td>\n",
       "      <td>-0.237956</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>age diff</td>\n",
       "      <td>-0.300117</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>gt_percentage_team_x</td>\n",
       "      <td>-0.318002</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>age</td>\n",
       "      <td>-0.417880</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>service years</td>\n",
       "      <td>-0.547133</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      columns      beta  rank\n",
       "0        ebm level of the job  0.322586     1\n",
       "1        cr_percentage_team_x  0.310930     2\n",
       "2                      as_min  0.304760     3\n",
       "3                       group  0.266390     4\n",
       "4        bt_percentage_team_x  0.254814     5\n",
       "5                 manager age  0.234963     6\n",
       "6        gt_percentage_team_y  0.179876     7\n",
       "7                      bt_max  0.172255     8\n",
       "8                      as_max  0.140559     9\n",
       "9          months in position  0.137823    10\n",
       "10              cr_percentage  0.128560    11\n",
       "11    bonus_comparison_team_y  0.124590    12\n",
       "12                       as_s  0.116717    13\n",
       "13             incentive flag  0.109174    14\n",
       "14          target by me_band  0.090810    15\n",
       "15         g_total_comparison  0.090523    16\n",
       "16                       bt_s  0.084981    17\n",
       "17                 net target  0.068453    18\n",
       "18           manager position  0.063813    19\n",
       "19                    gt_skew  0.057885    20\n",
       "20     target by macro entity  0.054667    21\n",
       "21                  cr_team_y  0.054381    22\n",
       "22  target by functional area  0.053793    23\n",
       "23          target by cc_band  0.053170    24\n",
       "24             target by band  0.045066    25\n",
       "25         no of band changes  0.044459    26\n",
       "26               time in band  0.035159    27\n",
       "27             count_sal_team  0.028606    28\n",
       "28       m no of band changes  0.022711    29\n",
       "29                       as_m  0.020005    30\n",
       "30                    m_opr_1  0.012473    31\n",
       "31           target by org id  0.011892    32\n",
       "32                  cr_team_x  0.010446    33\n",
       "33         target by oid_band  0.009431    34\n",
       "34                bonus total  0.009247    35\n",
       "35                    m_opr_2  0.009155    36\n",
       "36         target by aid_band  0.007963    37\n",
       "37  g_total_comparison_team_y -0.005149    38\n",
       "38      target by cost center -0.008831    39\n",
       "39                grand total -0.010648    40\n",
       "40                    m_opr_3 -0.010654    41\n",
       "41      manager service years -0.015341    42\n",
       "42                  lc_recent -0.020283    43\n",
       "43  g_total_comparison_team_x -0.023417    44\n",
       "44           bonus_comparison -0.024562    45\n",
       "45                         cr -0.029671    46\n",
       "46              annual salary -0.029703    47\n",
       "47             m time in band -0.033534    48\n",
       "48                    bt_skew -0.034802    49\n",
       "49    bonus_comparison_team_x -0.045082    50\n",
       "50        target by appraiser -0.045916    51\n",
       "51                      opr_2 -0.047029    52\n",
       "52       bt_percentage_team_y -0.047212    53\n",
       "53                   psg diff -0.047612    54\n",
       "54            no of positions -0.050785    55\n",
       "55          target by fa_band -0.057249    56\n",
       "56                      opr_3 -0.066020    57\n",
       "57                     gt_min -0.072057    58\n",
       "58                     bt_min -0.072642    59\n",
       "59              bt_percentage -0.072826    60\n",
       "60                      opr_1 -0.074346    61\n",
       "61                    as_skew -0.083750    62\n",
       "62                  fc_recent -0.084682    63\n",
       "63               appraiser id -0.086126    64\n",
       "64       cr_percentage_team_y -0.093122    65\n",
       "65                       gt_m -0.095753    66\n",
       "66            pay scale group -0.107307    67\n",
       "67               m_opr change -0.112255    68\n",
       "68          manager pay scale -0.114854    69\n",
       "69                 opr change -0.141334    70\n",
       "70                       gt_s -0.154918    71\n",
       "71                     gt_max -0.176928    72\n",
       "72                       bt_m -0.186044    73\n",
       "73              gt_percentage -0.237956    74\n",
       "74                   age diff -0.300117    75\n",
       "75       gt_percentage_team_x -0.318002    76\n",
       "76                        age -0.417880    77\n",
       "77              service years -0.547133    78"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_model_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_row = pd.DataFrame({'columns': valid_for_interpretation.columns,\n",
    "                          'value': valid_for_interpretation.loc[1744][:]}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_model_coeffs_1 = pd.merge(lime_model_coeffs, valid_row)\n",
    "lime_model_coeffs_1['attribution'] = lime_model_coeffs_1['beta']*lime_model_coeffs_1['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_probs = pd.DataFrame({'probs': lime_model_pred[:, 1],\n",
    "                           'actuals': y_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.319454674085863"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lime_model_coeffs_1.attribution.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return interpretation across methods (specify which or All)\n",
    "\n",
    "class model_interpret():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module takes as input the model and train/test datasets to generate interpretations of the\n",
    "        predictions generated by the model\n",
    "        LIME and SHAP methods have been added as a provision currently, treeinterpreter will be added later\n",
    "        \"\"\"\n",
    "    \n",
    "    def lime_interpreter(feat_names, classnames, categindices, categnames, \n",
    "                         kw, num_feature, train, test, n):\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(training_data = train.values,\n",
    "                                                   feature_names = list(feat_names),\n",
    "                                                   class_names = classnames,\n",
    "                                                   categorical_features=categindices, \n",
    "                                                   categorical_names=categnames, kernel_width = kw)\n",
    "        xtest = test.values\n",
    "        exp = explainer.explain_instance(xtest[n], model.predict_proba, num_features = num_feature)\n",
    "        return exp.show_in_notebook()\n",
    "    \n",
    "    def shap_interpreter(model, train, test, n, method = 'tree'):\n",
    "        \"\"\" specify n as the prediction/observation you want the interpretation to be returned for \"\"\"\n",
    "        \n",
    "        if method == 'tree':\n",
    "            # create our SHAP explainer\n",
    "            shap_explainer = shap.TreeExplainer(model)\n",
    "            # calculate the shapley values for our test set\n",
    "            shap_values = shap_explainer.shap_values(test.values)\n",
    "        elif method == 'kernel':\n",
    "            # create our SHAP explainer\n",
    "            shap_explainer = shap.KernelExplainer(model.predict_proba, shap.kmeans(train[:100], 5))\n",
    "            shap_values = shap_explainer.shap_values(test.values)\n",
    "            \n",
    "        # load JS in order to use some of the plotting functions from the shap package in the notebook\n",
    "        shap.initjs()\n",
    "        \n",
    "        # plot the explanation for a single prediction\n",
    "        return shap.force_plot(shap_values[n, :], test.iloc[n, :])\n",
    "    \n",
    "    def model_interpreter(interpreter_algo, train, test, shap_method = 'tree', kw = 3, n = 0, model = None,\n",
    "                          feat_names = None, classnames = None,\n",
    "                          categindices = None, categnames = None, num_feature = None):\n",
    "        if interpreter_algo == 'lime':\n",
    "            return model_interpret.lime_interpreter(feat_names, classnames, categindices, categnames,\n",
    "                                                    kw, num_feature, train, test, n)\n",
    "        elif interpreter_algo == 'shap':\n",
    "            return model_interpret.shap_interpreter(model, train, test, n, method = shap_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_interpret.model_interpreter(interpreter_algo='lime', model = model, train = X_train, test = X_test, feat_names = feat_names,\n",
    "                                  classnames = ['not delayed', 'delayed'], categindices = categ_idx, categnames = categ_names,\n",
    "                                 num_feature = 5, n=0)"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
