{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '.svn', 'archive', 'branches', 'FEATURE_ENGINEERING.ipynb', 'Missing Data Imputation.ipynb', 'MODEL INTERPRETER.ipynb', 'MODEL_SELECTION_TUNING_TEST_2017Dec_model3.ipynb', 'MODEL_TOP_FEATURES_DISTRIBUTION_Graphs.ipynb', 'OTHER_MODELS.ipynb', 'preparation', 'PREPARATION.ipynb', 'salary_bonus_2017.csv', 'snippets', 'tags', 'test2.csv', 'test3.csv', 'test_final.csv', 'train_dec2017.csv', 'train_final.csv', 'train_june2017.csv', 'trunk', 'Turnover2018tillapril.xlsx', 'valid_final.csv', 'wrong', 'X_train.csv', 'X_valid.csv']\n"
     ]
    }
   ],
   "source": [
    "## importing the relevant packages:\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# print/display all plots inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# the base packages\n",
    "import collections # for the Counter function\n",
    "import csv # for reading/writing csv files\n",
    "import pandas as pd, numpy as np, time, gc, bisect\n",
    "\n",
    "# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing, decomposition\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# modelling algorithms\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Exporting packages for SHAP/LIME\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# missing value imputation\n",
    "from fancyimpute import KNN, MICE #, NuclearNormMinimization\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 10 # number of iterations/parameter sets created towards tuning\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAIN CLASSES ####\n",
    "\n",
    "## Two defined for now ##\n",
    "# 1. DataFrame Imputer\n",
    "#    - for imputing missing values\n",
    "# 2. Prepare Data\n",
    "#    - for sourcing, processing, and returning the train/test datasets\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X.groupby(['pay scale group', 'abinbev entity2'])\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        X.groupby('abinbev entity2')\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "    def num_missing(self):\n",
    "        return sum(self.isnull())\n",
    "    \n",
    "    def imputer_mean(self, column):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def imputer_median(self, column):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def imputer_mode(self, column):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def fancy_impute(X, which_method):\n",
    "        \"\"\" currently supported algorithms are KNN, NNM and MICE from the fancyimpute package\n",
    "        which_method = ['KNN', 'NNM', 'MICE']\n",
    "        \"\"\"\n",
    "        if which_method == 'NNM': X = NuclearNormMinimization().complete(X) # NNM method\n",
    "        if which_method == 'KNN': X = KNN(k=7, verbose=False).complete(X) # KNN method\n",
    "        if which_method == 'MICE':\n",
    "            X_complete_df = X.copy()\n",
    "            mice = MICE(verbose=False)\n",
    "            X_complete = mice.complete(np.asarray(X.values, dtype=float))\n",
    "            X_complete_df.loc[:, X.columns] = X_complete[:][:]\n",
    "            X = X_complete_df\n",
    "        return X\n",
    "\n",
    "class prepare_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" To prepare data,\n",
    "                1. read in data\n",
    "                2. pre-processing/cleaning\n",
    "                3. creating helper objects for later steps\n",
    "                4. processing for modelling\n",
    "                5. function return objects are the train, valid, response, categ cols/indices, feature names\n",
    "        \"\"\"\n",
    "    \n",
    "    def labelEncoder(train_df, valid_df, cat_columns, test_df = None):\n",
    "        categorical_names = {}\n",
    "        for feature in tqdm(cat_columns):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(train_df[feature].astype(str))\n",
    "            train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "            if test_df is not None : test_df[feature] = test_df[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "            valid_df[feature] = valid_df[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "            le_classes = le.classes_.tolist()\n",
    "            bisect.insort_left(le_classes, 'No Data')\n",
    "            le.classes_ = le_classes\n",
    "            if test_df is not None : test_df[feature] = le.transform(test_df[feature].astype(str))\n",
    "            valid_df[feature] = le.transform(valid_df[feature].astype(str))\n",
    "            categorical_names[feature] = le.classes_\n",
    "        if test_df is not None :\n",
    "            return train_df, test_df, valid_df, categorical_names\n",
    "        if test_df is None :\n",
    "            return train_df, valid_df, categorical_names\n",
    "    \n",
    "    ## function to get frequency count of elements in a vector/list\n",
    "    def freq_count(input_vector):\n",
    "        return collections.Counter(input_vector)\n",
    "    \n",
    "    def categ_feats(train_df, valid_df, test_df = None):\n",
    "        x = list(train_df.dtypes)\n",
    "        x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "        categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "\n",
    "        # Get feature names and their values for categorical data (needed for LIME)\n",
    "        cat_columns = train_df.select_dtypes(include=['object']).columns.values\n",
    "        \n",
    "        if test_df is not None:\n",
    "            train_df, test_df, valid_df, categorical_names = prepare_data.labelEncoder(train_df, valid_df, cat_columns, test_df)\n",
    "            return train_df, test_df, valid_df, categorical_names, categorical_idx\n",
    "        elif test_df is None:\n",
    "            train_df, valid_df, categorical_names = prepare_data.labelEncoder(train_df, valid_df, cat_columns)\n",
    "            return train_df, valid_df, categorical_names, categorical_idx\n",
    "\n",
    "    def create(input_file_path, input_file_path_2, response, cols_to_remove = ['id'], random_seed = 1):\n",
    "        train = pd.read_csv(input_file_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        test = pd.read_csv(input_file_path_2, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        \n",
    "        train = pd.DataFrame(train)\n",
    "        test = pd.DataFrame(test)\n",
    "        \n",
    "        train.drop(cols_to_remove, axis = 1, inplace = True)\n",
    "        test = pd.DataFrame(data = test[train.columns])\n",
    "        \n",
    "        print(train.shape, '\\n')\n",
    "        train.dropna(thresh=0.5*(train.shape[0]), axis=1, inplace = True)\n",
    "        train.dropna(thresh=0.4*(train.shape[1]), axis=0, inplace = True)\n",
    "        print(train.shape, '\\n')\n",
    "        test = test[train.columns]\n",
    "        test.dropna(thresh=0.5*(test.shape[0]), axis=1, inplace = True)\n",
    "        train = train[test.columns]\n",
    "        \n",
    "        # calling the missing value imputation function\n",
    "        #print(train.apply(DataFrameImputer.num_missing, axis=0), '\\n')\n",
    "        imputer_object = DataFrameImputer()\n",
    "        imputer_object.fit(train)\n",
    "        train = imputer_object.transform(train)\n",
    "        test = imputer_object.transform(test)\n",
    "        \n",
    "        print(prepare_data.freq_count(train[response]), '\\n')\n",
    "\n",
    "        # shuffle the dataframes so that the training is done in a random order.\n",
    "        train = shuffle(train)\n",
    "        test = shuffle(test)\n",
    "        \n",
    "        # creating the response vector\n",
    "        y_train = train[response].values\n",
    "        X_train = train.drop([response], axis = 1)\n",
    "        y_valid = test[response].values\n",
    "        X_valid = test.drop([response], axis = 1)\n",
    "        \n",
    "        ##  segment for usage if doing the train/test split ##\n",
    "        ##  not essential if doing tuning using cross-validation ##\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = random_seed)\n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        X_valid = pd.DataFrame(X_valid)\n",
    "        \n",
    "        X_train, X_test, X_valid, categ_names, categ_idx = prepare_data.categ_feats(X_train, X_test, X_valid)\n",
    "        \n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        X_train = pd.DataFrame(data=X_train, columns=X_train.columns.values)\n",
    "        X_test = pd.DataFrame(data=X_test, columns=X_test.columns.values)\n",
    "        X_valid = pd.DataFrame(data=X_valid, columns=X_valid.columns.values)\n",
    "        \n",
    "        return X_train, X_test, X_valid, y_train, y_test, y_valid, categ_names, categ_idx\n",
    "    \n",
    "    def create_without_split(input_file_path, input_file_path_2, response, cols_to_remove = ['id'], random_seed = 1):\n",
    "        train = pd.read_csv(input_file_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        test = pd.read_csv(input_file_path_2, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        \n",
    "        train = pd.DataFrame(train)\n",
    "        test = pd.DataFrame(test)\n",
    "        \n",
    "        train.drop(cols_to_remove, axis = 1, inplace = True)\n",
    "        test = pd.DataFrame(data = test[train.columns])\n",
    "        \n",
    "        print(train.shape, '\\n')\n",
    "        train.dropna(thresh=0.5*(train.shape[0]), axis=1, inplace = True)\n",
    "        train.dropna(thresh=0.4*(train.shape[1]), axis=0, inplace = True)\n",
    "        print(train.shape, '\\n')\n",
    "        test = test[train.columns]\n",
    "        test.dropna(thresh=0.5*(test.shape[0]), axis=1, inplace = True)\n",
    "        train = train[test.columns]\n",
    "                \n",
    "        # calling the missing value imputation function\n",
    "        #print(train.apply(DataFrameImputer.num_missing, axis=0), '\\n')\n",
    "#         imputer_object = DataFrameImputer()\n",
    "#         imputer_object.fit(train)\n",
    "#         train = imputer_object.transform(train)\n",
    "#         test = imputer_object.transform(test)\n",
    "        \n",
    "        print(prepare_data.freq_count(train[response]), '\\n')\n",
    "\n",
    "        # shuffle the dataframes so that the training is done in a random order.\n",
    "        train = shuffle(train)\n",
    "        test = shuffle(test)\n",
    "        \n",
    "        # creating the response vector\n",
    "        y_train = train[response].values\n",
    "        X_train = train.drop([response], axis = 1)\n",
    "        y_valid = test[response].values\n",
    "        X_valid = test.drop([response], axis = 1)\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_valid = pd.DataFrame(X_valid)\n",
    "        \n",
    "        X_train, X_valid, categ_names, categ_idx = prepare_data.categ_feats(X_train, X_valid)\n",
    "        \n",
    "        feat_names = X_train.columns.values\n",
    "        feat_names2 = X_valid.columns.values\n",
    "\n",
    "        X_train = DataFrameImputer.fancy_impute(X_train, which_method='MICE')\n",
    "        X_valid = DataFrameImputer.fancy_impute(X_valid, which_method='MICE')\n",
    "        \n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        X_train = pd.DataFrame(data=X_train, columns=feat_names)\n",
    "        X_valid = pd.DataFrame(data=X_valid, columns=feat_names2)\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6102, 77) \n",
      "\n",
      "(6085, 75) \n",
      "\n",
      "Counter({0: 5796, 1: 289}) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 41/41 [00:02<00:00, 18.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# create data function call\n",
    "# CV approach\n",
    "# train and valid features/response dataframes returned\n",
    "# categorical column names/indices and all feature names also returned\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names = prepare_data.create_without_split(input_file_path='train_final.csv',\n",
    "                                                                  input_file_path_2='test_final.csv', response = 'label',\n",
    "                                cols_to_remove = ['global id', 'original hire date', 'original id', 'pers. subarea text',\n",
    "                                                  'manager global id', 'personnel number manager', \n",
    "                                                  'short text of organizational unit', 'position text', \n",
    "                                                  'physical work location-description', 'physical work location-city',\n",
    "                                                  'position start date', 'manager position desc', 'costcenter description',\n",
    "                                                  'local entity description', 'appraiser id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6085, 74)\n",
      "(5897, 74)\n",
      "(6085,)\n",
      "(5897,)\n",
      "Counter({0: 5796, 1: 289})\n",
      "Counter({0: 5616, 1: 281})\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(collections.Counter(y_train))\n",
    "#print(collections.Counter(y_test))\n",
    "print(collections.Counter(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING MODULE\n",
    "\n",
    "1. PCA\n",
    "2. ICA\n",
    "3. tSVD\n",
    "4. GRP\n",
    "5. SRP\n",
    "6. Binning\n",
    "7. Deviation Encoding features\n",
    "8. Salary related features\n",
    "9. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feat_eng():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module contains several functions for creating new features. find below a brief description of each \"\"\"\n",
    "    \n",
    "    def scalers(train, valid, which_method):\n",
    "        if which_method == 'ss':\n",
    "            sc = StandardScaler()\n",
    "            sc.fit(train)\n",
    "            train = pd.DataFrame(sc.transform(train))\n",
    "            valid = pd.DataFrame(sc.transform(valid))\n",
    "            return train, valid # scale all variables to zero mean and unit variance, required for PCA and related\n",
    "        if which_method == 'mm':\n",
    "            mm = MinMaxScaler()\n",
    "            mm.fit(train)\n",
    "            train = pd.DataFrame(mm.transform(train))\n",
    "            valid = pd.DataFrame(mm.transform(valid))\n",
    "            return train, valid # use this method to iterate\n",
    "        \n",
    "    def pca_feats(train, valid, n = .95):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='mm')\n",
    "            pca_fit = decomposition.PCA(n_components=n)\n",
    "            pca_fit.fit(train)\n",
    "            pca_train = pd.DataFrame(pca_fit.transform(train))\n",
    "            pca_valid = pd.DataFrame(pca_fit.transform(valid))\n",
    "            pca_cols = list(set(list(pca_train)))\n",
    "            pca_cols = ['pca_' + str(s) for s in pca_cols]\n",
    "            pca_train.columns = pca_cols\n",
    "            pca_valid.columns = pca_cols\n",
    "            return pca_train, pca_valid\n",
    "        \n",
    "    def ica_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='mm')\n",
    "            ica_fit = decomposition.FastICA(n_components=n)\n",
    "            ica_fit.fit(train)\n",
    "            ica_train = pd.DataFrame(ica_fit.transform(train))\n",
    "            ica_valid = pd.DataFrame(ica_fit.transform(valid))\n",
    "            ica_cols = list(set(list(ica_train)))\n",
    "            ica_cols = ['ica_' + str(s) for s in ica_cols]\n",
    "            ica_train.columns = ica_cols\n",
    "            ica_valid.columns = ica_cols\n",
    "            return ica_train, ica_valid\n",
    "        \n",
    "    def tsvd_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='mm')\n",
    "            tsvd_fit = decomposition.TruncatedSVD(n_components=n)\n",
    "            tsvd_fit.fit(train)\n",
    "            tsvd_train = pd.DataFrame(tsvd_fit.transform(train))\n",
    "            tsvd_valid = pd.DataFrame(tsvd_fit.transform(valid))\n",
    "            tsvd_cols = list(set(list(tsvd_train)))\n",
    "            tsvd_cols = ['tsvd_' + str(s) for s in tsvd_cols]\n",
    "            tsvd_train.columns = tsvd_cols\n",
    "            tsvd_valid.columns = tsvd_cols\n",
    "            return tsvd_train, tsvd_valid\n",
    "        \n",
    "    def grp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='mm')\n",
    "            grp_fit = GaussianRandomProjection(n_components=n, eps=0.1)\n",
    "            grp_fit.fit(train)\n",
    "            grp_train = pd.DataFrame(grp_fit.transform(train))\n",
    "            grp_valid = pd.DataFrame(grp_fit.transform(valid))\n",
    "            grp_cols = list(set(list(grp_train)))\n",
    "            grp_cols = ['grp_' + str(s) for s in grp_cols]\n",
    "            grp_train.columns = grp_cols\n",
    "            grp_valid.columns = grp_cols\n",
    "            return grp_train, grp_valid\n",
    "    \n",
    "    def srp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='mm')\n",
    "            srp_fit = SparseRandomProjection(n_components=n, dense_output=True, eps=0.1)\n",
    "            srp_fit.fit(train)\n",
    "            srp_train = pd.DataFrame(srp_fit.transform(train))\n",
    "            srp_valid = pd.DataFrame(srp_fit.transform(valid))\n",
    "            srp_cols = list(set(list(srp_train)))\n",
    "            srp_cols = ['srp_' + str(s) for s in srp_cols]\n",
    "            srp_train.columns = srp_cols\n",
    "            srp_valid.columns = srp_cols\n",
    "            return srp_train, srp_valid\n",
    "        \n",
    "    def return_combined(train, valid, list_objects = ['pca', 'ica', 'tsvd', 'grp', 'srp', 'tsne']):\n",
    "        if 'pca' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), pca_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), pca_valid], axis=1)\n",
    "        if 'ica' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), ica_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), ica_valid], axis=1)\n",
    "        if 'tsvd' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), tsvd_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), tsvd_valid], axis=1)\n",
    "        if 'grp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), grp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), grp_valid], axis=1)\n",
    "        if 'srp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), srp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), srp_valid], axis=1)\n",
    "        return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calling the various feat engineering functions and adding those features\n",
    "## pca, ica, tsvd, grp, srp\n",
    "pca_train, pca_valid = feat_eng.pca_feats(train=X_train, valid=X_valid, n=.95)\n",
    "ica_train, ica_valid = feat_eng.ica_feats(train=X_train, valid=X_valid, n=10)\n",
    "tsvd_train, tsvd_valid = feat_eng.tsvd_feats(train=X_train, valid=X_valid, n=10)\n",
    "grp_train, grp_valid = feat_eng.grp_feats(train=X_train, valid=X_valid, n=10)\n",
    "srp_train, srp_valid = feat_eng.srp_feats(train=X_train, valid=X_valid, n=10)\n",
    "\n",
    "## scale the data\n",
    "X_train, X_valid = feat_eng.scalers(train=X_train, valid=X_valid, which_method='mm')\n",
    "\n",
    "## return the final datasets with the added features\n",
    "X_train, X_valid = feat_eng.return_combined(train = X_train, valid = X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['response'] = y_train\n",
    "X_valid['response'] = y_valid\n",
    "\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_valid.to_csv('X_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "# Import a sample binary outcome train/test set into H2O\n",
    "h2o_train = h2o.import_file(\"X_train.csv\", header=1)\n",
    "h2o_test = h2o.import_file(\"X_valid.csv\", header=1)\n",
    "\n",
    "# Identify the response and set of predictors\n",
    "y = \"response\"\n",
    "x = list(h2o_train.columns)  #if x is defined as all columns except the response, then x is not required\n",
    "x.remove(y)\n",
    "\n",
    "# For binary classification, response should be a factor\n",
    "h2o_train[y] = h2o_train[y].asfactor()\n",
    "h2o_test[y] = h2o_test[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run AutoML for n seconds\n",
    "n = 30\n",
    "aml = H2OAutoML(max_runtime_secs = n, stopping_metric='mean_per_class_error', sort_metric='mean_per_class_error',\n",
    "                class_sampling_factors=[1, 0.2], balance_classes = False)\n",
    "aml.train(x = x, y = y, training_frame = h2o_train)\n",
    "\n",
    "# Print Leaderboard (ranked by xval metrics)\n",
    "print(aml.leaderboard)\n",
    "\n",
    "# (Optional) Evaluate performance on a test set\n",
    "perf = aml.leader.model_performance(h2o_test)\n",
    "print(perf.auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = aml.predict(h2o_test)[:,2]\n",
    "pred = pred.as_data_frame().as_matrix()\n",
    "predict = np.where(pred > 0.1, 1, 0)\n",
    "y_test=y_valid\n",
    "\n",
    "recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "auc_score = roc_auc_score(y_test, pred)\n",
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "print('recall score is: ', recall_score)\n",
    "print('precision score is: ', precision_score)\n",
    "print('f1_score is: ', f1_score)\n",
    "print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_9353 closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONE-CLASS METHODS ###\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "class oneclass_models():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this class contains several modelling algorithms for one-class classification/anomaly detection \"\"\"\n",
    "\n",
    "    def data_prepare(X_train, X_valid):\n",
    "        Negatives=X_train[X_train['response']==0]\n",
    "        Positives=X_train[X_train['response']==1]\n",
    "        Negatives.drop(['response'], axis=1, inplace=True)\n",
    "        Positives.drop(['response'], axis=1, inplace=True)\n",
    "        print(Negatives.shape)\n",
    "        print(Positives.shape)\n",
    "        \n",
    "        X_v = X_valid.drop(['response'], axis=1, inplace=False)\n",
    "        \n",
    "        return Positives, Negatives, X_v\n",
    "        \n",
    "    def uni_svm(X_train, X_valid):\n",
    "        \"\"\" one-class svm by training separately on positives and negatives \"\"\"\n",
    "        \n",
    "        Positives, Negatives, X_v = oneclass_models.data_prepare(X_train, X_valid)\n",
    "        \n",
    "        # Set the parameters by cross-validation\n",
    "        params = [{'kernel': ['rbf', 'linear', 'poly'],\n",
    "                   'gamma': [0.01, 0.1, 0.5],\n",
    "                   'nu': [0.01, 0.1, 0.5]}]\n",
    "\n",
    "        clf_P = GridSearchCV(sklearn.svm.OneClassSVM(), cv=5, param_grid=params, scoring='accuracy', verbose=1)\n",
    "        clf_N = GridSearchCV(sklearn.svm.OneClassSVM(), cv=5, param_grid=params, scoring='accuracy', verbose=1)\n",
    "        clf_P.fit(X=Positives, y=np.full(len(Positives),1))\n",
    "        clf_N.fit(X=Negatives, y=np.full(len(Negatives),1))\n",
    "        clf_AD_P = sklearn.svm.OneClassSVM(gamma=clf_P.best_params_['gamma'],\n",
    "                                      kernel=clf_P.best_params_['kernel'], nu=clf_P.best_params_['nu'])\n",
    "        clf_AD_P.fit(Positives)\n",
    "        clf_AD_N = sklearn.svm.OneClassSVM(gamma=clf_N.best_params_['gamma'],\n",
    "                                      kernel=clf_N.best_params_['kernel'], nu=clf_N.best_params_['nu'])\n",
    "        clf_AD_N.fit(Negatives)\n",
    "\n",
    "        valid_pred_P=clf_AD_P.predict(X_v)\n",
    "        valid_pred_N=clf_AD_N.predict(X_v)\n",
    "        \n",
    "        return valid_pred_P, valid_pred_N, clf_AD_P, clf_AD_N\n",
    "    \n",
    "    def score_table(valid_pred_P, valid_pred_N):\n",
    "        table = pd.DataFrame({'P': valid_pred_P,\n",
    "                              'N': -1*valid_pred_N,\n",
    "                              'O': y_valid})\n",
    "        table['P_N'] = np.where((table['P'] == 1) & (table['N'] == -1), 1, 0)\n",
    "\n",
    "        print(sklearn.metrics.accuracy_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        print(sklearn.metrics.precision_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        print(sklearn.metrics.recall_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        \n",
    "        return table        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5753, 139)\n",
      "(283, 139)\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 135 out of 135 | elapsed: 10.1min finished\n"
     ]
    }
   ],
   "source": [
    "p, n, clf_p, clf_n = oneclass_models.uni_svm(X_train=X_train, X_valid=X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24792267254536204\n",
      "0.04371704745166959\n",
      "0.708185053380783\n"
     ]
    }
   ],
   "source": [
    "table=oneclass_models.score_table(valid_pred_N=n, valid_pred_P=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>N</th>\n",
       "      <th>O</th>\n",
       "      <th>P_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5867</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5869</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5871</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5877</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5878</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5879</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5881</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5882</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5883</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5884</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5885</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5886</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5887</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5889</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5890</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5891</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5892</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5895</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5897 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      P  N  O  P_N\n",
       "0     1 -1  0    1\n",
       "1     1 -1  0    1\n",
       "2     1 -1  0    1\n",
       "3     1 -1  0    1\n",
       "4    -1  1  0    0\n",
       "5     1 -1  0    1\n",
       "6     1 -1  0    1\n",
       "7     1 -1  0    1\n",
       "8     1 -1  0    1\n",
       "9     1 -1  0    1\n",
       "10    1 -1  0    1\n",
       "11    1 -1  0    1\n",
       "12    1 -1  0    1\n",
       "13    1 -1  0    1\n",
       "14    1 -1  0    1\n",
       "15    1 -1  0    1\n",
       "16    1 -1  0    1\n",
       "17    1 -1  0    1\n",
       "18    1 -1  0    1\n",
       "19    1 -1  0    1\n",
       "20    1 -1  0    1\n",
       "21    1 -1  0    1\n",
       "22    1 -1  0    1\n",
       "23    1 -1  0    1\n",
       "24    1 -1  0    1\n",
       "25    1 -1  0    1\n",
       "26    1 -1  0    1\n",
       "27   -1  1  0    0\n",
       "28    1 -1  0    1\n",
       "29    1 -1  0    1\n",
       "...  .. .. ..  ...\n",
       "5867  1 -1  0    1\n",
       "5868  1 -1  0    1\n",
       "5869  1 -1  0    1\n",
       "5870 -1  1  0    0\n",
       "5871  1 -1  0    1\n",
       "5872  1 -1  0    1\n",
       "5873  1 -1  0    1\n",
       "5874  1 -1  0    1\n",
       "5875  1 -1  0    1\n",
       "5876 -1  1  0    0\n",
       "5877  1 -1  0    1\n",
       "5878 -1  1  0    0\n",
       "5879  1 -1  0    1\n",
       "5880  1 -1  0    1\n",
       "5881 -1  1  1    0\n",
       "5882 -1  1  0    0\n",
       "5883  1 -1  0    1\n",
       "5884 -1 -1  0    0\n",
       "5885  1 -1  0    1\n",
       "5886  1 -1  0    1\n",
       "5887 -1  1  0    0\n",
       "5888  1 -1  0    1\n",
       "5889 -1  1  0    0\n",
       "5890 -1  1  0    0\n",
       "5891  1 -1  0    1\n",
       "5892  1 -1  0    1\n",
       "5893  1 -1  0    1\n",
       "5894  1 -1  0    1\n",
       "5895  1 -1  0    1\n",
       "5896 -1  1  0    0\n",
       "\n",
       "[5897 rows x 4 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(bootstrap=False, contamination=0.1, max_features=0.3,\n",
       "        max_samples='auto', n_estimators=200, n_jobs=1, random_state=None,\n",
       "        verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFA=IsolationForest(n_estimators=200, max_features=0.3)\n",
    "IFA.fit(Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5753, 139)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negatives.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IFA=IFA.predict(Negatives)\n",
    "test_IFA=IFA.predict(Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Accuracy(Mat):\n",
    "   \n",
    "   Sum=0\n",
    "   for i in Mat:\n",
    "    \n",
    "        if(i==1):\n",
    "        \n",
    "           Sum+=1.0\n",
    "            \n",
    "   return(Sum/len(Mat)*100)\n",
    "\n",
    "def Test_Accuracy(Mat):\n",
    "   \n",
    "   Sum=0\n",
    "   for i in Mat:\n",
    "    \n",
    "        if(i==-1):\n",
    "        \n",
    "           Sum+=1.0\n",
    "            \n",
    "   return(Sum/len(Mat)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Isolation Forest:  89.98783243525116 %\n",
      "Test: Isolation Forest:  10.247349823321555 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Training: Isolation Forest: \",(Train_Accuracy(train_IFA)),\"%\")\n",
    "print(\"Test: Isolation Forest: \",(Test_Accuracy(test_IFA)),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
