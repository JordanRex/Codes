{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY CLASSIFICATION NOTEBOOK\n",
    "\n",
    "This is a notebook intended for a semi automated binary classification problem. User inputs are explicitly called out, primarily around specific null treatment, feature encoding, sampling technique, modelling evaluation function. All of them have defaults, so only the mandatory inputs are listed here\n",
    "\n",
    "***\n",
    "\n",
    "### Contents\n",
    "1. Pre-processing\n",
    "2. Feature Engineering\n",
    "3. Feature Selection\n",
    "4. Modelling\n",
    "    - xgboost\n",
    "    - lightgbm\n",
    "    - randomforest\n",
    "    - simple logit\n",
    "    - SVC\n",
    "    - 1class methods\n",
    "    - h2o AUTOML framework\n",
    "    \n",
    "***\n",
    "- *Author* : VARUN V\n",
    "- *Language* : python\n",
    "- *Date* : 17-08-2017\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'FeatureEncoder.py', 'FEATURE_ENGINEERING.ipynb', 'MODEL_SELECTION_TUNING_TEST_2017Dec_model3.ipynb', 'OTHER_MODELS.ipynb', 'ULTIMATE.ipynb']\n"
     ]
    }
   ],
   "source": [
    "## importing the relevant packages:\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# print/display all plots inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# the base packages\n",
    "import collections # for the Counter function\n",
    "import csv # for reading/writing csv files\n",
    "import pandas as pd, numpy as np, time, gc, bisect\n",
    "\n",
    "# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing, decomposition\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "import category_encoders as ce\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# modelling algorithms\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Exporting packages for SHAP/LIME\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# missing value imputation\n",
    "from fancyimpute import KNN, MICE #, NuclearNormMinimization\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 2 # number of iterations/parameter sets created towards tuning (all hyper-opt frameworks)\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6102, 92)\n",
      "(5897, 92)\n",
      "(5734, 92)\n"
     ]
    }
   ],
   "source": [
    "x1=pd.read_csv('train_final.csv', na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "x2=pd.read_csv('test_final.csv', na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "x3=pd.read_csv('valid_final.csv', na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "\n",
    "chars_to_remove = [' ', '.', '(', ')', '__', '-']\n",
    "for i in chars_to_remove:\n",
    "    x1.columns = x1.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "    x2.columns = x2.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "    x3.columns = x3.columns.str.strip().str.lower().str.replace(i, '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_missing(self):\n",
    "    return sum(self.isnull())\n",
    "x11=x1.apply(num_missing, axis=0)\n",
    "x21=x2.apply(num_missing, axis=0)\n",
    "x31=x3.apply(num_missing, axis=0)\n",
    "missing = pd.DataFrame({'dec16': x11, 'june17': x21, 'dec17': x31}) # display count of NAs per column\n",
    "missing['dec16_perc'] = 100*(missing['dec16']/x1.shape[0])\n",
    "missing['june17_perc'] = 100*(missing['june17']/x2.shape[0])\n",
    "missing['dec17_perc'] = 100*(missing['dec17']/x3.shape[0])\n",
    "\n",
    "missing.index.name = 'columns'\n",
    "missing.reset_index(inplace=True)\n",
    "\n",
    "missing.to_csv('NAZ_missing_distribution.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAIN CLASSES ####\n",
    "\n",
    "## Two defined for now ##\n",
    "# 1. DataFrame Imputer\n",
    "#    - for imputing missing values\n",
    "# 2. Prepare Data\n",
    "#    - for sourcing, processing, and returning the train/test datasets\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        specify columns to be grouped by before imputation. use only for specific cases when user knows a specific level\n",
    "        at which a column can be grouped at for missing value treatment\n",
    "        \n",
    "        X.groupby(['...'])\n",
    "        \"\"\"\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "    def num_missing(self):\n",
    "        return sum(self.isnull())\n",
    "    \n",
    "    def imputer_method(self, column, method=['mean', 'median', 'most_frequent']):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = method, axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def fancy_impute(X, which_method):\n",
    "        \"\"\" currently supported algorithms are KNN, NNM and MICE from the fancyimpute package\n",
    "        which_method = ['KNN', 'NNM', 'MICE']\n",
    "        \"\"\"\n",
    "        if which_method == 'NNM': X = NuclearNormMinimization().complete(X) # NNM method\n",
    "        if which_method == 'KNN': X = KNN(k=7, verbose=False).complete(X) # KNN method\n",
    "        if which_method == 'MICE':\n",
    "            X_complete_df = X.copy()\n",
    "            mice = MICE(verbose=False)\n",
    "            X_complete = mice.complete(np.asarray(X.values, dtype=float))\n",
    "            X_complete_df.loc[:, X.columns] = X_complete[:][:]\n",
    "            X = X_complete_df\n",
    "        return X\n",
    "\n",
    "class prepare_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" To prepare data,\n",
    "                1. read in data\n",
    "                2. pre-processing/cleaning\n",
    "                3. creating helper objects for later steps\n",
    "                4. processing for modelling\n",
    "                5. function return objects are the train, valid, response, categ cols/indices, feature names\n",
    "        \"\"\"\n",
    "    \n",
    "    def labelEncoder(train_df, valid_df, cat_columns, categorical_names):\n",
    "        for feature in tqdm(cat_columns):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(train_df[feature].astype(str))\n",
    "            train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "            valid_df[feature] = valid_df[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "            le_classes = le.classes_.tolist()\n",
    "            bisect.insort_left(le_classes, 'No Data')\n",
    "            le.classes_ = le_classes\n",
    "            valid_df[feature] = le.transform(valid_df[feature].astype(str))\n",
    "            categorical_names[feature] = le.classes_\n",
    "            return train_df, valid_df, categorical_names\n",
    "        \n",
    "    def ce_encodings(train_df, valid_df, y_train, y_valid, cat_columns, encoding):\n",
    "        if encoding=='bne':\n",
    "            enc1=ce.BaseNEncoder(cat_columns, base=3)\n",
    "            enc1.fit(train_df)\n",
    "            train_df=enc1.transform(train_df)\n",
    "            valid_df=enc1.transform(valid_df)\n",
    "        elif encoding=='be':\n",
    "            enc2=ce.BinaryEncoder(cat_columns)\n",
    "            enc2.fit(train_df)\n",
    "            train_df=enc2.transform(train_df)\n",
    "            valid_df=enc2.transform(valid_df)\n",
    "        elif encoding=='he':\n",
    "            enc3=ce.HashingEncoder(cat_columns)\n",
    "            enc3.fit(train_df, y_train)\n",
    "            train_df=enc3.transform(train_df)\n",
    "            valid_df=enc3.transform(valid_df)\n",
    "        elif encoding=='oe':\n",
    "            enc4=ce.OrdinalEncoder(cat_columns)\n",
    "            enc4.fit(train_df, y_train)\n",
    "            train_df=enc4.transform(train_df)\n",
    "            valid_df=enc4.transform(valid_df)\n",
    "        return train_df, valid_df\n",
    "    \n",
    "    ## function to get frequency count of elements in a vector/list\n",
    "    def freq_count(input_vector):\n",
    "        return collections.Counter(input_vector)\n",
    "    \n",
    "    def categ_feats(train_df, valid_df, y_train, y_valid, encoding='le'):\n",
    "        x = list(train_df.dtypes)\n",
    "        x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "        categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "        # Get feature names and their values for categorical data (needed for LIME)\n",
    "        cat_columns = train_df.select_dtypes(include=['object']).columns.values\n",
    "        categorical_names = {}\n",
    "\n",
    "        if encoding=='le':\n",
    "            train_df, valid_df, categorical_names = prepare_data.labelEncoder(train_df, valid_df, cat_columns, categorical_names)\n",
    "        elif encoding in ['be', 'bne', 'he', 'oe']:\n",
    "            train_df, valid_df = prepare_data.ce_encodings(train_df, valid_df, y_train, y_valid, cat_columns, encoding)\n",
    "        else :\n",
    "            print('Not supported. Use one of [be, bne, he, oe]')\n",
    "        return train_df, valid_df, categorical_names, categorical_idx\n",
    "\n",
    "    def create(input_file_path, input_file_path_2, response, cols_to_remove = ['id'], random_seed = 1,\n",
    "                            encoding = 'le'):\n",
    "        train = pd.read_csv(input_file_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        test = pd.read_csv(input_file_path_2, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        \n",
    "        train.drop(cols_to_remove, axis = 1, inplace = True)\n",
    "        test = pd.DataFrame(data = test[train.columns])\n",
    "        \n",
    "        chars_to_remove = [' ', '.', '(', ')', '__', '-']\n",
    "        for i in chars_to_remove:\n",
    "            train.columns = train.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "            test.columns = test.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "            \n",
    "        print(train.shape, '\\n')\n",
    "        train.dropna(thresh=0.5*(train.shape[0]), axis=1, inplace = True)\n",
    "        train.dropna(thresh=0.4*(train.shape[1]), axis=0, inplace = True)\n",
    "        print(train.shape, '\\n')\n",
    "        test = test[train.columns]\n",
    "        test.dropna(thresh=0.5*(test.shape[0]), axis=1, inplace = True)\n",
    "        train = train[test.columns]\n",
    "        \n",
    "        print(prepare_data.freq_count(train[response]), '\\n')\n",
    "\n",
    "        # shuffle the dataframes so that the training is done in a random order.\n",
    "        train = shuffle(train)\n",
    "        test = shuffle(test)\n",
    "        \n",
    "        # creating the response vector\n",
    "        y_train = train[response].values\n",
    "        X_train = train.drop([response], axis = 1)\n",
    "        y_valid = test[response].values\n",
    "        X_valid = test.drop([response], axis = 1)\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_valid = pd.DataFrame(X_valid)\n",
    "        \n",
    "        X_train, X_valid, categ_names, categ_idx = prepare_data.categ_feats(X_train, X_valid, y_train, y_valid, encoding)\n",
    "        \n",
    "        feat_names = X_train.columns.values\n",
    "        feat_names2 = X_valid.columns.values\n",
    "        \n",
    "        X_train = DataFrameImputer.fancy_impute(X_train, which_method='MICE')\n",
    "        X_valid = DataFrameImputer.fancy_impute(X_valid, which_method='MICE')\n",
    "        \n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        X_train = pd.DataFrame(data=X_train, columns=feat_names)\n",
    "        X_valid = pd.DataFrame(data=X_valid, columns=feat_names2)\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6102, 77) \n",
      "\n",
      "(6085, 75) \n",
      "\n",
      "Counter({0: 5796, 1: 289}) \n",
      "\n",
      "['pay_scale_group_0' 'pay_scale_group_1' 'pay_scale_group_2'\n",
      " 'pay_scale_group_3' 'gender_0' 'gender_1' 'abinbev_entity2_0'\n",
      " 'abinbev_entity2_1' 'abinbev_entity2_2' 'abinbev_entity2_3'\n",
      " 'macro_entity4_0' 'macro_entity4_1' 'macro_entity4_2' 'macro_entity4_3'\n",
      " 'macro_entity4_4' 'physical_work_location_code_0'\n",
      " 'physical_work_location_code_1' 'physical_work_location_code_2'\n",
      " 'physical_work_location_code_3' 'physical_work_location_code_4'\n",
      " 'physical_work_location_code_5' 'global_job_0' 'global_job_1'\n",
      " 'global_job_2' 'global_job_3' 'global_job_4' 'global_job_5'\n",
      " 'global_job_6' 'job_family_0' 'job_family_1' 'job_family_2'\n",
      " 'job_family_3' 'job_family_4' 'job_family_5' 'job_family_6'\n",
      " 'functional_area_0' 'functional_area_1' 'functional_area_2'\n",
      " 'functional_area_3' 'functional_area_4' 'functional_area_5'\n",
      " 'cost_center_0' 'cost_center_1' 'cost_center_2' 'cost_center_3'\n",
      " 'cost_center_4' 'cost_center_5' 'cost_center_6' 'macro_entity_type_0'\n",
      " 'macro_entity_type_1' 'macro_entity_type_2' 'm_opr_1_0' 'm_opr_1_1'\n",
      " 'm_opr_1_2' 'm_opr_2_0' 'm_opr_2_1' 'm_opr_2_2' 'm_opr_3_0' 'm_opr_3_1'\n",
      " 'm_opr_3_2' 'm_opr_change_0' 'm_opr_change_1' 'm_opr_change_2'\n",
      " 'm_opr_change_3' 'opr_1_0' 'opr_1_1' 'opr_1_2' 'opr_2_0' 'opr_2_1'\n",
      " 'opr_2_2' 'opr_3_0' 'opr_3_1' 'opr_3_2' 'opr_change_0' 'opr_change_1'\n",
      " 'opr_change_2' 'opr_change_3' 'cr_0' 'cr_1' 'cr_2' 'bonus_comparison_0'\n",
      " 'bonus_comparison_1' 'bonus_comparison_2' 'g_total_comparison_0'\n",
      " 'g_total_comparison_1' 'g_total_comparison_2' 'incentive_flag_0'\n",
      " 'incentive_flag_1' 'cr_team_0' 'cr_team_1' 'cr_team_2'\n",
      " 'bonus_comparison_team_0' 'bonus_comparison_team_1'\n",
      " 'bonus_comparison_team_2' 'g_total_comparison_team_0'\n",
      " 'g_total_comparison_team_1' 'g_total_comparison_team_2' 'cr_team2_0'\n",
      " 'cr_team2_1' 'cr_team2_2' 'bonus_comparison_team2_0'\n",
      " 'bonus_comparison_team2_1' 'bonus_comparison_team2_2'\n",
      " 'g_total_comparison_team2_0' 'g_total_comparison_team2_1'\n",
      " 'g_total_comparison_team2_2' 'm_gender_0' 'm_gender_1'\n",
      " 'gender_relation_0' 'gender_relation_1' 'gender_relation_2'\n",
      " 'target_by_appraiser_0' 'target_by_appraiser_1' 'target_by_appraiser_2'\n",
      " 'target_by_macro_entity_0' 'target_by_macro_entity_1'\n",
      " 'target_by_macro_entity_2' 'target_by_cost_center_0'\n",
      " 'target_by_cost_center_1' 'target_by_cost_center_2' 'target_by_org_id_0'\n",
      " 'target_by_org_id_1' 'target_by_org_id_2' 'target_by_band_0'\n",
      " 'target_by_band_1' 'target_by_band_2' 'target_by_aid_band_0'\n",
      " 'target_by_aid_band_1' 'target_by_aid_band_2' 'target_by_me_band_0'\n",
      " 'target_by_me_band_1' 'target_by_me_band_2' 'target_by_fa_band_0'\n",
      " 'target_by_fa_band_1' 'target_by_fa_band_2' 'target_by_cc_band_0'\n",
      " 'target_by_cc_band_1' 'target_by_cc_band_2' 'target_by_oid_band_0'\n",
      " 'target_by_oid_band_1' 'target_by_oid_band_2' 'manager_pay_scale_0'\n",
      " 'manager_pay_scale_1' 'manager_pay_scale_2' 'manager_pay_scale_3'\n",
      " 'service_months' 'age' 'manager_service_months' 'manager_age' 'age_diff'\n",
      " 'position' 'manager_position' 'ebm_level_of_the_job' 'months_in_position'\n",
      " 'local_entity_code' 'manager_l2' 'fc_recent' 'lc_recent' 'cr_percentage'\n",
      " 'gt_percentage' 'bt_percentage' 'cr_percentage_team' 'gt_percentage_team'\n",
      " 'bt_percentage_team' 'cr_percentage_team2' 'gt_percentage_team2'\n",
      " 'bt_percentage_team2' 'no_of_position_changes' 'time_in_position'\n",
      " 'time_in_band' 'number_of_band_changes' 'net_target'\n",
      " 'manager_time_in_position' 'manager_time_in_band' 'psg_diff'\n",
      " 'bonus_total' 'annual_salary' 'grand_total']\n",
      "0\n",
      "      pay_scale_group_0  pay_scale_group_1  pay_scale_group_2  \\\n",
      "5205                  0                  0                  0   \n",
      "5588                  0                  0                  0   \n",
      "4338                  0                  0                  0   \n",
      "5872                  0                  0                  1   \n",
      "5196                  0                  0                  1   \n",
      "\n",
      "      pay_scale_group_3  gender_0  gender_1  abinbev_entity2_0  \\\n",
      "5205                  1         0         1                  0   \n",
      "5588                  2         0         1                  0   \n",
      "4338                  1         0         1                  0   \n",
      "5872                  0         0         1                  0   \n",
      "5196                  1         0         1                  0   \n",
      "\n",
      "      abinbev_entity2_1  abinbev_entity2_2  abinbev_entity2_3     ...       \\\n",
      "5205                  0                  0                  1     ...        \n",
      "5588                  0                  0                  1     ...        \n",
      "4338                  0                  0                  2     ...        \n",
      "5872                  0                  0                  1     ...        \n",
      "5196                  0                  0                  1     ...        \n",
      "\n",
      "      time_in_position  time_in_band  number_of_band_changes  net_target  \\\n",
      "5205             132.0          39.0                       1       100.0   \n",
      "5588             144.0          38.0                       1       100.0   \n",
      "4338             216.0         100.0                       2        82.5   \n",
      "5872               9.0          11.0                       2        85.0   \n",
      "5196             360.0          41.0                       1        97.0   \n",
      "\n",
      "      manager_time_in_position  manager_time_in_band  psg_diff  bonus_total  \\\n",
      "5205                 20.402883                  42.0         2     29553.86   \n",
      "5588                  4.303990                   5.0         4     10908.58   \n",
      "4338                143.970102                 126.0         2         0.00   \n",
      "5872                  6.012444                   6.0         3     23336.42   \n",
      "5196                  6.998090                   6.0         1      7875.97   \n",
      "\n",
      "      annual_salary  grand_total  \n",
      "5205      123562.08    153115.94  \n",
      "5588       93636.00    104544.58  \n",
      "4338           0.00         0.00  \n",
      "5872      165624.00    188960.42  \n",
      "5196       99180.72    107056.69  \n",
      "\n",
      "[5 rows x 178 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input matrix is not missing any values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-189-77141d56b628>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m                                                   \u001b[1;34m'physical work location-description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'physical work location-city'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                   \u001b[1;34m'position start date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'manager position desc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'costcenter description'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                                                   'local entity description', 'appraiser id'], encoding = 'bne')\n\u001b[0m",
      "\u001b[1;32m<ipython-input-188-490d60f23e49>\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(input_file_path, input_file_path_2, response, cols_to_remove, random_seed, encoding)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrameImputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfancy_impute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MICE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[0mX_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrameImputer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfancy_impute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhich_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MICE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-188-490d60f23e49>\u001b[0m in \u001b[0;36mfancy_impute\u001b[1;34m(X, which_method)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0mX_complete_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mmice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMICE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mX_complete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mX_complete_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_complete\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_complete_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fancyimpute\\mice.py\u001b[0m in \u001b[0;36mcomplete\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[MICE] Completing matrix with shape %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[0mX_completed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mimputed_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiple_imputations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[1;31m# average the imputed values for each feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0maverage_imputated_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputed_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fancyimpute\\mice.py\u001b[0m in \u001b[0;36mmultiple_imputations\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0mmissing_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_missing_value_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mvisit_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_visit_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fancyimpute\\solver.py\u001b[0m in \u001b[0;36m_check_missing_value_mask\u001b[1;34m(self, missing)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_missing_value_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input matrix is not missing any values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input matrix must have some non-missing values\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input matrix is not missing any values"
     ]
    }
   ],
   "source": [
    "# create data function call\n",
    "# CV approach\n",
    "# train and valid features/response dataframes returned\n",
    "# categorical column names/indices and all feature names also returned\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names = prepare_data.create(input_file_path='../input/adult.data',\n",
    "                                                                  input_file_path_2='../input/adult.test.csv', response = 'label',\n",
    "                                cols_to_remove = [], encoding = 'bne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6085, 74)\n",
      "(5734, 74)\n",
      "(6085,)\n",
      "(5734,)\n",
      "Counter({0: 5796, 1: 289})\n",
      "Counter({0: 5453, 1: 281})\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(collections.Counter(y_train))\n",
    "#print(collections.Counter(y_test))\n",
    "print(collections.Counter(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING MODULE\n",
    "\n",
    "1. PCA\n",
    "2. ICA\n",
    "3. tSVD\n",
    "4. GRP\n",
    "5. SRP\n",
    "6. Binning\n",
    "7. Deviation Encoding features\n",
    "8. Salary related features\n",
    "9. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feat_eng():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module contains several functions for creating new features. find below a brief description of each \"\"\"\n",
    "    \n",
    "    def scalers(train, valid, which_method):\n",
    "        if which_method == 'ss':\n",
    "            sc = StandardScaler()\n",
    "            sc.fit(train)\n",
    "            train = pd.DataFrame(sc.transform(train))\n",
    "            valid = pd.DataFrame(sc.transform(valid))\n",
    "            return train, valid # scale all variables to zero mean and unit variance, required for PCA and related\n",
    "        if which_method == 'mm':\n",
    "            mm = MinMaxScaler()\n",
    "            mm.fit(train)\n",
    "            train = pd.DataFrame(mm.transform(train))\n",
    "            valid = pd.DataFrame(mm.transform(valid))\n",
    "            return train, valid # use this method to iterate\n",
    "        \n",
    "    def pca_feats(train, valid, n = .95):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            pca_fit = decomposition.PCA(n_components=n)\n",
    "            pca_fit.fit(train)\n",
    "            pca_train = pd.DataFrame(pca_fit.transform(train))\n",
    "            pca_valid = pd.DataFrame(pca_fit.transform(valid))\n",
    "            pca_cols = list(set(list(pca_train)))\n",
    "            pca_cols = ['pca_' + str(s) for s in pca_cols]\n",
    "            pca_train.columns = pca_cols\n",
    "            pca_valid.columns = pca_cols\n",
    "            return pca_train, pca_valid\n",
    "        \n",
    "    def ica_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            ica_fit = decomposition.FastICA(n_components=n)\n",
    "            ica_fit.fit(train)\n",
    "            ica_train = pd.DataFrame(ica_fit.transform(train))\n",
    "            ica_valid = pd.DataFrame(ica_fit.transform(valid))\n",
    "            ica_cols = list(set(list(ica_train)))\n",
    "            ica_cols = ['ica_' + str(s) for s in ica_cols]\n",
    "            ica_train.columns = ica_cols\n",
    "            ica_valid.columns = ica_cols\n",
    "            return ica_train, ica_valid\n",
    "        \n",
    "    def tsvd_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            tsvd_fit = decomposition.TruncatedSVD(n_components=n)\n",
    "            tsvd_fit.fit(train)\n",
    "            tsvd_train = pd.DataFrame(tsvd_fit.transform(train))\n",
    "            tsvd_valid = pd.DataFrame(tsvd_fit.transform(valid))\n",
    "            tsvd_cols = list(set(list(tsvd_train)))\n",
    "            tsvd_cols = ['tsvd_' + str(s) for s in tsvd_cols]\n",
    "            tsvd_train.columns = tsvd_cols\n",
    "            tsvd_valid.columns = tsvd_cols\n",
    "            return tsvd_train, tsvd_valid\n",
    "        \n",
    "    def grp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            grp_fit = GaussianRandomProjection(n_components=n, eps=0.1)\n",
    "            grp_fit.fit(train)\n",
    "            grp_train = pd.DataFrame(grp_fit.transform(train))\n",
    "            grp_valid = pd.DataFrame(grp_fit.transform(valid))\n",
    "            grp_cols = list(set(list(grp_train)))\n",
    "            grp_cols = ['grp_' + str(s) for s in grp_cols]\n",
    "            grp_train.columns = grp_cols\n",
    "            grp_valid.columns = grp_cols\n",
    "            return grp_train, grp_valid\n",
    "    \n",
    "    def srp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            srp_fit = SparseRandomProjection(n_components=n, dense_output=True, eps=0.1)\n",
    "            srp_fit.fit(train)\n",
    "            srp_train = pd.DataFrame(srp_fit.transform(train))\n",
    "            srp_valid = pd.DataFrame(srp_fit.transform(valid))\n",
    "            srp_cols = list(set(list(srp_train)))\n",
    "            srp_cols = ['srp_' + str(s) for s in srp_cols]\n",
    "            srp_train.columns = srp_cols\n",
    "            srp_valid.columns = srp_cols\n",
    "            return srp_train, srp_valid\n",
    "        \n",
    "    def return_combined(train, valid, list_objects = ['pca', 'ica', 'tsvd', 'grp', 'srp']):\n",
    "        if 'pca' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), pca_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), pca_valid], axis=1)\n",
    "        if 'ica' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), ica_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), ica_valid], axis=1)\n",
    "        if 'tsvd' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), tsvd_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), tsvd_valid], axis=1)\n",
    "        if 'grp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), grp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), grp_valid], axis=1)\n",
    "        if 'srp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), srp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), srp_valid], axis=1)\n",
    "        return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "## calling the various feat engineering functions and adding those features\n",
    "## pca, ica, tsvd, grp, srp\n",
    "pca_train, pca_valid = feat_eng.pca_feats(train=X_train, valid=X_valid, n=.95)\n",
    "ica_train, ica_valid = feat_eng.ica_feats(train=X_train, valid=X_valid, n=10)\n",
    "tsvd_train, tsvd_valid = feat_eng.tsvd_feats(train=X_train, valid=X_valid, n=10)\n",
    "grp_train, grp_valid = feat_eng.grp_feats(train=X_train, valid=X_valid, n=10)\n",
    "srp_train, srp_valid = feat_eng.srp_feats(train=X_train, valid=X_valid, n=10)\n",
    "\n",
    "## scale the data\n",
    "X_train, X_valid = feat_eng.scalers(train=X_train, valid=X_valid, which_method='mm')\n",
    "\n",
    "## return the final datasets with the added features\n",
    "X_train, X_valid = feat_eng.return_combined(train = X_train, valid = X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['response'] = y_train\n",
    "X_valid['response'] = y_valid\n",
    "\n",
    "X_train.to_csv('X_train.csv', index=False)\n",
    "X_valid.to_csv('X_valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## h2o AUTO_ML grid search framework\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "class h2o_automl():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" module to invoke h2o auto ml and tune on the sampling parameters. \n",
    "        will need to incorporate to super pipeline later \"\"\"\n",
    "        \n",
    "    def score(params):        \n",
    "        # function to be minimized (1 - ROCAUC)\n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        randomseed = 1\n",
    "        \n",
    "        #h2o_train = h2o.H2OFrame(params['h2o_train'])\n",
    "        #h2o_valid = h2o.H2OFrame(params['h2o_valid'])\n",
    "        \n",
    "        aml = H2OAutoML(max_runtime_secs = params['time_to_run'], stopping_metric='mean_per_class_error', sort_metric='mean_per_class_error',\n",
    "                        class_sampling_factors=[params['oversampling'], params['undersampling']],\n",
    "                        balance_classes = params['balance_classes'])\n",
    "\n",
    "        aml.train(y = 'response', training_frame = h2o_train)\n",
    "        \n",
    "        # Print Leaderboard (ranked by xval metrics)\n",
    "        print(aml.leaderboard)\n",
    "        # Evaluate performance on a test set\n",
    "        perf = aml.leader.model_performance(h2o_valid)\n",
    "        print(perf.auc())   \n",
    "        \n",
    "        return {'loss': (1 - perf.auc()), 'status': STATUS_OK, 'params': params, 'iteration': ITERATION, 'aml': aml}\n",
    "    \n",
    "    def auto_ml(train_path, valid_path, response = 'response', time_to_run = 30):\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        global h2o_train, h2o_valid\n",
    "        \n",
    "        # initializing the h2o cluster\n",
    "        h2o.init()\n",
    "        \n",
    "        # Import a sample binary outcome train/test set into H2O\n",
    "        h2o_train = h2o.import_file(train_path, header=1)\n",
    "        h2o_valid = h2o.import_file(valid_path, header=1)\n",
    "\n",
    "        # Identify the response and set of predictors\n",
    "        x = list(h2o_train.columns)  #if x is defined as all columns except the response, then x is not required\n",
    "        x.remove(response)\n",
    "\n",
    "        # For binary classification, response should be a factor\n",
    "        h2o_train[response] = h2o_train[response].asfactor()\n",
    "        h2o_valid[response] = h2o_valid[response].asfactor()\n",
    "\n",
    "        # space to be traversed for the hyperopt function\n",
    "        space = {\n",
    "            'undersampling': hp.uniform('us', 0.01, 1),\n",
    "            'oversampling': hp.uniform('os', 1, 10),\n",
    "            'balance_classes': hp.choice('bc', ['True', 'False']),\n",
    "            'time_to_run': 30, 'x': x, 'response': response}\n",
    "\n",
    "        best = fmin(h2o_automl.score, space, algo=tpe.suggest, trials=trials, max_evals=MAX_EVALS,\n",
    "                    rstate=np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        aml = trials.best_trial['result']['aml']\n",
    "        \n",
    "        return trials, best, aml # results of all the iterations\n",
    "    \n",
    "    def get_score(aml, h2o_valid, y_valid, threshold = 0.1):\n",
    "        pred = aml.predict(h2o_valid)[:,2]\n",
    "        pred = pred.as_data_frame().as_matrix()\n",
    "        predict = np.where(pred > threshold, 1, 0)\n",
    "        y_test=y_valid\n",
    "\n",
    "        recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "        precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "        f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "        auc_score = roc_auc_score(y_test, pred)\n",
    "        tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "        print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "        print('recall score is: ', recall_score)\n",
    "        print('precision score is: ', precision_score)\n",
    "        print('f1_score is: ', f1_score)\n",
    "        print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "        print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  GBM_grid_0_AutoML_20180814_180357_model_2\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.012345731874956202\n",
      "RMSE: 0.11111134899260382\n",
      "LogLoss: 0.05451098842472014\n",
      "Mean Per-Class Error: 0.013020362902665505\n",
      "AUC: 0.9970281519607433\n",
      "Gini: 0.9940563039214867\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.1987366596826228: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>4668.0</td>\n",
       "<td>5.0</td>\n",
       "<td>0.0011</td>\n",
       "<td> (5.0/4673.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>12.0</td>\n",
       "<td>219.0</td>\n",
       "<td>0.0519</td>\n",
       "<td> (12.0/231.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>4680.0</td>\n",
       "<td>224.0</td>\n",
       "<td>0.0035</td>\n",
       "<td> (17.0/4904.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  -------------\n",
       "0      4668  5    0.0011   (5.0/4673.0)\n",
       "1      12    219  0.0519   (12.0/231.0)\n",
       "Total  4680  224  0.0035   (17.0/4904.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1987367</td>\n",
       "<td>0.9626374</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1525126</td>\n",
       "<td>0.9664948</td>\n",
       "<td>182.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2089268</td>\n",
       "<td>0.9740840</td>\n",
       "<td>170.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.2089268</td>\n",
       "<td>0.9965334</td>\n",
       "<td>170.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9251269</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0167882</td>\n",
       "<td>1.0</td>\n",
       "<td>343.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9251269</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1987367</td>\n",
       "<td>0.9609453</td>\n",
       "<td>172.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0959240</td>\n",
       "<td>0.9869463</td>\n",
       "<td>207.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0959240</td>\n",
       "<td>0.9869796</td>\n",
       "<td>207.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.198737     0.962637  172\n",
       "max f2                       0.152513     0.966495  182\n",
       "max f0point5                 0.208927     0.974084  170\n",
       "max accuracy                 0.208927     0.996533  170\n",
       "max precision                0.925127     1         0\n",
       "max recall                   0.0167882    1         343\n",
       "max specificity              0.925127     1         0\n",
       "max absolute_mcc             0.198737     0.960945  172\n",
       "max min_per_class_accuracy   0.095924     0.986946  207\n",
       "max mean_per_class_accuracy  0.095924     0.98698   207"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4.71 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101958</td>\n",
       "<td>0.7218179</td>\n",
       "<td>21.2294372</td>\n",
       "<td>21.2294372</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2164502</td>\n",
       "<td>0.2164502</td>\n",
       "<td>2022.9437229</td>\n",
       "<td>2022.9437229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201876</td>\n",
       "<td>0.5969222</td>\n",
       "<td>21.2294372</td>\n",
       "<td>21.2294372</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2121212</td>\n",
       "<td>0.4285714</td>\n",
       "<td>2022.9437229</td>\n",
       "<td>2022.9437229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0301794</td>\n",
       "<td>0.4868321</td>\n",
       "<td>21.2294372</td>\n",
       "<td>21.2294372</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.2121212</td>\n",
       "<td>0.6406926</td>\n",
       "<td>2022.9437229</td>\n",
       "<td>2022.9437229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401713</td>\n",
       "<td>0.3360900</td>\n",
       "<td>20.3629296</td>\n",
       "<td>21.0139099</td>\n",
       "<td>0.9591837</td>\n",
       "<td>0.9898477</td>\n",
       "<td>0.2034632</td>\n",
       "<td>0.8441558</td>\n",
       "<td>1936.2929587</td>\n",
       "<td>2001.3909948</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501631</td>\n",
       "<td>0.1411981</td>\n",
       "<td>12.9976146</td>\n",
       "<td>19.4171682</td>\n",
       "<td>0.6122449</td>\n",
       "<td>0.9146341</td>\n",
       "<td>0.1298701</td>\n",
       "<td>0.9740260</td>\n",
       "<td>1199.7614630</td>\n",
       "<td>1841.7168198</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001223</td>\n",
       "<td>0.0549396</td>\n",
       "<td>0.2599523</td>\n",
       "<td>9.8580686</td>\n",
       "<td>0.0122449</td>\n",
       "<td>0.4643585</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.9870130</td>\n",
       "<td>-74.0047707</td>\n",
       "<td>885.8068612</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500816</td>\n",
       "<td>0.0406388</td>\n",
       "<td>0.1733015</td>\n",
       "<td>6.6341991</td>\n",
       "<td>0.0081633</td>\n",
       "<td>0.3125</td>\n",
       "<td>0.0086580</td>\n",
       "<td>0.9956710</td>\n",
       "<td>-82.6698472</td>\n",
       "<td>563.4199134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000408</td>\n",
       "<td>0.0330241</td>\n",
       "<td>0.0</td>\n",
       "<td>4.9773400</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2344546</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9956710</td>\n",
       "<td>-100.0</td>\n",
       "<td>397.7340023</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999592</td>\n",
       "<td>0.0247311</td>\n",
       "<td>0.0</td>\n",
       "<td>3.3193546</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1563562</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9956710</td>\n",
       "<td>-100.0</td>\n",
       "<td>231.9354563</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000816</td>\n",
       "<td>0.0199896</td>\n",
       "<td>0.0</td>\n",
       "<td>2.4886700</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1172273</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9956710</td>\n",
       "<td>-100.0</td>\n",
       "<td>148.8670012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0163951</td>\n",
       "<td>0.0433254</td>\n",
       "<td>2.0</td>\n",
       "<td>0.0020408</td>\n",
       "<td>0.0942088</td>\n",
       "<td>0.0043290</td>\n",
       "<td>1.0</td>\n",
       "<td>-95.6674618</td>\n",
       "<td>100.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999184</td>\n",
       "<td>0.0138598</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6668933</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0785180</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.6893270</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000408</td>\n",
       "<td>0.0117692</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4284882</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0672881</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8488203</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999592</td>\n",
       "<td>0.0098428</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500637</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0588835</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0063727</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8998777</td>\n",
       "<td>0.0081211</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1112622</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0523453</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1262180</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0038617</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0471044</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0101958                   0.721818           21.2294    21.2294            1                1                           0.21645         0.21645                    2022.94   2022.94\n",
       "    2        0.0201876                   0.596922           21.2294    21.2294            1                1                           0.212121        0.428571                   2022.94   2022.94\n",
       "    3        0.0301794                   0.486832           21.2294    21.2294            1                1                           0.212121        0.640693                   2022.94   2022.94\n",
       "    4        0.0401713                   0.33609            20.3629    21.0139            0.959184         0.989848                    0.203463        0.844156                   1936.29   2001.39\n",
       "    5        0.0501631                   0.141198           12.9976    19.4172            0.612245         0.914634                    0.12987         0.974026                   1199.76   1841.72\n",
       "    6        0.100122                    0.0549396          0.259952   9.85807            0.0122449        0.464358                    0.012987        0.987013                   -74.0048  885.807\n",
       "    7        0.150082                    0.0406388          0.173302   6.6342             0.00816327       0.3125                      0.00865801      0.995671                   -82.6698  563.42\n",
       "    8        0.200041                    0.0330241          0          4.97734            0                0.234455                    0               0.995671                   -100      397.734\n",
       "    9        0.299959                    0.0247311          0          3.31935            0                0.156356                    0               0.995671                   -100      231.935\n",
       "    10       0.400082                    0.0199896          0          2.48867            0                0.117227                    0               0.995671                   -100      148.867\n",
       "    11       0.5                         0.0163951          0.0433254  2                  0.00204082       0.0942088                   0.004329        1                          -95.6675  100\n",
       "    12       0.599918                    0.0138598          0          1.66689            0                0.078518                    0               1                          -100      66.6893\n",
       "    13       0.700041                    0.0117692          0          1.42849            0                0.0672881                   0               1                          -100      42.8488\n",
       "    14       0.799959                    0.00984282         0          1.25006            0                0.0588835                   0               1                          -100      25.0064\n",
       "    15       0.899878                    0.00812114         0          1.11126            0                0.0523453                   0               1                          -100      11.1262\n",
       "    16       1                           0.0038617          0          1                  0                0.0471044                   0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.04511686381990303\n",
      "RMSE: 0.2124073064183599\n",
      "LogLoss: 0.18568613876747775\n",
      "Mean Per-Class Error: 0.33176988976571375\n",
      "AUC: 0.7111800288635736\n",
      "Gini: 0.42236005772714713\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.08171475032008457: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>1067.0</td>\n",
       "<td>56.0</td>\n",
       "<td>0.0499</td>\n",
       "<td> (56.0/1123.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>39.0</td>\n",
       "<td>19.0</td>\n",
       "<td>0.6724</td>\n",
       "<td> (39.0/58.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>1106.0</td>\n",
       "<td>75.0</td>\n",
       "<td>0.0804</td>\n",
       "<td> (95.0/1181.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  -------------\n",
       "0      1067  56   0.0499   (56.0/1123.0)\n",
       "1      39    19   0.6724   (39.0/58.0)\n",
       "Total  1106  75   0.0804   (95.0/1181.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0817148</td>\n",
       "<td>0.2857143</td>\n",
       "<td>72.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0817148</td>\n",
       "<td>0.3094463</td>\n",
       "<td>72.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.1408014</td>\n",
       "<td>0.3235294</td>\n",
       "<td>27.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3472012</td>\n",
       "<td>0.9517358</td>\n",
       "<td>6.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.3472012</td>\n",
       "<td>0.5714286</td>\n",
       "<td>6.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0063866</td>\n",
       "<td>1.0</td>\n",
       "<td>388.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8370580</td>\n",
       "<td>0.9991095</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1408014</td>\n",
       "<td>0.2478839</td>\n",
       "<td>27.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0227196</td>\n",
       "<td>0.6384684</td>\n",
       "<td>251.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0219016</td>\n",
       "<td>0.6682301</td>\n",
       "<td>256.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.0817148    0.285714  72\n",
       "max f2                       0.0817148    0.309446  72\n",
       "max f0point5                 0.140801     0.323529  27\n",
       "max accuracy                 0.347201     0.951736  6\n",
       "max precision                0.347201     0.571429  6\n",
       "max recall                   0.00638656   1         388\n",
       "max specificity              0.837058     0.99911   0\n",
       "max absolute_mcc             0.140801     0.247884  27\n",
       "max min_per_class_accuracy   0.0227196    0.638468  251\n",
       "max mean_per_class_accuracy  0.0219016    0.66823   256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4.91 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101609</td>\n",
       "<td>0.2056922</td>\n",
       "<td>8.4841954</td>\n",
       "<td>8.4841954</td>\n",
       "<td>0.4166667</td>\n",
       "<td>0.4166667</td>\n",
       "<td>0.0862069</td>\n",
       "<td>0.0862069</td>\n",
       "<td>748.4195402</td>\n",
       "<td>748.4195402</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0203218</td>\n",
       "<td>0.1436148</td>\n",
       "<td>6.7873563</td>\n",
       "<td>7.6357759</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.375</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.1551724</td>\n",
       "<td>578.7356322</td>\n",
       "<td>663.5775862</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0304826</td>\n",
       "<td>0.1169444</td>\n",
       "<td>3.3936782</td>\n",
       "<td>6.2217433</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.3055556</td>\n",
       "<td>0.0344828</td>\n",
       "<td>0.1896552</td>\n",
       "<td>239.3678161</td>\n",
       "<td>522.1743295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0406435</td>\n",
       "<td>0.1027651</td>\n",
       "<td>0.0</td>\n",
       "<td>4.6663075</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2291667</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1896552</td>\n",
       "<td>-100.0</td>\n",
       "<td>366.6307471</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0508044</td>\n",
       "<td>0.0930130</td>\n",
       "<td>5.0905172</td>\n",
       "<td>4.7511494</td>\n",
       "<td>0.25</td>\n",
       "<td>0.2333333</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.2413793</td>\n",
       "<td>409.0517241</td>\n",
       "<td>375.1149425</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1007621</td>\n",
       "<td>0.0609932</td>\n",
       "<td>2.0707189</td>\n",
       "<td>3.4221965</td>\n",
       "<td>0.1016949</td>\n",
       "<td>0.1680672</td>\n",
       "<td>0.1034483</td>\n",
       "<td>0.3448276</td>\n",
       "<td>107.0718878</td>\n",
       "<td>242.2196465</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1507197</td>\n",
       "<td>0.0475663</td>\n",
       "<td>1.0353594</td>\n",
       "<td>2.6310539</td>\n",
       "<td>0.0508475</td>\n",
       "<td>0.1292135</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.3965517</td>\n",
       "<td>3.5359439</td>\n",
       "<td>163.1053855</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2006774</td>\n",
       "<td>0.0383641</td>\n",
       "<td>0.6902396</td>\n",
       "<td>2.1478976</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.1054852</td>\n",
       "<td>0.0344828</td>\n",
       "<td>0.4310345</td>\n",
       "<td>-30.9760374</td>\n",
       "<td>114.7897570</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3005927</td>\n",
       "<td>0.0277649</td>\n",
       "<td>1.3804793</td>\n",
       "<td>1.8928120</td>\n",
       "<td>0.0677966</td>\n",
       "<td>0.0929577</td>\n",
       "<td>0.1379310</td>\n",
       "<td>0.5689655</td>\n",
       "<td>38.0479252</td>\n",
       "<td>89.2812045</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4005080</td>\n",
       "<td>0.0209429</td>\n",
       "<td>1.3804793</td>\n",
       "<td>1.7649996</td>\n",
       "<td>0.0677966</td>\n",
       "<td>0.0866808</td>\n",
       "<td>0.1379310</td>\n",
       "<td>0.7068966</td>\n",
       "<td>38.0479252</td>\n",
       "<td>76.4999635</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5004234</td>\n",
       "<td>0.0177512</td>\n",
       "<td>0.5176797</td>\n",
       "<td>1.5159578</td>\n",
       "<td>0.0254237</td>\n",
       "<td>0.0744501</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.7586207</td>\n",
       "<td>-48.2320281</td>\n",
       "<td>51.5957757</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6003387</td>\n",
       "<td>0.0146230</td>\n",
       "<td>0.6902396</td>\n",
       "<td>1.3785322</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.0677010</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.8275862</td>\n",
       "<td>-30.9760374</td>\n",
       "<td>37.8532173</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7002540</td>\n",
       "<td>0.0124380</td>\n",
       "<td>0.5176797</td>\n",
       "<td>1.2557020</td>\n",
       "<td>0.0254237</td>\n",
       "<td>0.0616687</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.8793103</td>\n",
       "<td>-48.2320281</td>\n",
       "<td>25.5701956</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8001693</td>\n",
       "<td>0.0107031</td>\n",
       "<td>0.3451198</td>\n",
       "<td>1.1419996</td>\n",
       "<td>0.0169492</td>\n",
       "<td>0.0560847</td>\n",
       "<td>0.0344828</td>\n",
       "<td>0.9137931</td>\n",
       "<td>-65.4880187</td>\n",
       "<td>14.1999635</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000847</td>\n",
       "<td>0.0085770</td>\n",
       "<td>0.6902396</td>\n",
       "<td>1.0918513</td>\n",
       "<td>0.0338983</td>\n",
       "<td>0.0536218</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.9827586</td>\n",
       "<td>-30.9760374</td>\n",
       "<td>9.1851299</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0036475</td>\n",
       "<td>0.1725599</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0084746</td>\n",
       "<td>0.0491109</td>\n",
       "<td>0.0172414</td>\n",
       "<td>1.0</td>\n",
       "<td>-82.7440094</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain     cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -------  -----------------  ---------------  --------------------------  --------------  -------------------------  -------  -----------------\n",
       "    1        0.0101609                   0.205692           8.4842   8.4842             0.416667         0.416667                    0.0862069       0.0862069                  748.42   748.42\n",
       "    2        0.0203218                   0.143615           6.78736  7.63578            0.333333         0.375                       0.0689655       0.155172                   578.736  663.578\n",
       "    3        0.0304826                   0.116944           3.39368  6.22174            0.166667         0.305556                    0.0344828       0.189655                   239.368  522.174\n",
       "    4        0.0406435                   0.102765           0        4.66631            0                0.229167                    0               0.189655                   -100     366.631\n",
       "    5        0.0508044                   0.093013           5.09052  4.75115            0.25             0.233333                    0.0517241       0.241379                   409.052  375.115\n",
       "    6        0.100762                    0.0609932          2.07072  3.4222             0.101695         0.168067                    0.103448        0.344828                   107.072  242.22\n",
       "    7        0.15072                     0.0475663          1.03536  2.63105            0.0508475        0.129213                    0.0517241       0.396552                   3.53594  163.105\n",
       "    8        0.200677                    0.0383641          0.69024  2.1479             0.0338983        0.105485                    0.0344828       0.431034                   -30.976  114.79\n",
       "    9        0.300593                    0.0277649          1.38048  1.89281            0.0677966        0.0929577                   0.137931        0.568966                   38.0479  89.2812\n",
       "    10       0.400508                    0.0209429          1.38048  1.765              0.0677966        0.0866808                   0.137931        0.706897                   38.0479  76.5\n",
       "    11       0.500423                    0.0177512          0.51768  1.51596            0.0254237        0.0744501                   0.0517241       0.758621                   -48.232  51.5958\n",
       "    12       0.600339                    0.014623           0.69024  1.37853            0.0338983        0.067701                    0.0689655       0.827586                   -30.976  37.8532\n",
       "    13       0.700254                    0.012438           0.51768  1.2557             0.0254237        0.0616687                   0.0517241       0.87931                    -48.232  25.5702\n",
       "    14       0.800169                    0.0107031          0.34512  1.142              0.0169492        0.0560847                   0.0344828       0.913793                   -65.488  14.2\n",
       "    15       0.900085                    0.00857705         0.69024  1.09185            0.0338983        0.0536218                   0.0689655       0.982759                   -30.976  9.18513\n",
       "    16       1                           0.00364745         0.17256  1                  0.00847458       0.0491109                   0.0172414       1                          -82.744  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.042897149903799614\n",
      "RMSE: 0.20711627146074163\n",
      "LogLoss: 0.1860541583463351\n",
      "Mean Per-Class Error: 0.36826227485332985\n",
      "AUC: 0.6680335500151464\n",
      "Gini: 0.33606710003029283\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.06265996800929555: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>4269.0</td>\n",
       "<td>404.0</td>\n",
       "<td>0.0865</td>\n",
       "<td> (404.0/4673.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>162.0</td>\n",
       "<td>69.0</td>\n",
       "<td>0.7013</td>\n",
       "<td> (162.0/231.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>4431.0</td>\n",
       "<td>473.0</td>\n",
       "<td>0.1154</td>\n",
       "<td> (566.0/4904.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0     1    Error    Rate\n",
       "-----  ----  ---  -------  --------------\n",
       "0      4269  404  0.0865   (404.0/4673.0)\n",
       "1      162   69   0.7013   (162.0/231.0)\n",
       "Total  4431  473  0.1154   (566.0/4904.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0626600</td>\n",
       "<td>0.1960227</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0385108</td>\n",
       "<td>0.2700445</td>\n",
       "<td>244.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3073742</td>\n",
       "<td>0.2882206</td>\n",
       "<td>39.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4312340</td>\n",
       "<td>0.9545269</td>\n",
       "<td>22.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.6418320</td>\n",
       "<td>0.75</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0031637</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8182682</td>\n",
       "<td>0.9997860</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3073742</td>\n",
       "<td>0.2195738</td>\n",
       "<td>39.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0198797</td>\n",
       "<td>0.6147186</td>\n",
       "<td>309.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0262994</td>\n",
       "<td>0.6317377</td>\n",
       "<td>283.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.06266      0.196023  194\n",
       "max f2                       0.0385108    0.270045  244\n",
       "max f0point5                 0.307374     0.288221  39\n",
       "max accuracy                 0.431234     0.954527  22\n",
       "max precision                0.641832     0.75      3\n",
       "max recall                   0.00316367   1         399\n",
       "max specificity              0.818268     0.999786  0\n",
       "max absolute_mcc             0.307374     0.219574  39\n",
       "max min_per_class_accuracy   0.0198797    0.614719  309\n",
       "max mean_per_class_accuracy  0.0262994    0.631738  283"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4.71 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101958</td>\n",
       "<td>0.2759044</td>\n",
       "<td>9.7655411</td>\n",
       "<td>9.7655411</td>\n",
       "<td>0.46</td>\n",
       "<td>0.46</td>\n",
       "<td>0.0995671</td>\n",
       "<td>0.0995671</td>\n",
       "<td>876.5541126</td>\n",
       "<td>876.5541126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0201876</td>\n",
       "<td>0.1768234</td>\n",
       "<td>2.5995229</td>\n",
       "<td>6.2187240</td>\n",
       "<td>0.1224490</td>\n",
       "<td>0.2929293</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1255411</td>\n",
       "<td>159.9522926</td>\n",
       "<td>521.8724037</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0301794</td>\n",
       "<td>0.1357597</td>\n",
       "<td>1.7330153</td>\n",
       "<td>4.7335907</td>\n",
       "<td>0.0816327</td>\n",
       "<td>0.2229730</td>\n",
       "<td>0.0173160</td>\n",
       "<td>0.1428571</td>\n",
       "<td>73.3015284</td>\n",
       "<td>373.3590734</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0401713</td>\n",
       "<td>0.1159468</td>\n",
       "<td>2.5995229</td>\n",
       "<td>4.2027820</td>\n",
       "<td>0.1224490</td>\n",
       "<td>0.1979695</td>\n",
       "<td>0.0259740</td>\n",
       "<td>0.1688312</td>\n",
       "<td>159.9522926</td>\n",
       "<td>320.2781990</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0501631</td>\n",
       "<td>0.0987736</td>\n",
       "<td>1.2997615</td>\n",
       "<td>3.6245381</td>\n",
       "<td>0.0612245</td>\n",
       "<td>0.1707317</td>\n",
       "<td>0.0129870</td>\n",
       "<td>0.1818182</td>\n",
       "<td>29.9761463</td>\n",
       "<td>262.4538064</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1001223</td>\n",
       "<td>0.0604267</td>\n",
       "<td>2.4262214</td>\n",
       "<td>3.0266000</td>\n",
       "<td>0.1142857</td>\n",
       "<td>0.1425662</td>\n",
       "<td>0.1212121</td>\n",
       "<td>0.3030303</td>\n",
       "<td>142.6221398</td>\n",
       "<td>202.6600012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500816</td>\n",
       "<td>0.0440954</td>\n",
       "<td>1.2131107</td>\n",
       "<td>2.4229249</td>\n",
       "<td>0.0571429</td>\n",
       "<td>0.1141304</td>\n",
       "<td>0.0606061</td>\n",
       "<td>0.3636364</td>\n",
       "<td>21.3110699</td>\n",
       "<td>142.2924901</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000408</td>\n",
       "<td>0.0351749</td>\n",
       "<td>1.2997615</td>\n",
       "<td>2.1424203</td>\n",
       "<td>0.0612245</td>\n",
       "<td>0.1009174</td>\n",
       "<td>0.0649351</td>\n",
       "<td>0.4285714</td>\n",
       "<td>29.9761463</td>\n",
       "<td>114.2420271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999592</td>\n",
       "<td>0.0247010</td>\n",
       "<td>1.1697853</td>\n",
       "<td>1.8184290</td>\n",
       "<td>0.0551020</td>\n",
       "<td>0.0856560</td>\n",
       "<td>0.1168831</td>\n",
       "<td>0.5454545</td>\n",
       "<td>16.9785317</td>\n",
       "<td>81.8429022</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000816</td>\n",
       "<td>0.0192419</td>\n",
       "<td>0.6917943</td>\n",
       "<td>1.5364832</td>\n",
       "<td>0.0325866</td>\n",
       "<td>0.0723751</td>\n",
       "<td>0.0692641</td>\n",
       "<td>0.6147186</td>\n",
       "<td>-30.8205711</td>\n",
       "<td>53.6483225</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0154639</td>\n",
       "<td>0.9964838</td>\n",
       "<td>1.4285714</td>\n",
       "<td>0.0469388</td>\n",
       "<td>0.0672920</td>\n",
       "<td>0.0995671</td>\n",
       "<td>0.7142857</td>\n",
       "<td>-0.3516212</td>\n",
       "<td>42.8571429</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999184</td>\n",
       "<td>0.0127343</td>\n",
       "<td>0.4332538</td>\n",
       "<td>1.2627979</td>\n",
       "<td>0.0204082</td>\n",
       "<td>0.0594833</td>\n",
       "<td>0.0432900</td>\n",
       "<td>0.7575758</td>\n",
       "<td>-56.6746179</td>\n",
       "<td>26.2797932</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000408</td>\n",
       "<td>0.0105025</td>\n",
       "<td>0.8647429</td>\n",
       "<td>1.2058667</td>\n",
       "<td>0.0407332</td>\n",
       "<td>0.0568016</td>\n",
       "<td>0.0865801</td>\n",
       "<td>0.8441558</td>\n",
       "<td>-13.5257139</td>\n",
       "<td>20.5866665</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999592</td>\n",
       "<td>0.0087478</td>\n",
       "<td>0.5199046</td>\n",
       "<td>1.1201870</td>\n",
       "<td>0.0244898</td>\n",
       "<td>0.0527657</td>\n",
       "<td>0.0519481</td>\n",
       "<td>0.8961039</td>\n",
       "<td>-48.0095415</td>\n",
       "<td>12.0186976</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8998777</td>\n",
       "<td>0.0067856</td>\n",
       "<td>0.3899284</td>\n",
       "<td>1.0391023</td>\n",
       "<td>0.0183673</td>\n",
       "<td>0.0489463</td>\n",
       "<td>0.0389610</td>\n",
       "<td>0.9350649</td>\n",
       "<td>-61.0071561</td>\n",
       "<td>3.9102298</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0029427</td>\n",
       "<td>0.6485571</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0305499</td>\n",
       "<td>0.0471044</td>\n",
       "<td>0.0649351</td>\n",
       "<td>1.0</td>\n",
       "<td>-35.1442854</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain       cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  ---------  -----------------\n",
       "    1        0.0101958                   0.275904           9.76554   9.76554            0.46             0.46                        0.0995671       0.0995671                  876.554    876.554\n",
       "    2        0.0201876                   0.176823           2.59952   6.21872            0.122449         0.292929                    0.025974        0.125541                   159.952    521.872\n",
       "    3        0.0301794                   0.13576            1.73302   4.73359            0.0816327        0.222973                    0.017316        0.142857                   73.3015    373.359\n",
       "    4        0.0401713                   0.115947           2.59952   4.20278            0.122449         0.19797                     0.025974        0.168831                   159.952    320.278\n",
       "    5        0.0501631                   0.0987736          1.29976   3.62454            0.0612245        0.170732                    0.012987        0.181818                   29.9761    262.454\n",
       "    6        0.100122                    0.0604267          2.42622   3.0266             0.114286         0.142566                    0.121212        0.30303                    142.622    202.66\n",
       "    7        0.150082                    0.0440954          1.21311   2.42292            0.0571429        0.11413                     0.0606061       0.363636                   21.3111    142.292\n",
       "    8        0.200041                    0.0351749          1.29976   2.14242            0.0612245        0.100917                    0.0649351       0.428571                   29.9761    114.242\n",
       "    9        0.299959                    0.024701           1.16979   1.81843            0.055102         0.085656                    0.116883        0.545455                   16.9785    81.8429\n",
       "    10       0.400082                    0.0192419          0.691794  1.53648            0.0325866        0.0723751                   0.0692641       0.614719                   -30.8206   53.6483\n",
       "    11       0.5                         0.0154639          0.996484  1.42857            0.0469388        0.067292                    0.0995671       0.714286                   -0.351621  42.8571\n",
       "    12       0.599918                    0.0127343          0.433254  1.2628             0.0204082        0.0594833                   0.04329         0.757576                   -56.6746   26.2798\n",
       "    13       0.700041                    0.0105025          0.864743  1.20587            0.0407332        0.0568016                   0.0865801       0.844156                   -13.5257   20.5867\n",
       "    14       0.799959                    0.00874779         0.519905  1.12019            0.0244898        0.0527657                   0.0519481       0.896104                   -48.0095   12.0187\n",
       "    15       0.899878                    0.00678564         0.389928  1.0391             0.0183673        0.0489463                   0.038961        0.935065                   -61.0072   3.91023\n",
       "    16       1                           0.00294275         0.648557  1                  0.0305499        0.0471044                   0.0649351       1                          -35.1443   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>0.8966253</td>\n",
       "<td>0.0547259</td>\n",
       "<td>0.7502549</td>\n",
       "<td>0.9592253</td>\n",
       "<td>0.9408767</td>\n",
       "<td>0.8858308</td>\n",
       "<td>0.9469388</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>0.6689358</td>\n",
       "<td>0.0309648</td>\n",
       "<td>0.6292334</td>\n",
       "<td>0.7481481</td>\n",
       "<td>0.6305302</td>\n",
       "<td>0.6569443</td>\n",
       "<td>0.6798231</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.1033747</td>\n",
       "<td>0.0547259</td>\n",
       "<td>0.2497452</td>\n",
       "<td>0.0407747</td>\n",
       "<td>0.0591233</td>\n",
       "<td>0.1141692</td>\n",
       "<td>0.0530612</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>101.4</td>\n",
       "<td>53.690968</td>\n",
       "<td>245.0</td>\n",
       "<td>40.0</td>\n",
       "<td>58.0</td>\n",
       "<td>112.0</td>\n",
       "<td>52.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>0.2732962</td>\n",
       "<td>0.0946900</td>\n",
       "<td>0.0904523</td>\n",
       "<td>0.4320988</td>\n",
       "<td>0.2439024</td>\n",
       "<td>0.1801802</td>\n",
       "<td>0.4198473</td></tr>\n",
       "<tr><td>f1</td>\n",
       "<td>0.2246218</td>\n",
       "<td>0.0398393</td>\n",
       "<td>0.1281139</td>\n",
       "<td>0.2592592</td>\n",
       "<td>0.2162162</td>\n",
       "<td>0.2222222</td>\n",
       "<td>0.2972973</td></tr>\n",
       "<tr><td>f2</td>\n",
       "<td>0.2237705</td>\n",
       "<td>0.0260584</td>\n",
       "<td>0.2195122</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.1941748</td>\n",
       "<td>0.2898551</td>\n",
       "<td>0.2301255</td></tr>\n",
       "<tr><td>lift_top_group</td>\n",
       "<td>10.38274</td>\n",
       "<td>2.9475746</td>\n",
       "<td>4.562791</td>\n",
       "<td>15.26</td>\n",
       "<td>11.147727</td>\n",
       "<td>6.6886363</td>\n",
       "<td>14.254545</td></tr>\n",
       "<tr><td>logloss</td>\n",
       "<td>0.1860590</td>\n",
       "<td>0.0099820</td>\n",
       "<td>0.1886031</td>\n",
       "<td>0.1659469</td>\n",
       "<td>0.1826269</td>\n",
       "<td>0.1832709</td>\n",
       "<td>0.2098472</td></tr>\n",
       "<tr><td>max_per_class_error</td>\n",
       "<td>0.7360771</td>\n",
       "<td>0.0751266</td>\n",
       "<td>0.5813953</td>\n",
       "<td>0.8444445</td>\n",
       "<td>0.8181818</td>\n",
       "<td>0.6363636</td>\n",
       "<td>0.8</td></tr>\n",
       "<tr><td>mcc</td>\n",
       "<td>0.2243456</td>\n",
       "<td>0.0654080</td>\n",
       "<td>0.0879062</td>\n",
       "<td>0.3366421</td>\n",
       "<td>0.1903413</td>\n",
       "<td>0.1874307</td>\n",
       "<td>0.3194076</td></tr>\n",
       "<tr><td>mean_per_class_accuracy</td>\n",
       "<td>0.5961161</td>\n",
       "<td>0.0153353</td>\n",
       "<td>0.5920315</td>\n",
       "<td>0.5767094</td>\n",
       "<td>0.5791695</td>\n",
       "<td>0.6369943</td>\n",
       "<td>0.5956757</td></tr>\n",
       "<tr><td>mean_per_class_error</td>\n",
       "<td>0.4038839</td>\n",
       "<td>0.0153353</td>\n",
       "<td>0.4079685</td>\n",
       "<td>0.4232906</td>\n",
       "<td>0.4208305</td>\n",
       "<td>0.3630057</td>\n",
       "<td>0.4043243</td></tr>\n",
       "<tr><td>mse</td>\n",
       "<td>0.0428984</td>\n",
       "<td>0.0022812</td>\n",
       "<td>0.0423650</td>\n",
       "<td>0.0398324</td>\n",
       "<td>0.0412788</td>\n",
       "<td>0.0418947</td>\n",
       "<td>0.0491212</td></tr>\n",
       "<tr><td>precision</td>\n",
       "<td>0.3718044</td>\n",
       "<td>0.1874451</td>\n",
       "<td>0.0756303</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.2666667</td>\n",
       "<td>0.16</td>\n",
       "<td>0.5789474</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.0420648</td>\n",
       "<td>0.0254078</td>\n",
       "<td>-0.0108196</td>\n",
       "<td>0.0899068</td>\n",
       "<td>0.0364531</td>\n",
       "<td>0.0220767</td>\n",
       "<td>0.0727068</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>0.2639230</td>\n",
       "<td>0.0751266</td>\n",
       "<td>0.4186046</td>\n",
       "<td>0.1555556</td>\n",
       "<td>0.1818182</td>\n",
       "<td>0.3636364</td>\n",
       "<td>0.2</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.2069790</td>\n",
       "<td>0.0053905</td>\n",
       "<td>0.2058275</td>\n",
       "<td>0.1995805</td>\n",
       "<td>0.2031719</td>\n",
       "<td>0.2046819</td>\n",
       "<td>0.2216331</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>0.9283092</td>\n",
       "<td>0.0616273</td>\n",
       "<td>0.7654584</td>\n",
       "<td>0.9978632</td>\n",
       "<td>0.9765208</td>\n",
       "<td>0.9103522</td>\n",
       "<td>0.9913514</td></tr></table></div>"
      ],
      "text/plain": [
       "                         mean       sd          cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n",
       "-----------------------  ---------  ----------  ------------  ------------  ------------  ------------  ------------\n",
       "accuracy                 0.896625   0.0547259   0.750255      0.959225      0.940877      0.885831      0.946939\n",
       "auc                      0.668936   0.0309648   0.629233      0.748148      0.63053       0.656944      0.679823\n",
       "err                      0.103375   0.0547259   0.249745      0.0407747     0.0591233     0.114169      0.0530612\n",
       "err_count                101.4      53.691      245           40            58            112           52\n",
       "f0point5                 0.273296   0.09469     0.0904523     0.432099      0.243902      0.18018       0.419847\n",
       "f1                       0.224622   0.0398393   0.128114      0.259259      0.216216      0.222222      0.297297\n",
       "f2                       0.223771   0.0260584   0.219512      0.185185      0.194175      0.289855      0.230126\n",
       "lift_top_group           10.3827    2.94757     4.56279       15.26         11.1477       6.68864       14.2545\n",
       "logloss                  0.186059   0.00998199  0.188603      0.165947      0.182627      0.183271      0.209847\n",
       "max_per_class_error      0.736077   0.0751266   0.581395      0.844444      0.818182      0.636364      0.8\n",
       "mcc                      0.224346   0.065408    0.0879062     0.336642      0.190341      0.187431      0.319408\n",
       "mean_per_class_accuracy  0.596116   0.0153353   0.592032      0.576709      0.57917       0.636994      0.595676\n",
       "mean_per_class_error     0.403884   0.0153353   0.407968      0.423291      0.420831      0.363006      0.404324\n",
       "mse                      0.0428984  0.00228121  0.042365      0.0398324     0.0412788     0.0418947     0.0491212\n",
       "precision                0.371804   0.187445    0.0756303     0.777778      0.266667      0.16          0.578947\n",
       "r2                       0.0420648  0.0254078   -0.0108196    0.0899068     0.0364531     0.0220767     0.0727068\n",
       "recall                   0.263923   0.0751266   0.418605      0.155556      0.181818      0.363636      0.2\n",
       "rmse                     0.206979   0.00539053  0.205828      0.19958       0.203172      0.204682      0.221633\n",
       "specificity              0.928309   0.0616273   0.765458      0.997863      0.976521      0.910352      0.991351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:55</td>\n",
       "<td>12.264 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.2118622</td>\n",
       "<td>0.1898994</td>\n",
       "<td>0.5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9528956</td>\n",
       "<td>0.2161089</td>\n",
       "<td>0.1959333</td>\n",
       "<td>0.5</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9508891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:56</td>\n",
       "<td>12.528 sec</td>\n",
       "<td>5.0</td>\n",
       "<td>0.1912917</td>\n",
       "<td>0.1436173</td>\n",
       "<td>0.9263643</td>\n",
       "<td>18.2573160</td>\n",
       "<td>0.0391517</td>\n",
       "<td>0.2131652</td>\n",
       "<td>0.1861076</td>\n",
       "<td>0.6754076</td>\n",
       "<td>5.0905172</td>\n",
       "<td>0.0982218</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:56</td>\n",
       "<td>12.836 sec</td>\n",
       "<td>10.0</td>\n",
       "<td>0.1692332</td>\n",
       "<td>0.1106636</td>\n",
       "<td>0.9760876</td>\n",
       "<td>20.8048485</td>\n",
       "<td>0.0203915</td>\n",
       "<td>0.2114478</td>\n",
       "<td>0.1813669</td>\n",
       "<td>0.7087159</td>\n",
       "<td>8.4841954</td>\n",
       "<td>0.0685859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:56</td>\n",
       "<td>13.132 sec</td>\n",
       "<td>15.0</td>\n",
       "<td>0.1546803</td>\n",
       "<td>0.0932565</td>\n",
       "<td>0.9871292</td>\n",
       "<td>21.2294372</td>\n",
       "<td>0.0134584</td>\n",
       "<td>0.2111540</td>\n",
       "<td>0.1813863</td>\n",
       "<td>0.6947524</td>\n",
       "<td>8.4841954</td>\n",
       "<td>0.1278577</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:56</td>\n",
       "<td>13.416 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.1424780</td>\n",
       "<td>0.0806138</td>\n",
       "<td>0.9919418</td>\n",
       "<td>21.2294372</td>\n",
       "<td>0.0103997</td>\n",
       "<td>0.2118183</td>\n",
       "<td>0.1826487</td>\n",
       "<td>0.7049698</td>\n",
       "<td>10.1810345</td>\n",
       "<td>0.0499577</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:57</td>\n",
       "<td>13.676 sec</td>\n",
       "<td>25.0</td>\n",
       "<td>0.1312188</td>\n",
       "<td>0.0704007</td>\n",
       "<td>0.9942231</td>\n",
       "<td>21.2294372</td>\n",
       "<td>0.0065253</td>\n",
       "<td>0.2118712</td>\n",
       "<td>0.1838461</td>\n",
       "<td>0.6983450</td>\n",
       "<td>6.7873563</td>\n",
       "<td>0.0770533</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:57</td>\n",
       "<td>13.936 sec</td>\n",
       "<td>30.0</td>\n",
       "<td>0.1201879</td>\n",
       "<td>0.0611406</td>\n",
       "<td>0.9963440</td>\n",
       "<td>21.2294372</td>\n",
       "<td>0.0048940</td>\n",
       "<td>0.2121802</td>\n",
       "<td>0.1844297</td>\n",
       "<td>0.7119707</td>\n",
       "<td>8.4841954</td>\n",
       "<td>0.0736664</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2018-08-14 18:04:57</td>\n",
       "<td>14.188 sec</td>\n",
       "<td>34.0</td>\n",
       "<td>0.1111113</td>\n",
       "<td>0.0545110</td>\n",
       "<td>0.9970282</td>\n",
       "<td>21.2294372</td>\n",
       "<td>0.0034666</td>\n",
       "<td>0.2124073</td>\n",
       "<td>0.1856861</td>\n",
       "<td>0.7111800</td>\n",
       "<td>8.4841954</td>\n",
       "<td>0.0804403</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -----------------  ---------------------------------\n",
       "    2018-08-14 18:04:55  12.264 sec  0                  0.211862         0.189899            0.5             1                0.952896                         0.216109           0.195933              0.5               1                  0.950889\n",
       "    2018-08-14 18:04:56  12.528 sec  5                  0.191292         0.143617            0.926364        18.2573          0.0391517                        0.213165           0.186108              0.675408          5.09052            0.0982218\n",
       "    2018-08-14 18:04:56  12.836 sec  10                 0.169233         0.110664            0.976088        20.8048          0.0203915                        0.211448           0.181367              0.708716          8.4842             0.0685859\n",
       "    2018-08-14 18:04:56  13.132 sec  15                 0.15468          0.0932565           0.987129        21.2294          0.0134584                        0.211154           0.181386              0.694752          8.4842             0.127858\n",
       "    2018-08-14 18:04:56  13.416 sec  20                 0.142478         0.0806138           0.991942        21.2294          0.0103997                        0.211818           0.182649              0.70497           10.181             0.0499577\n",
       "    2018-08-14 18:04:57  13.676 sec  25                 0.131219         0.0704007           0.994223        21.2294          0.00652529                       0.211871           0.183846              0.698345          6.78736            0.0770533\n",
       "    2018-08-14 18:04:57  13.936 sec  30                 0.120188         0.0611406           0.996344        21.2294          0.00489396                       0.21218            0.18443               0.711971          8.4842             0.0736664\n",
       "    2018-08-14 18:04:57  14.188 sec  34                 0.111111         0.054511            0.997028        21.2294          0.00346656                       0.212407           0.185686              0.71118           8.4842             0.0804403"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>15.0153389</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0307435</td></tr>\n",
       "<tr><td>71</td>\n",
       "<td>12.2121620</td>\n",
       "<td>0.8133124</td>\n",
       "<td>0.0250040</td></tr>\n",
       "<tr><td>pca_12</td>\n",
       "<td>11.1265478</td>\n",
       "<td>0.7410121</td>\n",
       "<td>0.0227813</td></tr>\n",
       "<tr><td>pca_0</td>\n",
       "<td>10.4576960</td>\n",
       "<td>0.6964675</td>\n",
       "<td>0.0214118</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>10.4030600</td>\n",
       "<td>0.6928288</td>\n",
       "<td>0.0213000</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>59</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>60</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>62</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>64</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>66</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance    percentage\n",
       "----------  ---------------------  -------------------  --------------------\n",
       "2           15.015338897705078     1.0                  0.03074346551361264\n",
       "71          12.212162017822266     0.8133124467599432   0.025004043158756227\n",
       "pca_12      11.126547813415527     0.7410121003073792   0.02278127995096958\n",
       "pca_0       10.457695960998535     0.6964675277889915   0.02141182542193191\n",
       "3           10.403059959411621     0.6928288485717501   0.021299959812901552\n",
       "---         ---                    ---                  ---\n",
       "59          0.0                    0.0                  0.0\n",
       "60          0.0                    0.0                  0.0\n",
       "62          0.0                    0.0                  0.0\n",
       "64          0.0                    0.0                  0.0\n",
       "66          0.0                    0.0                  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials, best, aml = h2o_automl.auto_ml(train_path='X_train.csv', valid_path='X_valid.csv', response='response', time_to_run=60)\n",
    "\n",
    "print(aml.leader.model_performance(h2o_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_automl.get_score(aml=aml, h2o_valid=h2o_valid, y_valid=y_valid, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_bd02 closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
