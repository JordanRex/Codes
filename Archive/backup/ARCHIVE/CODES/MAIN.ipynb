{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', '.svn', '2. Target Risk Analysis - Univariates, Bivariates and Missing Value Treatment.py', '3 month iteration results', 'archive', 'AUTOML_BC-Copy1.ipynb', 'AUTOML_BC.ipynb', 'AUTOML_BC_best.ipynb', 'backup.out.bak', 'backup.out.dir', 'backup_bne.pickle', 'backup_le.pickle', 'backup_le_2.pickle', 'barplots.ipynb', 'branches', 'EDA_NB.ipynb', 'FEATURE_ENGINEERING.ipynb', 'final_distribution.png', 'friday_meeting.csv', 'JUNE18_TURNOVER.xlsx', 'MAIN.ipynb', 'MAIN_best.ipynb', 'MAIN_best2.ipynb', 'Missing Data Imputation.ipynb', 'MODEL INTERPRETER.ipynb', 'MODEL_SELECTION_TUNING_TEST_2017Dec_model3.ipynb', 'MODEL_TOP_FEATURES_DISTRIBUTION_Graphs.ipynb', 'NAZ_missing_distribution.csv', 'New folder', 'OTHER_MODELS.ipynb', 'p.csv', 'pred.csv', 'preparation', 'PREPARATION.ipynb', 'salary_bonus_2017.csv', 'snippets', 'stacking_ensemble', 'tags', 'test2.csv', 'test3.csv', 'test_final.csv', 'train_dec2017.csv', 'train_final.csv', 'train_final_ext_1.csv', 'train_final_ext_2.csv', 'train_june2017.csv', 'trunk', 'Turnover Report NAZ 2017 Year End.csv', 'Turnover Report NAZ 2017 Year End.xlsx', 'turnover-2015-final.csv', 'turnover-2016-final.csv', 'turnover_framework', 'ULTIMATE.ipynb', 'valid_dfs.csv', 'valid_final.csv', 'VERTICAL LEVEL MODELS', 'wrong', 'X.csv', 'X_train.csv', 'X_valid.csv', 'Y.csv', 'y_valid.csv']\n"
     ]
    }
   ],
   "source": [
    "## importing the relevant packages:\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# print/display all plots inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# the base packages\n",
    "import collections # for the Counter function\n",
    "import csv # for reading/writing csv files\n",
    "import pandas as pd, numpy as np, time, gc, bisect\n",
    "\n",
    "# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn import metrics, preprocessing, decomposition\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "import category_encoders as ce\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# modelling algorithms\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV, VarianceThreshold\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Exporting packages for SHAP/LIME\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# missing value imputation\n",
    "from fancyimpute import KNN, MICE #, NuclearNormMinimization\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 10 # number of iterations/parameter sets created towards tuning\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#### MAIN CLASSES ####\n",
    "\n",
    "## Two defined for now ##\n",
    "# 1. DataFrame Imputer\n",
    "#    - for imputing missing values\n",
    "# 2. Prepare Data\n",
    "#    - for sourcing, processing, and returning the train/valid datasets\n",
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "        Columns of other types are imputed with mean of column.\n",
    "        \"\"\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X.groupby(['pay scale group', 'abinbev entity2'])\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        X.groupby('abinbev entity2')\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        X.reset_index(drop=True)\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X], \n",
    "                              index=X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "    def num_missing(self):\n",
    "        return sum(self.isnull())\n",
    "    \n",
    "    def imputer_method(self, column, method=['mean', 'median', 'most_frequent']):\n",
    "        x = Imputer(missing_values = 'NaN', strategy = method, axis = 0)\n",
    "        return x.fit_transform(self[[column]]).ravel()\n",
    "    \n",
    "    def fancy_impute(X, which_method):\n",
    "        \"\"\" currently supported algorithms are KNN, NNM and MICE from the fancyimpute package\n",
    "        which_method = ['KNN', 'NNM', 'MICE']\n",
    "        \"\"\"\n",
    "        print(which_method, ' based missing value imputation is happening ...', '\\n')\n",
    "        \n",
    "        if which_method == 'NNM': X = NuclearNormMinimization().complete(X) # NNM method\n",
    "        if which_method == 'KNN': X = KNN(k=5, verbose=False).complete(X) # KNN method\n",
    "        if which_method == 'MICE':\n",
    "            X_complete_df = X.copy()\n",
    "            mice = MICE(verbose=False)\n",
    "            X_complete = mice.complete(np.asarray(X.values, dtype=float))\n",
    "            X_complete_df.loc[:, X.columns] = X_complete[:][:]\n",
    "            X = X_complete_df\n",
    "        print('missing value imputation completed', '\\n')\n",
    "        return X\n",
    "\n",
    "class prepare_data():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" To prepare data,\n",
    "                1. read in data\n",
    "                2. pre-processing/cleaning\n",
    "                3. creating helper objects for later steps\n",
    "                4. processing for modelling\n",
    "                5. function return objects are the train, valid, response, categ cols/indices, feature names\n",
    "        \"\"\"\n",
    "    \n",
    "    def labelEncoder(train_df, valid_df, categorical_names):\n",
    "        print('label encoding is happening ...', '\\n')\n",
    "        cat_columns = train_df.select_dtypes(include=['object']).columns.values        \n",
    "        for feature in tqdm(cat_columns):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(train_df[feature].astype(str))\n",
    "            train_df[feature] = le.transform(train_df[feature].astype(str))\n",
    "            valid_df[feature] = valid_df[feature].map(lambda i: 'No Data' if i not in le.classes_ else i)\n",
    "            le_classes = le.classes_.tolist()\n",
    "            bisect.insort_left(le_classes, 'No Data')\n",
    "            le.classes_ = le_classes\n",
    "            valid_df[feature] = le.transform(valid_df[feature].astype(str))\n",
    "            categorical_names[feature] = le.classes_\n",
    "        print('label encoding completed', '\\n')\n",
    "        return train_df, valid_df, categorical_names\n",
    "        \n",
    "    def ce_encodings(train_df, valid_df, y_train, y_valid, encoding):\n",
    "        print('category encoding is happening ...', '\\n')\n",
    "        if encoding=='bne':          \n",
    "            enc=ce.BaseNEncoder(base=3)\n",
    "        elif encoding=='be':\n",
    "            enc=ce.BinaryEncoder()\n",
    "        elif encoding=='he':\n",
    "            enc=ce.HashingEncoder()\n",
    "        elif encoding=='oe':\n",
    "            enc=ce.OrdinalEncoder()\n",
    "        elif encoding=='ohe':\n",
    "            enc=ce.BaseNEncoder(base=1)\n",
    "        enc.fit(train_df)\n",
    "        train_df=enc.transform(train_df)\n",
    "        valid_df=enc.transform(valid_df)\n",
    "        print('category encoding completed', '\\n')\n",
    "        return train_df, valid_df\n",
    "    \n",
    "    ## function to get frequency count of elements in a vector/list\n",
    "    def freq_count(input_vector):\n",
    "        return collections.Counter(input_vector)\n",
    "    \n",
    "    def categ_feats(train_df, valid_df, y_train, y_valid, encoding='le'):\n",
    "        x = list(train_df.dtypes)\n",
    "        x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "        categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "        # Get feature names and their values for categorical data (needed for LIME)\n",
    "        cat_columns = train_df.select_dtypes(include=['object']).columns.values\n",
    "        categorical_names = {}\n",
    "\n",
    "        train_df, valid_df = prepare_data.categ_feat_eng(train_df, valid_df, cat_columns)\n",
    "\n",
    "        train_df = train_df.drop(['label'], axis = 1)\n",
    "        valid_df = valid_df.drop(['label'], axis = 1)\n",
    "\n",
    "        train_df_cat = train_df[cat_columns]\n",
    "        num_cols = list(set(train_df.columns)-set(train_df_cat.columns))\n",
    "        train_df_num = train_df[num_cols]\n",
    "        valid_df_cat = valid_df[cat_columns]\n",
    "        valid_df_num = valid_df[num_cols]\n",
    "                \n",
    "        if encoding=='le':\n",
    "            train_df_cat, valid_df_cat, categorical_names = prepare_data.labelEncoder(train_df_cat, valid_df_cat, categorical_names)\n",
    "        elif encoding in ['be', 'bne', 'he', 'oe', 'ohe']:\n",
    "            train_df_cat, valid_df_cat = prepare_data.ce_encodings(train_df_cat, valid_df_cat, y_train, y_valid, encoding)\n",
    "        else :\n",
    "            print('Not supported. Use one of [be, bne, he, oe, ohe]', '\\n')\n",
    "        \n",
    "        train = pd.concat([train_df_cat.reset_index(drop=True), train_df_num], axis=1)\n",
    "        valid = pd.concat([valid_df_cat.reset_index(drop=True), valid_df_num], axis=1)\n",
    "        \n",
    "        print('encoding completed ...', '\\n')\n",
    "\n",
    "        return train, valid, categorical_names, categorical_idx\n",
    "    \n",
    "    def categ_feat_eng(train_df, valid_df, cat_columns):\n",
    "        print('categorical feature engineering is happening ...', '\\n')\n",
    "        \n",
    "        global iter\n",
    "        iter = 0\n",
    "        for i in tqdm(cat_columns):\n",
    "            grouped_df = pd.DataFrame(train_df.groupby([i])['label'].agg(['mean', 'std'])).reset_index()\n",
    "            grouped_df.rename(columns={'mean': str('mean_' + cat_columns[iter]),\n",
    "                                       'std': str('std_' + cat_columns[iter])}, inplace=True)\n",
    "            train_df = pd.merge(train_df, grouped_df, how='left')\n",
    "            valid_df = pd.merge(valid_df, grouped_df, how='left')\n",
    "            iter += 1\n",
    "        return train_df, valid_df\n",
    "    \n",
    "    def datetime_feats(train, valid, cols):\n",
    "        \"\"\" contains a nested function that creates several time dimension features from the datetime columns passed \"\"\"\n",
    "        print('datetime feature engineering is happening ...', '\\n')\n",
    "        \n",
    "        def dt_feats(df, col):\n",
    "            df[col] = pd.to_datetime(df[i])\n",
    "            #df[str(col+'_'+'day')] = df[col].dt.day\n",
    "            df[str(col+'_'+'day_name')] = df[col].dt.day_name\n",
    "            #df[str(col+'_'+'dayofweek')] = df[col].dt.dayofweek\n",
    "            df[str(col+'_'+'dayofyear')] = df[col].dt.dayofyear\n",
    "            #df[str(col+'_'+'days_in_month')] = df[col].dt.days_in_month\n",
    "            df[str(col+'_'+'month')] = df[col].dt.month\n",
    "            df[str(col+'_'+'month_name')] = df[col].dt.month_name\n",
    "            df[str(col+'_'+'quarter')] = df[col].dt.quarter\n",
    "            df[str(col+'_'+'week')] = df[col].dt.week\n",
    "            #df[str(col+'_'+'weekday')] = df[col].dt.weekday\n",
    "            df[str(col+'_'+'year')] = df[col].dt.year\n",
    "            #df[col] = df[col].dt.date\n",
    "            df = df.drop([col], axis = 1)\n",
    "            return df\n",
    "        for i in cols:\n",
    "            train = dt_feats(train, i)\n",
    "            valid = dt_feats(valid, i)\n",
    "        return train, valid\n",
    "    \n",
    "    def create(input_file_path, input_file_path_2, input_file_path_3, response, cols_to_remove = ['id'], random_seed = 1,\n",
    "                            encoding = 'le'):        \n",
    "        train = pd.read_csv(input_file_path, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        test = pd.read_csv(input_file_path_3, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        valid = pd.read_csv(input_file_path_2, na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        train_ext_1 = pd.read_csv('train_final_ext_2.csv', na_values=['No Data', ' ', 'UNKNOWN'])\n",
    "        \n",
    "        # remove all the columns specified by user\n",
    "        train.drop(cols_to_remove, axis = 1, inplace = True)\n",
    "        valid = pd.DataFrame(data = valid[train.columns])\n",
    "        test = pd.DataFrame(data = test[train.columns])\n",
    "        train_ext_1 = pd.DataFrame(data = train_ext_1[train.columns])\n",
    "        \n",
    "        # clean the column names\n",
    "        chars_to_remove = [' ', '.', '(', ')', '__', '-']\n",
    "        for i in chars_to_remove:\n",
    "            train.columns = train.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "            valid.columns = valid.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "            test.columns = test.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "            train_ext_1.columns = train_ext_1.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "        \n",
    "        print('original train df shape is: ', train.shape, '\\n')\n",
    "\n",
    "        train = train.append(pd.DataFrame(data = test), ignore_index=True)\n",
    "        train = train.append(pd.DataFrame(data = train_ext_1), ignore_index=True)\n",
    "        print('new train df shape is: ', train.shape, '\\n')\n",
    "        \n",
    "        # creating the datetime features from the datetime columns\n",
    "        cols = [s for s in train.columns.values if 'date' in s]        \n",
    "        train, valid = prepare_data.datetime_feats(train, valid, cols)\n",
    "        \n",
    "        # missing value threshold control (for both rows and columns)\n",
    "        print(train.shape, '\\n')\n",
    "        train.dropna(thresh=0.6*(train.shape[0]), axis=1, inplace = True)\n",
    "        train.dropna(thresh=0.6*(train.shape[1]), axis=0, inplace = True)\n",
    "        print(train.shape, '\\n')\n",
    "        valid = valid[train.columns]\n",
    "        valid.dropna(thresh=0.6*(valid.shape[0]), axis=1, inplace = True)\n",
    "        train = train[valid.columns]\n",
    "                \n",
    "        # reset the index since inplace operations happened earlier\n",
    "        train.index = pd.RangeIndex(len(train.index))\n",
    "        valid.index = pd.RangeIndex(len(valid.index))\n",
    "        \n",
    "        valid_ids = valid[['original_id', 'label']]\n",
    "        valid_ids.to_csv('valid_dfs.csv', index=False)\n",
    "        valid.drop('original_id', axis=1, inplace=True)\n",
    "        train.drop('original_id', axis=1, inplace=True)\n",
    "        \n",
    "        # the class balance in the training dataset for the response\n",
    "        print(prepare_data.freq_count(train[response]), '\\n')\n",
    "\n",
    "        # shuffle the dataframes so that the training is done in a random order.\n",
    "        #X_train = pd.DataFrame(shuffle(train))\n",
    "        #X_valid = pd.DataFrame(shuffle(valid))\n",
    "        X_train = pd.DataFrame(train)\n",
    "        X_valid = pd.DataFrame(valid)\n",
    "        \n",
    "        # creating the response vector\n",
    "        y_train = X_train[response].values\n",
    "        y_valid = X_valid[response].values\n",
    "        \n",
    "        X_train, X_valid, categ_names, categ_idx = prepare_data.categ_feats(X_train, X_valid, y_train, y_valid, encoding)\n",
    "\n",
    "        # remove highly correlated features to reduce further computation time\n",
    "        print('correlation analysis is happening ...', '\\n')\n",
    "        # Create correlation matrix\n",
    "        corr_matrix = X_train.corr().abs()\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        # Find index of feature columns with correlation greater than 0.8\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]        \n",
    "        # Drop features\n",
    "        print(to_drop, '\\n')\n",
    "        X_train.drop(to_drop, axis=1, inplace=True)\n",
    "        X_valid.drop(to_drop, axis=1, inplace=True)\n",
    "        \n",
    "        # store all feature names\n",
    "        feat_names = X_train.columns.values\n",
    "        feat_names2 = X_valid.columns.values\n",
    "        \n",
    "        # missing value imputation\n",
    "        X_train = DataFrameImputer.fancy_impute(X_train, which_method='KNN')\n",
    "        X_valid = DataFrameImputer.fancy_impute(X_valid, which_method='KNN')\n",
    "        \n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        X_train = pd.DataFrame(data=X_train, columns=feat_names)\n",
    "        X_valid = pd.DataFrame(data=X_valid, columns=feat_names2)\n",
    "        \n",
    "        return X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 39,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original train df shape is:  (6102, 79) \n",
      "\n",
      "new train df shape is:  (12181, 79) \n",
      "\n",
      "datetime feature engineering is happening ... \n",
      "\n",
      "(12181, 91) \n",
      "\n",
      "(10701, 89) \n",
      "\n",
      "Counter({0: 10059, 1: 642}) \n",
      "\n",
      "categorical feature engineering is happening ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:10<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding is happening ... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/45 [00:00<?, ?it/s]C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 45/45 [00:10<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label encoding completed \n",
      "\n",
      "encoding completed ... \n",
      "\n",
      "correlation analysis is happening ... \n",
      "\n",
      "['job_family', 'functional_area', 'g_total_comparison', 'gender_relation', 'target_by_cost_center', 'target_by_org_id', 'target_by_aid_band', 'target_by_me_band', 'target_by_fa_band', 'target_by_cc_band', 'target_by_oid_band', 'gt_percentage_team2', 'std_macro_entity_type', 'mean_target_by_band', 'original_hire_date_quarter', 'ebm_level_of_the_job', 'position_start_date_month', 'mean_m_gender', 'mean_manager_pay_scale', 'std_target_by_cc_band', 'bt_percentage_team2', 'annual_salary', 'mean_target_by_appraiser', 'mean_g_total_comparison_team', 'original_hire_date_dayofyear', 'std_target_by_fa_band', 'gt_percentage_team', 'position_start_date_dayofyear', 'mean_cr_team2', 'mean_original_hire_date_day_name', 'service_months', 'mean_opr_2', 'mean_original_hire_date_month_name', 'std_target_by_org_id', 'std_original_hire_date_day_name', 'std_bonus_comparison_team', 'std_target_by_aid_band', 'std_incentive_flag', 'std_opr_3', 'std_g_total_comparison_team2', 'mean_cr', 'mean_incentive_flag', 'mean_gender_relation', 'mean_position_start_date_day_name', 'mean_target_by_cc_band', 'std_original_hire_date_month_name', 'std_pay_scale_group', 'cr_percentage_team', 'mean_opr_change', 'std_m_gender', 'std_gender', 'mean_gender', 'mean_macro_entity_type', 'std_job_family', 'std_gender_relation', 'std_cr_team', 'mean_target_by_me_band', 'std_target_by_macro_entity', 'std_m_opr_1', 'mean_position_start_date_month_name', 'gt_percentage', 'std_global_job', 'std_cost_center', 'std_m_opr_change', 'std_g_total_comparison', 'original_hire_date_month', 'mean_m_opr_3', 'mean_target_by_cost_center', 'mean_bonus_comparison', 'std_m_opr_2', 'std_opr_1', 'std_position_start_date_month_name', 'mean_opr_1', 'std_bonus_comparison_team2', 'bonus_total', 'std_opr_change', 'position_start_date_year', 'std_target_by_band', 'std_target_by_oid_band'] \n",
      "\n",
      "KNN  based missing value imputation is happening ... \n",
      "\n",
      "missing value imputation completed \n",
      "\n",
      "KNN  based missing value imputation is happening ... \n",
      "\n",
      "missing value imputation completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create data function call\n",
    "# CV approach\n",
    "# train and valid features/response dataframes returned\n",
    "# categorical column names/indices and all feature names also returned\n",
    "\n",
    "X_train, X_valid, y_train, y_valid, categ_names, categ_idx, feat_names = prepare_data.create(input_file_path='train_final.csv',\n",
    "                                                                  input_file_path_2='valid_final.csv', input_file_path_3='test_final.csv',\n",
    "                                                                                             response = 'label',\n",
    "                                cols_to_remove = ['global id', 'pers. subarea text',\n",
    "                                                  'manager global id', 'personnel number manager', \n",
    "                                                  'short text of organizational unit', 'position text', \n",
    "                                                  'physical work location-description', 'physical work location-city',\n",
    "                                                  'manager position desc', 'costcenter description',\n",
    "                                                  'local entity description', 'appraiser id', 'time in position'], encoding = 'le')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16215, 91)\n",
      "(5734, 91)\n",
      "(16215,)\n",
      "(5734,)\n",
      "Counter({0: 15557, 1: 658})\n",
      "Counter({0: 5453, 1: 281})\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# f = open(\"backup_le_2.pickle\", \"wb\")\n",
    "# pickle.dump(X_train, f)\n",
    "# pickle.dump(X_valid, f)\n",
    "# pickle.dump(y_train, f)\n",
    "# pickle.dump(y_valid, f)\n",
    "# f.close()\n",
    "\n",
    "f = open(\"backup_le.pickle\", \"rb\")\n",
    "X_train = pickle.load(f)\n",
    "X_valid = pickle.load(f)\n",
    "y_train = pickle.load(f)\n",
    "y_valid = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "#################################################################################################################################\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "print(X_valid.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "#print(y_test.shape)\n",
    "print(y_valid.shape)\n",
    "\n",
    "print(collections.Counter(y_train))\n",
    "#print(collections.Counter(y_test))\n",
    "print(collections.Counter(y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 39,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "## FEATURE ENGINEERING MODULE\n",
    "\n",
    "- Decomposition features\n",
    "    - PCA\n",
    "    - ICA\n",
    "    - TSVD\n",
    "    - GRP\n",
    "    - SRP\n",
    "    - ...\n",
    "- Clustering output feaatures\n",
    "    - KMeans\n",
    "    - ...\n",
    "- Deterministic features\n",
    "    - Binning\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "class feat_eng():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module contains several functions for creating new features. find below a brief description of each \"\"\"\n",
    "    \n",
    "    def scalers(train, valid, which_method):\n",
    "        if which_method == 'ss':\n",
    "            sc = StandardScaler()\n",
    "            sc.fit(train)\n",
    "            train = pd.DataFrame(sc.transform(train), columns=train.columns.values)\n",
    "            valid = pd.DataFrame(sc.transform(valid), columns=valid.columns.values)\n",
    "            return train, valid # scale all variables to zero mean and unit variance, required for PCA and related\n",
    "        if which_method == 'mm':\n",
    "            mm = MinMaxScaler()\n",
    "            mm.fit(train)\n",
    "            train = pd.DataFrame(mm.transform(train), columns=train.columns.values)\n",
    "            valid = pd.DataFrame(mm.transform(valid), columns=train.columns.values)\n",
    "            return train, valid # use this method to iterate\n",
    "    \n",
    "    def decomp_various():\n",
    "        return None\n",
    "    \n",
    "    def pca_feats(train, valid, n = .95):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            pca_fit = decomposition.PCA(n_components=n)\n",
    "            pca_fit.fit(train)\n",
    "            pca_train = pd.DataFrame(pca_fit.transform(train))\n",
    "            pca_valid = pd.DataFrame(pca_fit.transform(valid))\n",
    "            pca_cols = list(set(list(pca_train)))\n",
    "            pca_cols = ['pca_' + str(s) for s in pca_cols]\n",
    "            pca_train.columns = pca_cols\n",
    "            pca_valid.columns = pca_cols\n",
    "            return pca_train, pca_valid\n",
    "        \n",
    "    def ica_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            ica_fit = decomposition.FastICA(n_components=n)\n",
    "            ica_fit.fit(train)\n",
    "            ica_train = pd.DataFrame(ica_fit.transform(train))\n",
    "            ica_valid = pd.DataFrame(ica_fit.transform(valid))\n",
    "            ica_cols = list(set(list(ica_train)))\n",
    "            ica_cols = ['ica_' + str(s) for s in ica_cols]\n",
    "            ica_train.columns = ica_cols\n",
    "            ica_valid.columns = ica_cols\n",
    "            return ica_train, ica_valid\n",
    "        \n",
    "    def tsvd_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            tsvd_fit = decomposition.TruncatedSVD(n_components=n)\n",
    "            tsvd_fit.fit(train)\n",
    "            tsvd_train = pd.DataFrame(tsvd_fit.transform(train))\n",
    "            tsvd_valid = pd.DataFrame(tsvd_fit.transform(valid))\n",
    "            tsvd_cols = list(set(list(tsvd_train)))\n",
    "            tsvd_cols = ['tsvd_' + str(s) for s in tsvd_cols]\n",
    "            tsvd_train.columns = tsvd_cols\n",
    "            tsvd_valid.columns = tsvd_cols\n",
    "            return tsvd_train, tsvd_valid\n",
    "        \n",
    "    def grp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            grp_fit = GaussianRandomProjection(n_components=n, eps=0.3)\n",
    "            grp_fit.fit(train)\n",
    "            grp_train = pd.DataFrame(grp_fit.transform(train))\n",
    "            grp_valid = pd.DataFrame(grp_fit.transform(valid))\n",
    "            grp_cols = list(set(list(grp_train)))\n",
    "            grp_cols = ['grp_' + str(s) for s in grp_cols]\n",
    "            grp_train.columns = grp_cols\n",
    "            grp_valid.columns = grp_cols\n",
    "            return grp_train, grp_valid\n",
    "    \n",
    "    def srp_feats(train, valid, n = 5):\n",
    "            train, valid = feat_eng.scalers(train, valid, which_method='ss')\n",
    "            srp_fit = SparseRandomProjection(n_components=n, dense_output=True, eps=0.3)\n",
    "            srp_fit.fit(train)\n",
    "            srp_train = pd.DataFrame(srp_fit.transform(train))\n",
    "            srp_valid = pd.DataFrame(srp_fit.transform(valid))\n",
    "            srp_cols = list(set(list(srp_train)))\n",
    "            srp_cols = ['srp_' + str(s) for s in srp_cols]\n",
    "            srp_train.columns = srp_cols\n",
    "            srp_valid.columns = srp_cols\n",
    "            return srp_train, srp_valid\n",
    "        \n",
    "    def return_combined(train, valid, list_objects = ['pca', 'ica', 'tsvd', 'grp', 'srp']):\n",
    "        if 'pca' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), pca_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), pca_valid], axis=1)\n",
    "        if 'ica' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), ica_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), ica_valid], axis=1)\n",
    "        if 'tsvd' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), tsvd_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), tsvd_valid], axis=1)\n",
    "        if 'grp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), grp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), grp_valid], axis=1)\n",
    "        if 'srp' in list_objects:\n",
    "            train = pd.concat([train.reset_index(drop=True), srp_train], axis=1)\n",
    "            valid = pd.concat([valid.reset_index(drop=True), srp_valid], axis=1)\n",
    "        return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\fastica_.py:118: UserWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn('FastICA did not converge. Consider increasing '\n"
     ]
    }
   ],
   "source": [
    "## calling the various feat engineering functions and adding those features\n",
    "## pca, ica, tsvd, grp, srp\n",
    "pca_train, pca_valid = feat_eng.pca_feats(train=X_train, valid=X_valid, n=.95)\n",
    "ica_train, ica_valid = feat_eng.ica_feats(train=X_train, valid=X_valid, n=5)\n",
    "tsvd_train, tsvd_valid = feat_eng.tsvd_feats(train=X_train, valid=X_valid, n=5)\n",
    "grp_train, grp_valid = feat_eng.grp_feats(train=X_train, valid=X_valid, n=5)\n",
    "srp_train, srp_valid = feat_eng.srp_feats(train=X_train, valid=X_valid, n=5)\n",
    "\n",
    "## scale the data\n",
    "X_train, X_valid = feat_eng.scalers(train=X_train, valid=X_valid, which_method='ss')\n",
    "\n",
    "## return the final datasets with the added features\n",
    "X_train, X_valid = feat_eng.return_combined(train = X_train, valid = X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## k-means clustering features\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class kmeans_feats():\n",
    "    def __init__():\n",
    "        \"\"\" module for adding features based on kmeans clusters generated \"\"\"\n",
    "    \n",
    "    def clusterer(train_df, valid_df, n):\n",
    "        clusterer = KMeans(n, random_state=1, init='k-means++')\n",
    "        \n",
    "        # fit the clusterer\n",
    "        clusterer.fit(train_df)\n",
    "        \n",
    "        train_clusters = clusterer.predict(train_df)\n",
    "        valid_clusters = clusterer.predict(valid_df)\n",
    "        \n",
    "        return train_clusters, valid_clusters\n",
    "    \n",
    "    def combine(train_df, valid_df, m=5):\n",
    "        for i in range(2, m):\n",
    "            t, v = kmeans_feats.clusterer(train_df, valid_df, n=i)\n",
    "            col_name = str('kmeans_'+ str(i))\n",
    "            t = pd.DataFrame({col_name: t})\n",
    "            v = pd.DataFrame({col_name: v})\n",
    "            \n",
    "            train_df = pd.concat([train_df.reset_index(drop=True), t], axis=1)\n",
    "            valid_df = pd.concat([valid_df.reset_index(drop=True), v], axis=1)\n",
    "            \n",
    "        return train_df, valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid = kmeans_feats.combine(X_train, X_valid, m=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## FEATURE SELECTION\n",
    "\n",
    "class feat_selection():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module is for dynamic feature selection after all the processing and feat engineering phases. ideally this\n",
    "        module is followed by the modelling phase immediately \"\"\"\n",
    "\n",
    "    # removing near zero variance columns\n",
    "    def variance_threshold_selector(train, valid, threshold):\n",
    "        print('input data shape is: ', train.shape, '\\n')\n",
    "        selector = VarianceThreshold(threshold)\n",
    "        selector.fit(train)\n",
    "        X = train[train.columns[selector.get_support(indices=True)]]\n",
    "        Y = valid[valid.columns[selector.get_support(indices=True)]]\n",
    "        #display(pd.DataFrame(X.head(5)))\n",
    "        print('output data shape is: ', X.shape, '\\n')\n",
    "        return X, Y\n",
    "\n",
    "    # using RFECV\n",
    "    def rfecv(train, valid, y_train):\n",
    "        # Create the RFE object and compute a cross-validated score.\n",
    "        #model = LogisticRegression(C=0.1, penalty='l1')\n",
    "        model = RandomForestClassifier(max_depth=7, max_features=0.25, n_estimators=100, n_jobs=1)\n",
    "        rfecv = RFECV(estimator=model, step=5, scoring='roc_auc', verbose=True)\n",
    "        rfecv.fit(train, y_train)\n",
    "\n",
    "        print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "        # Plot number of features VS. cross-validation scores\n",
    "#         plt.figure()\n",
    "#         plt.xlabel(\"Number of features selected\")\n",
    "#         plt.ylabel(\"Cross validation score (roc-auc)\")\n",
    "#         plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "#         plt.show()\n",
    "\n",
    "        features = [f for f,s in zip(train.columns, rfecv.support_) if s]\n",
    "        train = train[features]\n",
    "        valid = valid[features]\n",
    "        return train, valid\n",
    "    \n",
    "    def feat_selection(train, valid, y_t, t=0.1):\n",
    "        # read in the train, valid and y_train objects\n",
    "        X, Y = feat_selection.variance_threshold_selector(train, valid, threshold=t)\n",
    "        \n",
    "        X, Y = feat_selection.rfecv(train=X, valid=Y, y_train=y_t)\n",
    "        \n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape is:  (16215, 184) \n",
      "\n",
      "output data shape is:  (16215, 174) \n",
      "\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 174 features.\n",
      "Fitting estimator with 169 features.\n",
      "Fitting estimator with 164 features.\n",
      "Fitting estimator with 159 features.\n",
      "Fitting estimator with 154 features.\n",
      "Fitting estimator with 149 features.\n",
      "Fitting estimator with 144 features.\n",
      "Fitting estimator with 139 features.\n",
      "Fitting estimator with 134 features.\n",
      "Fitting estimator with 129 features.\n",
      "Fitting estimator with 124 features.\n",
      "Fitting estimator with 119 features.\n",
      "Fitting estimator with 114 features.\n",
      "Fitting estimator with 109 features.\n",
      "Fitting estimator with 104 features.\n",
      "Fitting estimator with 99 features.\n",
      "Fitting estimator with 94 features.\n",
      "Fitting estimator with 89 features.\n",
      "Fitting estimator with 84 features.\n",
      "Fitting estimator with 79 features.\n",
      "Fitting estimator with 74 features.\n",
      "Fitting estimator with 69 features.\n",
      "Fitting estimator with 64 features.\n",
      "Fitting estimator with 59 features.\n",
      "Fitting estimator with 54 features.\n",
      "Fitting estimator with 49 features.\n",
      "Fitting estimator with 44 features.\n",
      "Fitting estimator with 39 features.\n",
      "Fitting estimator with 34 features.\n",
      "Fitting estimator with 29 features.\n",
      "Fitting estimator with 24 features.\n",
      "Fitting estimator with 19 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 4 features.\n",
      "Optimal number of features : 29\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_valid = feat_selection.feat_selection(train=X_train, valid=X_valid, y_t=y_train, t=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data shape is:  (16215, 184) \n",
      "\n",
      "output data shape is:  (16215, 174) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid = feat_selection.variance_threshold_selector(train=X_train, valid=X_valid, threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function _ratio_float is deprecated; Use a float for 'ratio' is deprecated from version 0.2. The support will be removed in 0.4. Use a dict, str, or a callable instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18937, 174) (18937,) Counter({0: 11170, 1: 7767})\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Explicitly doing sampling. Use with care if going ahead with the CV based approach. Keep ratio low if so (recommended)\n",
    "\n",
    "oversampling the minority class using techniques from SMOTE (for oversampling) and ENN/Tomek (for undersampling/cleaning)\n",
    "ENN worked out better than Tomek\n",
    "added support for undersampling with ENN/RENN/AllKNN \"\"\"\n",
    "\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from imblearn.under_sampling import AllKNN, EditedNearestNeighbours, RepeatedEditedNearestNeighbours\n",
    "\n",
    "feat_names = X_train.columns.values\n",
    "\n",
    "### OVERSAMPLING (ADASYN) ###\n",
    "    # Apply ADASYN\n",
    "# ada = ADASYN(random_state=0)\n",
    "# X_train, y_train = ada.fit_sample(X_train, y_train)\n",
    "\n",
    "### OVERSAMPLING (SMOTE) AND THEN UNDERSAMPLING (ENN/Tomek) ###\n",
    "    # Apply SMOTE + Tomek links\n",
    "# sm = SMOTETomek(random_state=0, ratio=0.75)\n",
    "# X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    # Apply SMOTE + ENN\n",
    "smote_enn = SMOTEENN(random_state=0, ratio=0.5)\n",
    "X_train, y_train = smote_enn.fit_sample(X_train, y_train)\n",
    "\n",
    "### UNDERSAMPLING (ENN/RENN/AllKNN) ###\n",
    "    # Apply ENN\n",
    "# enn = EditedNearestNeighbours(random_state=0)\n",
    "# X_train, y_train = enn.fit_sample(X_train, y_train)\n",
    "    # Apply RENN\n",
    "# renn = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "# X_train, y_train = renn.fit_sample(X_train, y_train)\n",
    "    # Apply AllKNN\n",
    "# allknn = AllKNN(random_state=0)\n",
    "# X_train, y_train = allknn.fit_sample(X_train, y_train)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train,columns=feat_names)\n",
    "\n",
    "print(X_train.shape, y_train.shape, collections.Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' XGBOOST '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" XGBOOST \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgboost class for tuning parameters and returning the best model\n",
    "\n",
    "class xgboost_model():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this class initializes some functions used in the xgboost pipeline \"\"\"\n",
    "    \n",
    "    # define your custom evaluation metric here\n",
    "    # currently defined: recall, precision, f1, roc-auc, weighted of recall/precision metrics\n",
    "    def f1_score(preds, dtrain):\n",
    "        labels = dtrain.get_label()\n",
    "        #y_preds = [1 if y >= 0.5 else 0 for y in preds] # binaryzing your output\n",
    "        #rscore = sklearn.metrics.recall_score(y_pred=y_preds, y_true=labels)\n",
    "        #pscore = sklearn.metrics.precision_score(y_pred=y_preds, y_true=labels)\n",
    "        #score = sklearn.metrics.f1_score(y_pred=y_preds, y_true=labels)\n",
    "        score = sklearn.metrics.roc_auc_score(y_score=preds, y_true=labels)\n",
    "        #score = (4*rscore + pscore)/5\n",
    "        return 'score', score\n",
    "    \n",
    "    # function to be minimized and sent to the optimize function of hyperopt\n",
    "    def xgb_score(params):\n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        randomseed = 1\n",
    "        \n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['max_depth', 'scale_pos_weight']:\n",
    "            params[parameter_name] = int(params[parameter_name])\n",
    "                    \n",
    "        dtrain = xgb.DMatrix(data=X_train.values, feature_names=X_train.columns.values, label=y_train)\n",
    "        xgb_cv = xgb.cv(params = params, num_boost_round=1000, nfold=N_FOLDS, dtrain=dtrain, early_stopping_rounds=5,\n",
    "                       feval = xgboost_model.f1_score, maximize = True, stratified = True, verbose_eval=False) # may tune on the stratified flag\n",
    "        num_rounds = len(xgb_cv['test-score-mean'])\n",
    "        bst_score = xgb_cv['test-score-mean'][num_rounds-1]\n",
    "        #print('evaluation metric score of iteration is: ', bst_score, '\\n')\n",
    "        return {'loss': (1 - bst_score), 'status': STATUS_OK, 'params': params, 'num_boost': num_rounds, \n",
    "                'bst_score': bst_score, 'base_score': params['base_score']}\n",
    "    \n",
    "    # function to do hyperparameter tuning with hyperopt (bayesian based method)\n",
    "    def optimize(X_train, y_train):\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        \n",
    "        # space to be traversed for the hyperopt function\n",
    "        space = {\n",
    "            'base_score' : hp.quniform('base_score', 0.1, 0.9, 0.01),\n",
    "             'learning_rate' : hp.uniform('learning_rate', 0.001, 0.2),\n",
    "             #'max_depth' : hp.choice('max_depth', np.arange(3, 8, dtype=int)),\n",
    "            'max_depth' : hp.quniform('max_depth', 5, 20, 1),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 0, 5, 0.2),\n",
    "             'subsample' : hp.quniform('subsample', 0.7, 0.85, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0, 1, 0.1),\n",
    "            'reg_lambda' : hp.uniform ('reg_lambda', 0, 1),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.7, 0.85, 0.05),\n",
    "            'scale_pos_weight' : hp.quniform('scale_pos_weight', 1, 5, 1),\n",
    "            'objective' : 'binary:logistic'}\n",
    "        \n",
    "        best = fmin(xgboost_model.xgb_score, space, algo=tpe.suggest, trials=trials, max_evals=MAX_EVALS,\n",
    "                    rstate=np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        num_rounds = trials.best_trial['result']['num_boost']\n",
    "        \n",
    "        return trials, best, num_rounds # results of all the iterations, the best one and the number of rounds for the best run\n",
    "    \n",
    "    # train and return a model with the best params\n",
    "    def xgb_train(best_params, num_rounds):\n",
    "        dtrain = xgb.DMatrix(data=X_train.values, feature_names=X_train.columns.values, label=y_train)\n",
    "        model = xgb.train(best_params, dtrain=dtrain, maximize=True, num_boost_round=num_rounds, feval=xgboost_model.f1_score)\n",
    "        return model\n",
    "\n",
    "    # function to input a model and test matrix to output predictions and score parameters\n",
    "    def xgb_predict(X_test, y_test, model, trials, mode = \"validate\", threshold = 0.2):\n",
    "        dtest = xgb.DMatrix(data=X_test, feature_names=X_test.columns.values)\n",
    "        pred = model.predict(dtest)\n",
    "        #predict = np.where(pred > trials.best_trial['result']['base_score'], 1, 0)\n",
    "        predict = np.where(pred > threshold, 1, 0)\n",
    "        \n",
    "        if mode == \"validate\":\n",
    "            recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "            precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "            f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "            auc_score = roc_auc_score(y_test, pred)\n",
    "            tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "            print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "            print('recall score is: ', recall_score)\n",
    "            print('precision score is: ', precision_score)\n",
    "            print('f1_score is: ', f1_score)\n",
    "            print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "            print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n",
    "            return pred, predict, tn, fp, fn, tp\n",
    "        else:\n",
    "            return pred\n",
    "        \n",
    "    # function to return cv results for train dataset (recall/precision/f1/accuracy)\n",
    "    def xgb_cv(X_train, y_train, best_params):\n",
    "        model = xgb.XGBClassifier(**best, silent=True)\n",
    "        xgb_cv_scores = sklearn.model_selection.cross_val_predict(model, X_train, y_train, cv=5)\n",
    "        print('recall: ', sklearn.metrics.recall_score(y_pred=xgb_cv_scores, y_true=y_train))\n",
    "        print('precision: ', sklearn.metrics.precision_score(y_pred=xgb_cv_scores, y_true=y_train))\n",
    "        print('f1: ', sklearn.metrics.f1_score(y_pred=xgb_cv_scores, y_true=y_train))\n",
    "        print('accuracy: ', sklearn.metrics.accuracy_score(y_pred=xgb_cv_scores, y_true=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score was:  0.9981142000000001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" calling the model creation functions to return the trials (results object) and the best parameters.\n",
    "the best parameters are used to train the model and the predicted results are returned with the .xgb_predict call \"\"\"\n",
    "\n",
    "# return the trials and best parameters\n",
    "trials, best, num_rounds = xgboost_model.optimize(X_train=X_train, y_train=y_train)\n",
    "print('best score was: ', 1 - trials.average_best_error(), '\\n')\n",
    "#print(trials.best_trial['result']['bst_score'])\n",
    "\n",
    "# return the model object trained with the best parameters\n",
    "model = xgboost_model.xgb_train(best, num_rounds)\n",
    "\n",
    "# uncomment below line if you went ahead with the train/test split approach over the CV based approach\n",
    "#pred, predict, tn, fp, fn, tp = xgboost_model.xgb_predict(X_test=X_test, model=model, y_test=y_test, mode='validate', trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.9752800308999614\n",
      "precision:  0.7662350799109853\n",
      "f1:  0.8582110689401234\n",
      "accuracy:  0.8678248930664836\n"
     ]
    }
   ],
   "source": [
    "# cv results\n",
    "xgboost_model.xgb_cv(X_train, y_train, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2930 2523]\n",
      " [ 102  179]] \n",
      "\n",
      "recall score is:  0.6370106761565836\n",
      "precision score is:  0.06624722427831237\n",
      "f1_score is:  0.12001340931947703\n",
      "accuracy score:  0.5422043948378096\n",
      "The final AUC after taking the best params and num_rounds when it stopped is 0.6172. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_pred, xgb_predict, tn, fp, fn, tp = xgboost_model.xgb_predict(X_test=X_valid, model=model, y_test=y_valid, mode='validate',\n",
    "                                                                  trials=trials, threshold = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_pred, xgb_predict\n",
    "valid_dfs = pd.read_csv('valid_dfs.csv')\n",
    "TO_file = pd.read_excel('JUNE18_TURNOVER.xlsx')\n",
    "TO_file = TO_file[['Global ID', 'Termination Month']]\n",
    "TO_file.columns = ['original_id', 'term_month']\n",
    "\n",
    "pred_df = pd.DataFrame(xgb_pred)\n",
    "p = pd.concat([pred_df.reset_index(drop=True), pd.DataFrame(xgb_predict)], axis=1)\n",
    "p.columns = ['prob', 'predict']\n",
    "\n",
    "p['decile'] = pd.qcut(x=p['prob'], q=10, labels=False)\n",
    "p = pd.concat([p.reset_index(drop=True), valid_dfs], axis=1)\n",
    "\n",
    "p = pd.merge(p, TO_file, how='left')\n",
    "#p=p[p['label'] == 1]\n",
    "\n",
    "p.to_csv('p.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true negatives:  2356\n",
      "false positives:  3097\n",
      "false negatives:  72\n",
      "true positives:  209\n"
     ]
    }
   ],
   "source": [
    "print('true negatives: ', tn)\n",
    "print('false positives: ', fp)\n",
    "print('false negatives: ', fn)\n",
    "print('true positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = metrics.precision_recall_curve(y_true=y_valid, probas_pred=xgb_pred)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(model, open(\"xgb_june17.pickle.dat\", \"wb\"))\n",
    "#loaded_model = pickle.load(open(\"xgb_june17.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x16601515fd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAEWCAYAAABSXFx2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XeYpFWd/v/3PWQYyYgEAZEkOoiCAQkOYFhRgVURBSWYAxh2QXF1EXflt6xZxEBQgqhfFlBAUBGBEUVA0sAAOsRRWFiCiNAwAjNz//44p6XoqequDtVV03W/rquvrnriOV0z0585z3nuR7aJiIiI6AXTut2AiIiIiEEpTCIiIqJnpDCJiIiInpHCJCIiInpGCpOIiIjoGSlMIiIiomekMImIWEJI+o6kf+92OyI6SckxiYipTtI8YG1gYcPizWzfPY5jzgROtb3++Fq3ZJJ0EnCX7c90uy0xtWTEJCL6xRttT2/4GnNRMhEkLd3N84+HpKW63YaYulKYRERfk/RySb+T9JCk6+pIyOC6AyX9QdIjkm6X9P66fCXg58C6kgbq17qSTpL0+Yb9Z0q6q+H9PEmflHQ98Kikpet+Z0q6X9Idkj4yTFv/cfzBY0v6hKT7JN0jaU9Ju0m6WdKDkv6tYd8jJJ0h6bTan2skvbBh/fMkzao/hxsl7T7kvN+W9DNJjwLvBvYFPlH7/tO63WGSbqvHv0nSPzcc4wBJv5X0JUl/rX19XcP61SWdKOnuuv6shnVvkDS7tu13krZq+wOOJU4Kk4joW5LWA84DPg+sDhwCnClprbrJfcAbgJWBA4GvSnqx7UeB1wF3j2EE5u3A64FVgUXAT4HrgPWAXYGPSXptm8d6FrB83fdw4HjgHcA2wI7A4ZI2bth+D+D02tcfAmdJWkbSMrUdvwSeCRwM/EDS5g377gMcCTwDOAX4AfCF2vc31m1uq+ddBfgccKqkdRqO8TJgLrAm8AXgu5JU130fWBF4fm3DVwEkvRj4HvB+YA3gWOAcScu1+TOKJUwKk4joF2fV/3E/1PC/8XcAP7P9M9uLbF8AXAXsBmD7PNu3ufg15Rf3juNsx9G277Q9H3gJsJbt/7D9hO3bKcXF29o81pPAkbafBP4f5Rf+120/YvtG4EagcXThattn1O2/QilqXl6/pgNH1XZcBJxLKaIGnW370vpz+nuzxtg+3fbddZvTgFuAlzZs8ifbx9teCJwMrAOsXYuX1wEfsP1X20/WnzfAe4FjbV9he6Htk4HHa5tjClpir3FGRIzSnrZ/NWTZhsBekt7YsGwZ4GKAeqnhs8BmlP/IrQjMGWc77hxy/nUlPdSwbCngN20e6y/1lzzA/Pr93ob18ykFx2Lntr2oXmZad3Cd7UUN2/6JMhLTrN1NSdoP+Bdgo7poOqVYGvR/Ded/rA6WTKeM4Dxo+69NDrshsL+kgxuWLdvQ7phiUphERD+7E/i+7fcOXVEvFZwJ7EcZLXiyjrQMXnpodkvjo5TiZdCzmmzTuN+dwB22Nx1L48fg2YMvJE0D1gcGL0E9W9K0huJkA+Dmhn2H9vdp7yVtSBnt2RW4zPZCSbN56uc1nDuB1SWtavuhJuuOtH1kG8eJKSCXciKin50KvFHSayUtJWn5Oql0fcr/ypcD7gcW1NGT1zTsey+whqRVGpbNBnarEzmfBXxshPP/Hni4TohdobbhBZJeMmE9fLptJL2p3hH0McolkcuBKyhF1SfqnJOZwBspl4dauRdonL+yEqVYuR/KxGHgBe00yvY9lMnE35K0Wm3DTnX18cAHJL1MxUqSXi/pGW32OZYwKUwiom/ZvpMyIfTfKL9Q7wQOBabZfgT4CPA/wF8pkz/Padj3j8CPgNvrvJV1KRM4rwPmUeajnDbC+RdSCoCtgTuAB4ATKJNHO+FsYG9Kf94JvKnO53gC2J0yz+MB4FvAfrWPrXwX2HJwzo7tm4AvA5dRipYZwKWjaNs7KXNm/kiZdPwxANtXUeaZHFPbfStwwCiOG0uYBKxFRPQBSUcAm9h+R7fbEjGcjJhEREREz0hhEhERET0jl3IiIiKiZ2TEJCIiInpGckwiRmnVVVf1Jpts0u1mdM2jjz7KSiut1O1mdEX63p99h/7u/0T1/eqrr37A9lojbZfCJGKU1l57ba666qpuN6NrZs2axcyZM7vdjK5I32d2uxld08/9n6i+S/pTO9vlUk5ERET0jBQmERER0TNSmERERETPSGESERERPSOFSURERPSMFCYRERHRM1KYxKSQdICkY0bY5iRJbxnFMTeSdMMw67eVdPQIxzhC0iHtnjMiIjorOSYxZdXHpfdv4EhExBIohckUImkj4BfAb4GXA9cBJwKfA54J7AvcCHwDmEH5/I+wfXbd9/vAYLzfQbZ/J2kmcATwAPAC4GrgHW7xkCVJuwFfqdtfA2xs+w1DttkQ+B6wFnA/cKDtP9fVr5L0UWBt4F9sn9uqbW38PGYCh9h+g6TV6zk3Bh4D3mf7+rrpCyVdBDwb+ILt44c77vwnF7LRYeeNdPop619nLOCAPu1/+t6ffYep1f95R72+200YVgqTqWcTYC/gfcCVwD7ADsDuwL8BNwEX2X6XpFWB30v6FXAf8Grbf5e0KfAjYNt6zBcBzwfuBi4FtqcUP08jaXngWGAn23dI+lGLNh4DnGL7ZEnvAo4G9qzrNgJeCTwXuFjSJiO0rV2fA661vaekXYBTgK3ruq0ohdxKwLWSzrN995C+vY/yM2XNNdfi8BkLRnn6qWPtFco/0v0ofe/PvsPU6v+sWbNGtf3AwMCo9xmPFCZTzx225wBIuhG40LYlzaH80l8f2L1hXsXywAaUouMYSVsDC4HNGo75e9t31WPOrsdZrDABtgBut31Hff8j6i/zIbYD3lRffx/4QsO6/7G9CLhF0u31mHcM07Z27QC8GcD2RZLWkLRKXXe27fnAfEkXAy8Fzmrc2fZxwHEAm2++uQ/ed48xNGFqmDVrFm/t42ju9L0/9XP/JzuOP4XJ1PN4w+tFDe8XUT7vhcCbbc9t3EnSEcC9wAspk6L/3uKYC2n950ZjbLNbvB58//Fh2tauZm3zkO+t2hAREZMkd+X0n/OBgyUJQNKL6vJVgHvqaMU7gaXGcOw/AhvXOSEAe7fY7nfA2+rrfXn66MtekqZJei5lPsjcCWrbJfVcg3NPHrD9cF23h6TlJa0BzKRcAouIiC7IiEn/+U/ga8D1tTiZB7wB+BZwpqS9gIuBR0d7YNvzJX0I+IWkB4Dft9j0I8D3JB1KnfzasG4u8GvK5NcP1Hkl42nb4OjHEcCJkq6nTH7dv2Gb3wPnUS5p/efQ+SURETF5UphMIbbnUe6cGXx/QIt172+y7y2USaCDPlWXzwJmNWx30AjNuNj2FrXo+Sb1dl3bJwEnNbRllyZtOGDoshHa1tinZtYAHqzbPggsNjHE9hHDdSYiIiZXLuXERHtvnSB7I+USzLHdaISk3YEju3X+iIgYm4yYxJhI+gnwnCGLP2n7q8BXR9h3VWAf29+aoLa8FvjvIYvvqCM3G0nax/YPJ+JcERHRWSlMYkxs//M4dl8V+BBlXstEtOV8yqTeZjaiZLm0XZhIWsr2wgloWkREjJJaBHhGj+qRdNeXAF+vx3kc2BV4Evg2JfhsASW19WJJz6/tW5Zy6fDNlAm4e1Amul5g+9AW5/kE5S6cRcDPbR9W79b5JiU19jHgvbb/KOkk4OF6/mcBn7B9hqTLgedRslBOpoS5HUW5+2Y54Ju2j60/g88C9wBb296y6QcAbLDxJp721q+3Wj3l/euMBXx5Tn/+nyZ978++w9P73+vJqRNtonJMJF1te8RwzBQmS5haXNxKSWO9kXJr63XAuynprgdS0l1vsn3qYLpr3d7AosYEVdvb1l/KZ/P0dNdDbTdLd12Wclvw3ravlLQypUD4KPAC2wdK2gL4JSUI7YvA5bZ/UPddinLHzbm2W05clfQ64N+BV9l+TNLqth+UdCHlbp1bJL0M+C/bu9TCZCXKLcpbAOfY3qQxlr4e933AM21/XtJyta97ARtS7sx5QUNAXGN7GpNftzn8a8Om1k9pa68A987vdiu6I33vdiu6p7H/M9ZbZfiNp5iBgQGmT58+7uPsvPPObRUm/Vv+Ltm6me66OSVT5EqAwSwQSTtQRmmoIxh/qse/DPi0pPWBH9eCop0+vgo40fZj9ZgPSpoOvAI4veEYyzXsc1bNOrlJ0totjvsaYKuGpxivAmwKPFF/BosVJfX8SX6t+j0BM33vT/3c/yS/Rju6ne7abJitabVh+4eSrgBeD5wv6T3A7S2OPdJ5pgEP2d66yfbw9D60qn4EHFznpTy1sIysjDq7JSIiJlZuF56aOp3uum6dZ4KkZ0hamqcnq25GGaGZK2ljyvNzjgbOoeSRPAI8Y4Tz/BJ4l6QV6zFXr6Mzd9SgNVS8cITjDD3X+cAHJS0z2FZJKzXdMyIiJl0Kk6npP4FlKOmuN9T3UO6C2b9OCN2MsaW7PkGZx/ENSdcBF1AuFX0LWKpeTjoNOMD243XbG+rloS0oTxX+C3CppBskfbHFeX5BKWSuqvsOXpbaF3h3PfeNNAlNG+J6YIGk6yR9HDiBMgfnmvqzOZaMHEZE9IxMfo0Ypc0339xz584decMparKvN/eS9H1mt5vRNf3c/8m+KycjJhEREdEzMoQdLQ2T7toqzGws55hByVZp9Ljtl03UOSIiYsmRwiRaGme6a7vnmAO0ustmQtTbo78NrEy54+hI26fVdbsAX6IEwF0NvNv2gk62JyIiWkthEv3gMWC/mqGyLnC1pPMpSbEnA7vavlnSfwD7A98d7mDzn1zIRoed1/FG96p/nbGAA/q0/+n7U33vt/TTmDyZ/Bo9oyFu/wpKUu3NwH6URNqhEfhr0CRev83zXAe8BXgIuMz2JnX5jsCnbO/WZJ8kv1b9nACavj/1Pumn/WOyk19TmETPqIXJHcAOti+V9D1KbsoHWDwCf1maxOu3cY6XUkZJnk8JcJtHCaO7StLXgV1szxjuGLkrJ3cn9KN+7jv0d/8n+66cXMqJXnOn7Uvr61OBT9M8An8lWsfrNyVpHcooy/41ZA5JbwO+Wp+b80vKAwgjIqJLUphErxk6hPcwT38ezqCP0zpefzF1pOU84DO2L//HyezLgB3rNq+hjQInIiI6Jzkm0Ws2kLRdff124HKaR+C3Ha9fn2r8E0rq7OlD1j2zfl8O+CTwnQnuT0REjEIKk+g1f6DE5l8PrE55YnGrCPx24/XfCuwEHCBpdv0avEX5UEl/oETX/9T2RR3pVUREtCWXcqLXLLL9gSHLrgRePmTZLZQHAg76VKsD2j6VMl+l2bpDgUPH0M6IiOiAjJhEREREz8iISfQM2/OAF4x1/8TbR0Qs+VKYxJQxXLy9pA2BH1MmyS4DfMP2d+q6WcA6wGB81Gts39fxBkdExGJSmES/uAd4he3HJU0HbpB0ju276/p9bV/VzoESSZ9Y9qkm8fLRS1KYRE/pVCy97Sca3i7HKOdXDYmk5/AZ/ZvDtvYK5Rd0P5qqfZ81a9aI2wwMDLS13VTVz/2f7L4nkj56Sidj6SU9mxKytglwqO1v1uWzKEXOQuBM4PMe5i9GIukTzd2P+rnv0N/9n+xI+tyVE71oaCz9axkSS297AWWuyPGS5gCnA1sOd1Dbd9reilKY7C9p7bpq3/p8nB3r1zsnvEcREdGWFCbRi5rF0jcbwWiMpd+WMoIy8sHLvJIbqVH0tv+3fn8E+CHw0jG1OiIixi2FSfSiTsTSry9phfp6NWB7YK6kpSWtWZcvA7wBuKFD/YqIiBFk8mv0osFY+mMpCa/fAC6ixNKvQLmt91WUWPozJe0FXMzwsfTPA74syYCAL9meU59SfH4tSpYCfgUc36F+RUTECFKYRC/qRCz9BUO2HVz+KLDNGNsZERETLJdyIiIiomekMImeYnue7XHF0jc8QXjw64q67heSHpJ0bot9vyFpYKznjoiI8culnJhShoulB74IrAi8f+gKSdsCq7ZzjiS/Ts3003b0at+T3BpTSQLWoqd0Kvm14fgzgUNsv6Fh2eCk132AW2xPb7JfY/LrNod/rX/nx669Atw7f+TtpqJe7fuM9Vbp+DkGBgaYPn2xvxp9o5/7P1F933nnndsKWEthEj2lk8mv9fgzWbww+SgwzfZXJQ00K0waJfk1CZj9qJ/7Dv3d/8lOfs2lnOhFQ5NfP82Q5FeAeqvvMZK2psTJbzbaE0laF9gLmDkB7Y6IiHFKYRK9qFny63JNtmtMfp0G/H0M53oRJaL+VkkAK0q61fYmYzhWRESMU+7KiV404cmvrdg+z/azbG9keyPgsRQlERHdk8IketFg8uv1wOqU5Ne9Kcmv1wEXAMtTkl/3l3Q55TLOcMmvSPoN5WF/u0q6S9JrO9iHiIgYg1zKiV404cmvALZ3HOnEI018jYiIzsqISURERPSMjJhET7E9DxhX8isl26TRUsAAsDLl7p0jbZ9Wtz8I+BjwXGAt2w+M9dwRETF+KUxiSmmW/Cpps7LKt9Tbg6+WdL7th4BLgXOBWe2eI8mvvZl+Ohl6te9Jfo2pJAFr0TM6nfracJ7rgLfYvqVh2Txg21YjJkl+fUqvpp9Ohl7te5JfO6+f+z/Zya8ZMYlesznw7obU14NYPPV1PnAf8OrG1Fdg5Khj6aWUxNjbRtMo28cBxwFssPEm/vKc/v2r868zFtCv/e/Vvs/bd2bHz9HPyafQ3/2f7L733t+w6HcdS32VtA5llGX/mn0yJisssxRz+3jofNasWZPyi7AX9XPfIyZLCpPoNR1Jfa0jLecBn7F9+QS0MyIiOiC3C0evmfDUV0nLAj8BTrF9ekdbHxER45LCJHpNJ1Jf3wrsBBwgaXb92hpA0kck3QWsD1wv6YROdSwiIkaWSznRayY89dX2qZT5Ks3WHQ0cPYZ2RkREB2TEJCIiInpGRkyiZ3Qo9fVx2y8bT7siImLypDCJKaNZ6usgSQuBOfXtn23vXpefBLwS+Ftdd4Dt2R1uakREtJDCJPrFfNtNixbgUNtntH2gRNL3ZCz7ZOh23xM9H/0gkfTRMzoZSS9pwPZimcp1xOTckQqTRNI/pVdj2SdDt/s+GdHzrfRzJDv0d/8nO5I+hUn0jFqY3AHs0BBJ/0cWj6R/jBIrv6gxkn64P/CSFgCzgQXAUbbPqstPArajFDwXAofZfny4dm6++eaeO3fuuPq6JEs098xuN6Mr+rnv0N/9n6i+S2qrMMldOdFrhkbSv5YhkfS2FwDLAMdLmgOcDmw5wnE3qH8h9gG+Jum5dfmngC2Al1ByUz45ob2JiIhRSWESvaZZJH2zYb3GSPptKSMorQ9q312/3w7MolwqwvY9Lh4HTgReOp7GR0TE+KQwiV7TiUj61SQtV1+vCWwP3FTfr1O/C9gTuKEjvYqIiLbkrpzoNYOR9MdS0l2/AVxEiaRfAZgPvIoSSX+mpL2Aixk+kv55wLGSFlGK8aNs31TX/UDSWoAoc1CGps5GRMQkSmESvaYTkfS/A2a0WLfLWBoZERGdkUs5ERER0TMyYhI9o9OR9JI2AE4Ank2ZULub7XmSdgW+SCnUByjpr7eOtR0RETF2KUxiyhgukr46BTjS9gWSpgOL6vJvA3vY/oOkDwGfAQ5odZAkvyb5dTIl7TX6TQqT6CmdSn+VtCWwtO0LAGwPNKw2sHJ9vQpwd5P9G5NfOXzGgrF2cYm39grlF3Q/6kbfZ82aNanna2VgYKBn2tIN/dz/ye57kl+jp3Qq/VXSnsB7gCeA5wC/oqS8LpS0I3AW5Y6fh4GX2364VRuT/JoEzH7Uz32H/u5/kl8jOpP+ujSwI3AIJeV1Y566XPNxynyT9Skha1+Z2O5ERES7UphEL+pE+utdwLW2b69FzVnAi2uGyQttX1G3Ow14xXgaHxERY5fCJHrRhKe/UrJQVquFCMAulPTXvwKrSNqsLn81JeQtIiK6IJNfoxdNePprnUtyCHBhjZ+/Gjje9gJJ763HWUQpVN7Vwb5FRMQwUphEL5rw9FeAekfOVk2W/wT4yRjaGRERE2zUl3LqA9EW+8c9IiIiYrzaGjGRNAvYvW4/G7hf0q9t/0sH2xZ9qEPpr0tREl1XBhZSQtZOq9v/BnhG3e6ZwO9t7znW80dExPi0eylnFdsPS3oPcKLtz0q6vpMNixiLZumvdWKrbd8iaV3gaknn237I9o4N250JnD25LY6IiEbtFiZLS1oHeCvw6Q62J/pYp1Jfbd/c8PpuSfcBawEPNZz7GZQ7dQ4cqZ2JpE8k/WRKJH30m7aSX+tdD/8OXGr7g5I2Br5o+82dbmD0j06lvg45x0uBk4Hn19uMB5fvB+xu+y0t9muMpN/m8K8dP46eLtnWXgHund/tVnRHN/o+Y71VJveELQwMDDB9+vRuN6Nr+rn/E9X3nXfeua3k10TSR8+ohckltjeo73ehjNAtb3v7IduuAhxDuWyzENjM9oojHH8dYBawv+3Lh6z7OXCC7TNHamci6RPN3Y/6ue/Q3/3vyUh6SZtJulDSDfX9VpI+M95GRjTRidRX6kjLecBnmhQlawAvresjIqKL2r1d+HhKRsSTALavB97WqUZFX5vw1FdJy1JySk6xfXqTTfYCzrX99wnsR0REjEG7hcmKtn8/ZFl/Pvc8Om0w9fV6YHVK6uvelNTX64ALgOUpqa/7S7oc2IxhUl8pk7Z3Ag6QNLt+Nd658zbgRxPflYiIGK1278p5QNJzqUPqkt4C3NOxVkU/m/DUV9unUp5S3Gr9zFG2MSIiOqTdwuTDwHHAFpL+l3LnxL4da1VERET0pRELE0nTgG1tv0rSSsA02490vmnRbzqU+vq47ZeNp10RETF5RixMbC+SdBDwP7aHu44f0VXNUl8B6nySb9M8kv4k4JXA3+rmB9iePSkNjoiIxbR7KeeC+sj402iYZGj7wY60KmJiPQbs1yySvq4/1PYZ7R4sya9Jfp1MSX6NftNu8usdTRbb9sYT36ToV52KpG9ynuuAt9RC5STKrcLDFiZJfn1Kkl8n95xJfu0N/dz/JL9G3+pGJH0tTLajFDwXAofZfny4YyT5NQmY/aif+w793f/JTn5t61JOfY7IYmyfMtqGRYzgTtuX1tenUiLp77F9JYDthwHqROxj6vyRhZQsk2HVSPrvUyLpB5+T8yng/yiFznHAJ4H/mLjuRETEaLQ7x+QlDa+XpwylXwOkMImJ1iySfrkm2zVG0k8Dhk1tbRVJb3swj+dxSScCh4yx3RERMQHaKkxsH9z4vj5AbehtmRETYQNJ29m+jKci6d8v6SX1Us4zgPmUSPq76uWY/RljJL2kdWzfI0nAnsANHepXRES0od1I+qEeAzadyIZEVJMdSf8DSXOAOcCawOc70amIiGhPu3NMfspTQ+zTgC2BZg9DixivSY2kt73LWBoZERGd0e4cky81vF4A/Mn2XR1oT0RERPSxdi/l7Gb71/XrUtt3SfrvjrYs+o7tebbHFUnfcKlm8OuKum5rSZdJulHS9ZL2btjvu5Kuq8vPkNSfYQURET2g3RGTV1Nuo2z0uibLIrqmVSR9NVz668cbbkP+CnAQcFSr8yT5NcmvkynJr9Fvhi1MJH0Q+BCwcZ2MOOgZwKXN94oYu06lv9q+ueH13ZLuA9YCHmooSgSswOK3LA9NfuXwGQvG2dMl19orlF/Q/agbfZ81a9aknq+VgYGBnmlLN/Rz/ye778Mmv9bbglcD/gs4rGHVI3lOTnRCN9Jf67ITgd2Am4DX236s1f5Jfk0CZj/q575Df/d/spNfh51jYvtv9br/223/iZIfYWC6pA3G3cqI5oamv76WIemvthcAywDH19t9T6fcLTashvTXAxvSX7F9ILAu5XblvVvsHhERHdbW5FdJb5R0C+V/sr8G5gE/72C7or81S39tNrTXmP66LWUEpaVW6a//OKm9kPIE7TePoc0RETEB2r0r5/OUHImbbT+Hcn0/c0yiUzaQtF19PZj+uq6klwBIeoakpSnpr/fUkY93Mob0VxWbDL4G3ki5dBQREV3QbmHypO2/ANMkTbN9Ma3vfogYr8lMfxVwckP66zrkIX4REV3T7u3CD9Vsh99QIrzvowStRXTCpKa/AtuPuoUREdER7Y6Y7EG5C+JjlFs5b6MMeUdERERMmHafLvyopA2BTW2fLGlFhrmeHzFWtucB40p/ZfEnXz9OmRw7p77/s+3d6/YHUQru5wJr2X5grOeOiIjxa/chfu+lhEutTvkHfD3gO5RJsBE9o1X6q6QB283mRV0KnAvM6nDTIiKiDe3OMfkw8FJKGic11vuZHWtV9KVOpb4Ox/a19dxt75NI+kTSd1Ii6KPfDZv8+o+NpCtsv0zStbZfVG/VvMb2ViPuHNGmTqa+SloAzKZM2j7K9llD1s8Dtm11KWdIJP02h3/t+HH1dUm29gpw7/xut6I7JqPvM9ZbpbMnGKOBgQGmT+/f51v2c/8nqu8777xzW8mv7RYmXwAeovzv9WDK83Nusv3p8TY0YlAtTC6xvUF9vwvwaWB529sP2XYV4BjKZZuFwGa2Vxzm2OvWZ+RsDFwE7Gr7tob18ximMGmUSPpEc/ejfu479Hf/eyqSvsFhwP2UyYPvB34GfGbszYtoqSOpr7bvrt9vp8wnedF4GxoRERNv2MJk8Hk4thfZPt72XrbfUl+PPNQSMXqdSH1dTdJy9fWalNySmzrYh4iIGKORRkz+cR1e0pkdbksEdCb19XnAVXX/iylzTG4CkPQRSXcB6wPXSzqhQ/2KiIg2jHRXTuOtCht3siERVSdSX38HzGix7mjg6DG0MyIiOmCkERO3eB0REREx4UYaMXmhpIcpIycr1NfU97a9ckdbF32lU6mvtl82nnZFRMTkGbYwsZ3Y+VhitEp9hX9M5D4BeDZl9G832/MknQS8Evhb3fQA27MnobkREdFEu8mvEUu6U4AjbV9Qn5S9qGHdobbPaPdASX5N8msnJfk1+l0Kk+gZnYqkl7QlsLTtCwBsD4yhbY3Jrxw+Y8FoDzFlrL0Gubq8AAAWhUlEQVRC+QXdjyaj77Nmzero8cdqYGCgZ9s2Gfq5/5Pd97aSXyMmQ6ci6SXtCbwHeAJ4DvAr4DDbC+ulnO0oBc+Fdfnjw7Uzya9JwOxH/dx36O/+92rya8RkudP2pfX1qcBrKUFqVwLYftj2AmAZ4HhJc4DTgS2HOebSwI7AIcBLKLe+H1DXfQrYoi5fHfjkhPYmIiJGJYVJ9JpORNLfBVxr+/Za1JwFvBjA9j0uHgdOpDxFOyIiuiSFSfSaCY+kpwS0rSZprfp+F2okvaR16ncBewI3THB/IiJiFFKYRK+Z8Eh62wspl3EurJd+BBxfV/+gLpsDrAl8viO9ioiItuSunOg1Ex5JD1DvyNmqyfJdxtLIiIjojIyYRERERM/oy8JE0kmS3jIBx2mam9HGfkdIOmS855/I80zUz6TheBtJ2qfh/baShn1Ynu15tscVSS9p9pCvK1psO+osk4iI6LxcyhkH26/odhtaqRNEu2kjYB/ghwC2rwKu6uQJh4ukn0hJfk3yaycl+TX6Xcd+eTWkeP6WMj/gOsrtmJ8DngnsC9xImdw4o7blCNtn130XS/WUNBM4AniA8rC3q4F3uEVKnKR5wGnAznXRPrZvra93kvQvwLOAT9g+Q9L3gTNsn133/0Hd/7ba9mUpo0xvtn2LpAHb0+u2n6DcHbII+LntwyS9l5IWuixwK/BO24+N8HN7Zt1/G0kvBGYDG9r+s6Tb6s9qLeB79fv9wIF1/UnAg5TU1GuARxqO+17gTcCbbM8foQ27Al+ifCZXAh+0/Xi9M6bdBNajgOdJmg2cDFwLHGL7DZJWr+3fmBKW9j7b10s6AtigLt8A+JrtlqMskvajTGo1cL3td0rasMXP5jmUImlpyp/LxuMcCrwVWA74ie3PNjlXkl+rJL8m+bUf9XP/J73vtjvyRfkf8wLKL9JplCLie5Q7IvagZEn8f5TCAmBVSgT5SsCKwPJ1+abAVfX1TMrD1tavx7yMkhLaqg3zgE/X1/sB59bXJ1FCuaZRgrlurctfCZxVX69CSSFdmlI87VuXLwusUF8P1O+vA34HrFjfr16/r9HQls8DB9fXR1B+Sbdq943AysBBlMJgX2BD4LK6/qfA/vX1uxrafBJwLrBU43nqcc4BlhvmnCcBb6Hc8XInsFldfgrwsdrv24GX1OUr15/NcJ/VuQ3H/8f7+vP8bH29CzC7ob2/oxQIawJ/AZZp0d7nA3OBNYf8zFv9bM4B9quvP9zw2b0GOI7y53Ja/fntNNyf7c0228z97OKLL+52E7omfe9f/dz/ier74O+Hkb46PcfkDttzXLImbgQurI2bQylcXgMcVv9XPYvyS3EDhk/1/L3tu+oxZ9fjDOdHDd+3a1h+lu1Ftm8C1gaw/Wtgkzpq8XbgTJdArsuAf5P0ScroxdARh1cBJ7qOhth+sC5/gaTf1H7sS/ll2o7fAdsDO1GKt50oyaW/qeu3o14ioYxW7NCw7+kut8cOeielcHqzR4harzanfG431/cn1/NvzvgTWAftUNuN7YuANSStUtedZ/tx2w8A91E/myZ2oYxuPVCPM/gzb/Wz2Z6n/ix8v+E4r6lf11JGmbagFFgREdEFnZ6H0PiLcFHD+0X13AspvzCf9uCROqQ/mOo5Dfh7i2MuZOQ+uMXrxuOo4fX3KUXE2yj/48b2D+skytcD50t6T/2F2rh/s8tJJwF72r5O0gGUUYN2/IZSiGwInE2JSTflf/PNNJ57aJ7HDZR5F+tTRoBGomGWj5TAOvSzGs05Bo/d7ufbqj2tjjv0deNx/sv2sW0cKyIiOqzbd+WcDxxcUzeR9KK6fDSpniPZu+H7ZW1sfxLl0gW2b6zt2hi43WW+wzksnofxS+Bdklas269elz8DuEfSMpRip12XAO8Abqk/gweB3YDBZ8j8jlI4UY/722GOdS3wfuAcSeu2ce4/AhtJ2qS+fyfw67p8NAmsj1D636p/+9bjzAQesP1wG21rdCHwVklr1OMM/sxb/WwuHbJ80PmUz25wrtB6dcQsIiK6oNuFyX9SLgVcL+mG+h5GkerZhuXqaMdHKf+7H5bteynpoyc2LN4buKFectqCMu+icZ9fUAqWq+o2g7fo/jtwBSWt9I/tNtj2vPrykvr9t8BDtv9a338EOLCmo76z9m244/22tuk8SWuOsO3fgQOB0+vlmUXAd2w/wegSWK8HFki6TtLQn/sRwLa1/UcB+w/XphbtvBE4Evh1bc9X6qpWP5uPAh+WdCWlmBo8zi8pl34uq/09g9YFVUREdJjc/IaWKaHelbPt4DyENvdZkTIH5sW2/9aptsWSa/PNN/fcuXNH3nCKyuPfZ3a7GV3Rz32H/u7/RPVd0tW2tx1pu26PmPQUSa+ijGx8I0VJRETE5Ot2CNeEkPQT4DlDFn/S9kajOY7tX1HuCpoUkr5JuVuk0ddtn9hs+yX1nONR55Bc2GTVrrb/MorjLKSMhAH82fbudbkot3LvRZls+20Pk50SERGdNSUKE9v/3O02jIXtD/fDOcejFh8TkeY633az4xwAPBvYwvaiTHyNiOiuKVGYxNTQkBZ8BSW99mZKMN7zaT9xdrQ+SEkEXgRg+76RdkgkfSLpJ1pi6COeMqUnv8aSpRYmd1DSfC+V9D3KnJ8PAHvbvlLSypQY+2WBRbb/LmlT4EfDTaqStIASyLcAOMr2WXX5Xyh39PwzJcL+I7ZvabJ/YyT9Nod/7fgJ6vWSZ+0V4N5hH2owdXWq7zPWW2XkjbpsYGCA6dOnd7sZXdPP/Z+ovu+8885tTX5NYRI9oxYml9jeoL7fBfg0JfJ++yHbrgIcQ7nMs5ASob/iMMde1/bdNZPmIsocldvqU4Y/a/vLkt4EfNz2jsO1M3fl5O6EftTPfYf+7n/uyol+N7RSfrjJMnh64uy2lBGU1ge1767fb6c8/mAwzO8u4Mz6+icsHp4XERGTKIVJ9JoNJA0+0+jtwOWMLnF2MZJWk7Rcfb0m5a6km+rqsyjP3YHyEMebFz9CRERMlkx+jV7zB0qS7LHALZQnEV9ESZxdAZhPeWjit4AzJe0FXMzw6cDPA46VtIhSjB9VH94IJXn2BzWddgB4Twf6FBERbUphEr1mke0PDFl2JfDyIctu4emXXT7V6oD1bp0ZLdY9RHk4Y0RE9IBcyomIiIiekRGT6Bn14YUvGOv+kmZQsk0aPW77ZeNpV0RETJ4UJjFl2J7DMCmxkjYATqAkvRrYreFJzkj6BnCg7f4MK4iI6AEpTKKfnAIcafsCSdOBRYMrJG0LrNrOQZL8muTXiZC014jmErAWPaVTsfSStgSOs71Dk3VLAb8C9gFuaTZikuTXpyT5dWKOtSSkvTbq5+RT6O/+J/k1+lqnYukl7Um5FfgJypOofwUcZnuhpI8C02x/VdLASJdykvyaBMx+1M99h/7u/2Qnv+ZSTvSiO21fWl+fSomlv8f2lQC2HwaQtBJwjKR/xNIPc8ylgR0pozB/Bk4DDpD0c2AvYGYH+hEREaOUwiR6UbNY+uWabNcYSz8N+Pswx7wLuLZG0iPpLEo2yv8BmwC3SgJYUdKttjcZVw8iImJMkmMSvWjCY+kpIW2rSVqrvt8FuMn2ebafZXsj2xsBj6UoiYjonhQm0YsGY+mvB1anxNLvTYmlvw64AFieEku/v6TLKZdxWsbS214IHAJcKGkOIKB/Z7BGRPSoXMqJXjThsfQAti9ghKcHJ8MkIqK7MmISERERPSOFSfQU2/NsjyuWXtLsIV9XNKxfWdL/SjqmYdk2kuZIulXS0aqzYCMiYvLlUk5MKSPF0gP/Cfx6yLJvU8LTLgd+BvwT8PNWB0jya5JfxypprxEjS8Ba9JROJb/WY28DHFqPv63tgyStA1xse4u6zduBmbbfP2TfJL9WSX4d+/5LWtpro35OPoX+7v9kJ79mxCR60ebAuxuSXw9i8eTX+cB9wKsbk1+BVsmv04AvU24r3rVh1XqUjJNBd9VlT2P7OOA4KMmvB++7xzi7uOSaNWsWb+3jBMx+7nu/Jp9Cf/d/svuewiR6USeSXz8E/Mz2nUOmkDSbT5JhxIiILklhEr2oE8mv2wE7SvoQMB1YVtIA5fLQ+g3brQ/cPcZ2R0TEOOWunOhFE578antf2xvUdNdDgFNsH2b7HuARSS+vd+PsB5zdsZ5FRMSwUphEL5rw5NcRfBA4AbgVuI1h7siJiIjOyqWc6EUdSX4dZPsk4KSG91cBY85OiYiIiZMRk4iIiOgZKUyip3Qp+fVISXfWybAREdFFuZQTU8oYk19/ChxDuTQUERFdlMIkesokJL+uXY//jyA225fX9W21MZH0iaQfjcTQR4xOIumjp9TC5A5gh4bk1z+yePLrY8CylImy/0h+bRV3XJNfL+Kp5NdtbR80ZJsB201zlxNJ/5RE0o9unyU5hr5RP0eyQ3/3P5H0EZOb/NqWRNI/pd9j2fu57/0ayQ793f9E0kdMYvKr7cMmoL0RETFBcldO9KJJS37tZCciImL0UphEL5rU5FdJX5B0F7CipLskHTEBfYiIiDHIpZzoRZOd/PoJ4BOjbmVEREy4jJhEREREz8iISfQU2/MYx3NrJM2gZJs0etz2y8bTroiImBwpTGJKaZX8KmkhMKe+/bPt3evyXYAvUTJRrgbebXvBJDU3IiKGSGES/WK+7acVLDV07WRgV9s3S/oPYH/gu8MeKMmvSX4dhSS/RoxOkl+jZ3Q4jn6xVFdJawGX2d6kvt8R+JTt3Zrsn+TXKsmvo9snya9TQz/3f7KTX1OYRM/oVBx9PfYCYDawADjK9lkqEbDzgDfbvkrS14FdbM8Yrp2bb765586dO97uLrGSgDmz283oin7uO/R3/yeq75ISSR9LpE7E0QNsYPtuSRsDF0maY/s2SW8DvippOeCXlMIlIiK6JIVJ9JpOxNFj++76/XZJsyiXim6zfRmwI4Ck1zBygRMRER2UHJPoNRMeRy9ptToigqQ1ge2Bm+r7Z9bvywGfBL7TkV5FRERbUphEr+lEHP3zgKvq/hdT5pjcVNcdKukPwPXAT21f1IlORUREe3IpJ3rNhMfR17t1mk5otX0ocOgY2hkRER2QEZOIiIjoGRkxiZ7RyTh6SRsCP6bMRVkG+Ibt70haETgdeC7l7p6f2j5srG2IiIjxSWESU0arOPrqHuAVth+XNB24QdI5wEPAl2xfLGlZ4EJJr7P981bnSfJrkl9HkrTXiLFLwFr0lE6mvzacYw3gWuDlg7cRN6z7OnCD7eOHLE/ya5Xk15G3myppr436OfkU+rv/SX6Nvtbh9NdnA+cBmwCH2v7mkPWrAtcAr7J9e6vjJPk1CZj9qJ/7Dv3d/8lOfs3k1+hFQ9NfX8uQ9Nf6BOBlgOMlzaHME9lyuIPavtP2VpTCZH9Jaw+uq9koPwKOHq4oiYiIzkphEr2oWfprs6G9xvTXbSkjKCMfvFy+uZGa+FodB9xi+2ujbm1EREyYFCbRizqR/rq+pBXq69Uo6a9z6/vP12N9rEP9iYiINqUwiV7UqfTXK+r+v6bciTNH0vqUBwVuCVwjabak93SqYxERMbzcLhy9qBPprxcM2XZw+V2AxtjOiIiYYBkxiYiIiJ6REZPoKZ1Mfx1PuyIiYnKkMIkpZYT014iI6HG5lBMRERE9I8mvEaMk6RHqrcZ9ak3ggW43okvS9/7Vz/2fqL5vaHutkTbKpZyI0ZvbTqzyVCXpqn7tf/ren32H/u7/ZPc9l3IiIiKiZ6QwiYiIiJ6RwiRi9I7rdgO6rJ/7n773r37u/6T2PZNfIyIiomdkxCQiIiJ6RgqTiIiI6BkpTCJGQdI/SZor6VZJh3W7PZ0k6dmSLpb0B0k3SvpoXb66pAsk3VK/r9bttnaKpKUkXSvp3Pr+OZKuqH0/TdKy3W5jp0haVdIZkv5Y/wxs1y+fvaSP1z/zN0j6kaTlp/JnL+l7ku6TdEPDsqaftYqj67+B10t68US3J4VJRJskLQV8E3gdsCXwdklbdrdVHbUA+Ffbz6M82fnDtb+HARfa3hS4sL6fqj4K/KHh/X8DX619/yvw7q60anJ8HfiF7S2AF1J+DlP+s5e0HvARYFvbLwCWAt7G1P7sTwL+aciyVp/164BN69f7gG9PdGNSmES076XArbZvt/0E8P+APbrcpo6xfY/ta+rrRyi/mNaj9PnkutnJwJ7daWFnSVofeD1wQn0vYBfgjLrJVO77ysBOwHcBbD9h+yH65LOnhI+uIGlpYEXgHqbwZ2/7EuDBIYtbfdZ7AKe4uBxYVdI6E9meFCYR7VsPuLPh/V112ZQnaSPgRcAVwNq274FSvADP7F7LOuprwCeARfX9GsBDthfU91P5898YuB84sV7KOkHSSvTBZ2/7f4EvAX+mFCR/A66mfz77Qa0+647/O5jCJKJ9arJsyt9vL2k6cCbwMdsPd7s9k0HSG4D7bF/duLjJplP1818aeDHwbdsvAh5lCl62aabOpdgDeA6wLrAS5fLFUFP1sx9Jx/8epDCJaN9dwLMb3q8P3N2ltkwKSctQipIf2P5xXXzv4NBt/X5ft9rXQdsDu0uaR7lktwtlBGXVOrwPU/vzvwu4y/YV9f0ZlEKlHz77VwF32L7f9pPAj4FX0D+f/aBWn3XH/x1MYRLRviuBTevs/GUpE+LO6XKbOqbOqfgu8AfbX2lYdQ6wf329P3D2ZLet02x/yvb6tjeifM4X2d4XuBh4S91sSvYdwPb/AXdK2rwu2hW4iT747CmXcF4uacX6d2Cw733x2Tdo9VmfA+xX7855OfC3wUs+EyXJrxGjIGk3yv+clwK+Z/vILjepYyTtAPwGmMNT8yz+jTLP5H+ADSj/iO9le+jEuSlD0kzgENtvkLQxZQRldeBa4B22H+9m+zpF0taUib/LArcDB1L+MzvlP3tJnwP2ptyZdi3wHso8iin52Uv6ETATWBO4F/gscBZNPutarB1DuYvnMeBA21dNaHtSmERERESvyKWciIiI6BkpTCIiIqJnpDCJiIiInpHCJCIiInpGCpOIiIjoGUuPvElEREwGSQspt2cP2tP2vC41J6IrcrtwRESPkDRge/oknm/phue/RPSEXMqJiFhCSFpH0iWSZku6QdKOdfk/SbpG0nWSLqzLVpd0lqTrJV0uaau6/AhJx0n6JXCKpKUkfVHSlXXb93exixG5lBMR0UNWkDS7vr7D9j8PWb8PcL7tIyUtBawoaS3geGAn23dIWr1u+zngWtt7StoFOAXYuq7bBtjB9nxJ76PEir9E0nLApZJ+afuOTnY0opUUJhERvWO+7a2HWX8l8L36cMWzbM+ukfmXDBYSDRHxOwBvrssukrSGpFXqunNsz6+vXwNsJWnwOTCrAJsCKUyiK1KYREQsIWxfImkn4PXA9yV9EXiI5o+dH+7x9I8O2e5g2+dPaGMjxihzTCIilhCSNgTus3085cnPLwYuA14p6Tl1m8FLOZcA+9ZlM4EHbD/c5LDnAx+sozBI2kzSSh3tSMQwMmISEbHkmAkcKulJYADYz/b9dZ7IjyVNA+4DXg0cAZwo6XrKU2D3b35ITgA2Aq6pT469H9izk52IGE5uF46IiIiekUs5ERER0TNSmERERETPSGESERERPSOFSURERPSMFCYRERHRM1KYRERERM9IYRIRERE94/8HzZ2dVLekgKMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## important features from the best model above\n",
    "\n",
    "xgb.plot_importance(booster=model, max_num_features=20, show_values=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LIGHTGBM '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" LIGHTGBM \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm class for tuning\n",
    "\n",
    "class lightgbm_model():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this class initializes some functions used in the lightgbm pipeline \"\"\"\n",
    "    \n",
    "    def score(preds, train_set):\n",
    "        labels = train_set.get_label()\n",
    "        y_preds = [1 if y >= 0.5 else 0 for y in preds] # binaryzing your output\n",
    "\n",
    "        rscore = sklearn.metrics.recall_score(y_pred=y_preds, y_true=labels)\n",
    "        pscore = sklearn.metrics.precision_score(y_pred=y_preds, y_true=labels)\n",
    "        #score = sklearn.metrics.f1_score(y_pred=y_preds, y_true=labels)\n",
    "        #score = sklearn.metrics.roc_auc_score(y_score=y_preds, y_true=labels)\n",
    "        score = (4*rscore + pscore)/5\n",
    "        \n",
    "        return 'score', score, True\n",
    "    \n",
    "    def lgbm_score(params):        \n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        \n",
    "        # Retrieve the subsample if present otherwise set to 1.0\n",
    "        subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "        # Extract the boosting type\n",
    "        params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "        params['subsample'] = subsample\n",
    "\n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "            params[parameter_name] = int(params[parameter_name])\n",
    "        \n",
    "        start = timer()\n",
    "        # Perform n_folds cross validation\n",
    "        cv_results = lgb.cv(params, train_set, num_boost_round = 1000, nfold = N_FOLDS, \n",
    "                            early_stopping_rounds = 10, feval = lightgbm_model.score, seed = randomseed)\n",
    "        run_time = timer() - start\n",
    "        \n",
    "        # Extract the best score\n",
    "        best_score = np.max(cv_results['score-mean'])\n",
    "        \n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "\n",
    "        # Boosting rounds that returned the highest cv score\n",
    "        n_estimators = int(np.argmax(cv_results['score-mean']) + 1)\n",
    "\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "                'estimators': n_estimators, \n",
    "                'train_time': run_time, 'status': STATUS_OK}\n",
    "    \n",
    "    def optimize():\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        \n",
    "        space = {\n",
    "            'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.75, 0.9)}, \n",
    "                                                         {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.75, 0.9)},\n",
    "                                                         {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "            'num_leaves': hp.quniform('num_leaves', 100, 1000, 50),\n",
    "            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "            'subsample_for_bin': hp.quniform('subsample_for_bin', 30000, 300000, 20000),\n",
    "            'min_child_samples': hp.quniform('min_child_samples', 1, 3, 1),\n",
    "            'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "            'subsample': hp.uniform('subsample', 0.7, 0.9),\n",
    "            'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 0.8),\n",
    "            'scale_pos_weight': hp.quniform('scale_pos_weight', 1, 3, 1),\n",
    "            'objective': 'binary'\n",
    "        }\n",
    "        \n",
    "        # Run optimization\n",
    "        best = fmin(fn = lightgbm_model.lgbm_score, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        nestimators = trials.best_trial['result']['estimators']\n",
    "        return best, trials, nestimators\n",
    "    \n",
    "    def lgbm_train(best_params, nestimators):\n",
    "        train_set = lgb.Dataset(X_train, label = y_train)\n",
    "        #model = lgb.LGBMClassifier(silent = False, random_state = randomseed, objective = 'binary', n_estimators=nestimators)\n",
    "        model = lgb.train(best_params, train_set=train_set, num_boost_round=nestimators, feval=lightgbm_model.score)\n",
    "        #model.set_params(**best_params)\n",
    "        #model.fit(X_train, y_train, eval_metric = \"auc\")\n",
    "        return model\n",
    "    \n",
    "    def lgbm_predict(X_test, y_test, model, mode = \"validate\"):\n",
    "        #test_set = lgb.Dataset(X_test.values, feature_name=X_test.columns.values, label=y_test)\n",
    "        pred = model.predict(X_test)\n",
    "        predict = np.where(pred > 0.044, 1, 0)\n",
    "        \n",
    "        if mode == \"validate\":\n",
    "            recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "            precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "            f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "            auc_score = roc_auc_score(y_test, pred)\n",
    "            tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "            print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "            print('recall score is: ', recall_score)\n",
    "            print('precision score is: ', precision_score)\n",
    "            print('f1_score is: ', f1_score)\n",
    "            print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "            print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n",
    "            return pred, predict, tn, fp, fn, tp\n",
    "        else:\n",
    "            return pred\n",
    "        \n",
    "    def lgbm_cv(X_train, y_train, best):\n",
    "        model = lgb.LGBMClassifier(**best, silent=True)\n",
    "        lgb_cv_scores = sklearn.model_selection.cross_val_predict(model, X_train, y_train, cv=5)\n",
    "        print('recall: ', sklearn.metrics.recall_score(y_pred=lgb_cv_scores, y_true=y_train))\n",
    "        print('precision: ', sklearn.metrics.precision_score(y_pred=lgb_cv_scores, y_true=y_train))\n",
    "        print('f1: ', sklearn.metrics.f1_score(y_pred=lgb_cv_scores, y_true=y_train))\n",
    "        print('accuracy: ', sklearn.metrics.accuracy_score(y_pred=lgb_cv_scores, y_true=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lgb dataset\n",
    "train_set = lgb.Dataset(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-0249760ef351>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calling the lightgbm function and best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlightgbm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage_best_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlightgbm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlgbm_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnestimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-810f962d5410>\u001b[0m in \u001b[0;36moptimize\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# Run optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         best = fmin(fn = lightgbm_model.lgbm_score, space = space, algo = tpe.suggest, \n\u001b[1;32m---> 79\u001b[1;33m             max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(randomseed))\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mnestimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'estimators'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m         )\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     verbose=verbose)\n\u001b[0;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 840\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-139-810f962d5410>\u001b[0m in \u001b[0;36mlgbm_score\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Perform n_folds cross validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         cv_results = lgb.cv(params, train_set, num_boost_round = 1000, nfold = N_FOLDS, \n\u001b[1;32m---> 37\u001b[1;33m                             early_stopping_rounds = 10, feval = lightgbm_model.score, seed = randomseed)\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mrun_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[0;32m    449\u001b[0m                                     \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mhandlerFunction\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandlerFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1526\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1528\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# calling the lightgbm function and best model\n",
    "best, trials, nestimators = lightgbm_model.optimize()\n",
    "print(1 - trials.average_best_error(), '\\n')\n",
    "\n",
    "model = lightgbm_model.lgbm_train(best, nestimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "lightgbm_model.lgbm_cv(X_train, y_train, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lightgbm model on the validation set\n",
    "lgb_pred, lgb_predict, tn, fp, fn, tp = lightgbm_model.lgbm_predict(X_test=X_valid, model=model, y_test=y_valid, mode='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, thresholds = metrics.precision_recall_curve(y_true=y_valid, probas_pred=lgb_pred)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RANDOM FOREST \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest class for tuning\n",
    "\n",
    "class rf_model():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this class initializes some functions used in the random forest pipeline \"\"\"\n",
    "        \n",
    "    def rf_score(params):        \n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "\n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['max_depth', 'n_estimators']:\n",
    "            params[parameter_name] = int(params[parameter_name])\n",
    "                \n",
    "        rf_results = RandomForestClassifier(**params, random_state=randomseed)\n",
    "        #rf_results.fit(X_train, y_train)\n",
    "        rf_cv_scores = sklearn.model_selection.cross_val_predict(rf_results, X_train, y_train, cv=5, verbose=False)        \n",
    "        recall_score = sklearn.metrics.recall_score(y_pred=rf_cv_scores, y_true=y_train)\n",
    "        precision_score = sklearn.metrics.precision_score(y_pred=rf_cv_scores, y_true=y_train)\n",
    "        f1_score = sklearn.metrics.f1_score(y_pred=rf_cv_scores, y_true=y_train)\n",
    "\n",
    "        return {'loss': (1 - recall_score), 'status': STATUS_OK, 'params': params, 'iteration': ITERATION}\n",
    "    \n",
    "    def optimize():\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        \n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        space = {\n",
    "            'max_depth' : hp.quniform('max_depth', 5, 10, 1),\n",
    "            'max_features': hp.choice('max_features', range(20, int((X_train.shape[:][1])/5))),\n",
    "            'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "            'n_estimators': hp.choice('n_estimators', np.arange(200, 1000))\n",
    "        }\n",
    "        \n",
    "        # Run optimization\n",
    "        best = fmin(fn = rf_model.rf_score, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = trials, rstate = np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        return best, trials\n",
    "    \n",
    "    def rf_train(best_params):\n",
    "        model = RandomForestClassifier(random_state = randomseed)\n",
    "        model.set_params(**best_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    \n",
    "    def rf_predict(X_test, y_test, model, mode = \"validate\"):\n",
    "        pred = model.predict_proba(X_test)[:, 1]\n",
    "        predict = np.where(pred > 0.12, 1, 0)\n",
    "        \n",
    "        if mode == \"validate\":\n",
    "            recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "            precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "            f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "            auc_score = roc_auc_score(y_test, pred)\n",
    "            tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "            print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "            print('recall score is: ', recall_score)\n",
    "            print('precision score is: ', precision_score)\n",
    "            print('f1_score is: ', f1_score)\n",
    "            print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "            print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n",
    "            return pred, predict, tn, fp, fn, tp\n",
    "        else:\n",
    "            return pred\n",
    "        \n",
    "    def rf_cv(X_train, y_train, best):\n",
    "        model = RandomForestClassifier(**best, verbose=False)\n",
    "        rf_cv_scores = sklearn.model_selection.cross_val_predict(model, X_train, y_train, cv=5)\n",
    "        print('recall: ', sklearn.metrics.recall_score(y_pred=rf_cv_scores, y_true=y_train))\n",
    "        print('precision: ', sklearn.metrics.precision_score(y_pred=rf_cv_scores, y_true=y_train))\n",
    "        print('f1: ', sklearn.metrics.f1_score(y_pred=rf_cv_scores, y_true=y_train))\n",
    "        print('accuracy: ', sklearn.metrics.accuracy_score(y_pred=rf_cv_scores, y_true=y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the randomforest function and returning the best model\n",
    "best, trials = rf_model.optimize()\n",
    "print(1 - trials.average_best_error(), '\\n')\n",
    "\n",
    "model = rf_model.rf_train(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv results\n",
    "rf_model.rf_cv(X_train, y_train, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting using the best random forest model on the validation set\n",
    "rf_pred, rf_predict, tn, fp, fn, tp = rf_model.rf_predict(X_test=X_valid, model=model, y_test=y_valid, mode='validate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('true negatives: ', tn)\n",
    "print('false positives: ', fp)\n",
    "print('false negatives: ', fn)\n",
    "print('true positives: ', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SIMPLE LOGIT MODEL \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-5280a97a50a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Fit grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# View best hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[1;32m-> 1216\u001b[1;33m                          order=\"C\")\n\u001b[0m\u001b[0;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    574\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# implementing only the baseline logistic model\n",
    "# need to add support for parameter tuning\n",
    "\n",
    "#kfold = model_selection.KFold(n_splits=5, random_state=1)\n",
    "modelCV = LogisticRegression()\n",
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 5, 10)\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "# Create grid search using 5-fold cross validation\n",
    "clf = GridSearchCV(modelCV, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "model_fit = clf.fit(X_train, y_train)\n",
    "\n",
    "# View best hyperparameters\n",
    "print('Best Penalty:', model_fit.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', model_fit.best_estimator_.get_params()['C'])\n",
    "\n",
    "#scoring = 'recall' # give precision or f1\n",
    "#results = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "#print(\"5-fold cross validation average accuracy: %.3f\" % (results.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc:  0.5\n",
      "recall:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.0\n"
     ]
    }
   ],
   "source": [
    "# getting the predictions for the actual test set\n",
    "log_pred = model_fit.best_estimator_.predict_proba(X=X_valid)[:, 1]\n",
    "log_predict = np.where(log_pred > 0.5, 1, 0)\n",
    "\n",
    "# print the various evaluation metrics\n",
    "print('auc: ', sklearn.metrics.roc_auc_score(y_score=log_pred, y_true=y_valid))\n",
    "print('recall: ', sklearn.metrics.recall_score(y_pred=log_predict, y_true=y_valid))\n",
    "print('precision: ', sklearn.metrics.precision_score(y_pred=log_predict, y_true=y_valid))\n",
    "print('f1: ', sklearn.metrics.f1_score(y_pred=log_predict, y_true=y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" SVC \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train)\n",
    "X_train = scaling.transform(X_train)\n",
    "X_valid = scaling.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set: \n",
      "\n",
      "{'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Grid scores on development set:\n",
      "0.896 (+/-0.009) for {'C': 1, 'kernel': 'linear'}\n",
      "0.903 (+/-0.008) for {'C': 10, 'kernel': 'linear'}\n",
      "0.392 (+/-0.400) for {'degree': 5, 'kernel': 'poly'}\n",
      "0.292 (+/-0.000) for {'degree': 10, 'kernel': 'poly'}\n",
      "0.934 (+/-0.027) for {'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Detailed classification report:\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      5453\n",
      "          1       0.00      0.00      0.00       281\n",
      "\n",
      "avg / total       0.90      0.95      0.93      5734\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "Best parameters set found on development set: \n",
      "\n",
      "{'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Grid scores on development set:\n",
      "0.903 (+/-0.010) for {'C': 1, 'kernel': 'linear'}\n",
      "0.911 (+/-0.010) for {'C': 10, 'kernel': 'linear'}\n",
      "0.500 (+/-0.001) for {'degree': 5, 'kernel': 'poly'}\n",
      "0.500 (+/-0.000) for {'degree': 10, 'kernel': 'poly'}\n",
      "0.933 (+/-0.048) for {'gamma': 'auto', 'kernel': 'rbf'}\n",
      "Detailed classification report:\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      1.00      0.97      5453\n",
      "          1       0.00      0.00      0.00       281\n",
      "\n",
      "avg / total       0.90      0.95      0.93      5734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10]},\n",
    "                   {'kernel': ['poly'], 'degree': [5, 10]},\n",
    "                   {'kernel': ['rbf'], 'gamma': ['auto']}]\n",
    "\"\"\"  several more parameters need to be included \n",
    "        1. other kernel types (rbf, poly)\n",
    "        2. class balancing parameter (class weight) \n",
    "        3. cv (stratified, non-stratified, KFolds) \n",
    "\n",
    "tuned_parameters = [{'kernel': ['linear'], 'C': [1, 10, 100]},\n",
    "                    {'kernel': ['poly'], 'degree': [5, 10, 20]},\n",
    "                    {'kernel': ['rbf'], 'gamma': }]\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "\n",
    "    clf = GridSearchCV(SVC(probability=True), tuned_parameters, cv=StratifiedKFold(y=y_train, n_folds=5),\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\", '\\n')\n",
    "    print(clf.best_params_)\n",
    "    print(\"Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_valid, clf.predict(X_valid)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.0\n",
      "accuracy:  0.9509940704569236\n"
     ]
    }
   ],
   "source": [
    "svc_pred = clf.best_estimator_.predict_proba(X=X_valid)[:, 1]\n",
    "svc_predict = clf.best_estimator_.predict(X=X_valid)\n",
    "print('recall: ', sklearn.metrics.recall_score(y_pred=svc_predict, y_true=y_valid))\n",
    "print('precision: ', sklearn.metrics.precision_score(y_pred=svc_predict, y_true=y_valid))\n",
    "print('f1: ', sklearn.metrics.f1_score(y_pred=svc_predict, y_true=y_valid))\n",
    "print('accuracy: ', sklearn.metrics.accuracy_score(y_pred=svc_predict, y_true=y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_probs = pd.DataFrame({'xgb_pred': xgb_pred,\n",
    "                                  'lgb_pred': lgb_pred,\n",
    "                                  'rf_pred': rf_pred,\n",
    "                                  'log_pred': log_pred,\n",
    "                                    'svc_pred': svc_pred,\n",
    "                                    'actual': y_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>lgb_pred</th>\n",
       "      <th>rf_pred</th>\n",
       "      <th>log_pred</th>\n",
       "      <th>svc_pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.108690</td>\n",
       "      <td>0.039658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043399</td>\n",
       "      <td>0.135363</td>\n",
       "      <td>0.080101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.281030</td>\n",
       "      <td>0.354230</td>\n",
       "      <td>0.283404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.538680</td>\n",
       "      <td>0.293763</td>\n",
       "      <td>0.458449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009010</td>\n",
       "      <td>0.118185</td>\n",
       "      <td>0.080996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.109935</td>\n",
       "      <td>0.045332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.005802</td>\n",
       "      <td>0.124478</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.017966</td>\n",
       "      <td>0.093955</td>\n",
       "      <td>0.048342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010260</td>\n",
       "      <td>0.097249</td>\n",
       "      <td>0.013883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.132889</td>\n",
       "      <td>0.091374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   xgb_pred  lgb_pred   rf_pred  log_pred      svc_pred  actual\n",
       "0  0.006148  0.108690  0.039658       0.0  1.000000e-07       0\n",
       "1  0.043399  0.135363  0.080101       0.0  1.000000e-07       0\n",
       "2  0.281030  0.354230  0.283404       0.0  1.000000e-07       0\n",
       "3  0.538680  0.293763  0.458449       0.0  1.000000e-07       0\n",
       "4  0.009010  0.118185  0.080996       0.0  1.000000e-07       0\n",
       "5  0.005451  0.109935  0.045332       0.0  1.000000e-07       0\n",
       "6  0.005802  0.124478  0.030440       0.0  1.000000e-07       0\n",
       "7  0.017966  0.093955  0.048342       0.0  1.000000e-07       0\n",
       "8  0.010260  0.097249  0.013883       0.0  1.000000e-07       0\n",
       "9  0.010389  0.132889  0.091374       0.0  1.000000e-07       0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df_probs[predictions_df_probs['actual'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_pred</th>\n",
       "      <th>lgb_pred</th>\n",
       "      <th>rf_pred</th>\n",
       "      <th>log_pred</th>\n",
       "      <th>svc_pred</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.606796</td>\n",
       "      <td>0.463274</td>\n",
       "      <td>0.360117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.377779</td>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.667574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.137735</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.006822</td>\n",
       "      <td>0.175557</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.098636</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.309976</td>\n",
       "      <td>0.288429</td>\n",
       "      <td>0.311299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.005457</td>\n",
       "      <td>0.127535</td>\n",
       "      <td>0.039516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.167520</td>\n",
       "      <td>0.192762</td>\n",
       "      <td>0.174392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.093401</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.158935</td>\n",
       "      <td>0.101237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     xgb_pred  lgb_pred   rf_pred  log_pred      svc_pred  actual\n",
       "10   0.606796  0.463274  0.360117       0.0  1.000000e-07       1\n",
       "11   0.377779  0.394253  0.667574       0.0  1.000000e-07       1\n",
       "21   0.009337  0.137735  0.043991       0.0  1.000000e-07       1\n",
       "43   0.006822  0.175557  0.044283       0.0  1.000000e-07       1\n",
       "107  0.004953  0.098636  0.009567       0.0  1.000000e-07       1\n",
       "159  0.309976  0.288429  0.311299       0.0  1.000000e-07       1\n",
       "161  0.005457  0.127535  0.039516       0.0  1.000000e-07       1\n",
       "178  0.167520  0.192762  0.174392       0.0  1.000000e-07       1\n",
       "180  0.004517  0.093401  0.012048       0.0  1.000000e-07       1\n",
       "182  0.013904  0.158935  0.101237       0.0  1.000000e-07       1"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df_probs[predictions_df_probs['actual'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({'xgb_predict': xgb_predict,\n",
    "                              'lgb_predict': lgb_predict,\n",
    "                              'rf_predict': rf_predict,\n",
    "                              'log_predict': log_predict,\n",
    "                              'svc_predict': svc_predict,\n",
    "                              'actual': y_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb_predict</th>\n",
       "      <th>lgb_predict</th>\n",
       "      <th>rf_predict</th>\n",
       "      <th>log_predict</th>\n",
       "      <th>svc_predict</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     xgb_predict  lgb_predict  rf_predict  log_predict  svc_predict  actual\n",
       "10             1            1           1            0            0       1\n",
       "11             1            1           1            0            0       1\n",
       "21             0            0           0            0            0       1\n",
       "43             0            1           0            0            0       1\n",
       "107            0            0           0            0            0       1\n",
       "159            1            1           1            0            0       1\n",
       "161            0            0           0            0            0       1\n",
       "178            1            1           1            0            0       1\n",
       "180            0            0           0            0            0       1\n",
       "182            0            0           0            0            0       1"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[predictions_df['actual'] == 1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "### ONE-CLASS METHODS ###\n",
    "\n",
    "class oneclass_models():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this class contains several modelling algorithms for one-class classification/anomaly detection \"\"\"\n",
    "\n",
    "    def data_prepare(X_train, X_valid):\n",
    "        # split and create 2 dataframes corresponing to positive/negative classes\n",
    "        Negatives=X_train[X_train['response']==0]\n",
    "        Positives=X_train[X_train['response']==1]\n",
    "        Negatives.drop(['response'], axis=1, inplace=True)\n",
    "        Positives.drop(['response'], axis=1, inplace=True)\n",
    "        print(Negatives.shape)\n",
    "        print(Positives.shape)\n",
    "        \n",
    "        # remove response from validation df too\n",
    "        X_v = X_valid.drop(['response'], axis=1, inplace=False)\n",
    "        print(X_v.shape)\n",
    "        \n",
    "        # take a random fraction of the negatives to reduce computation time\n",
    "        Negatives = Negatives.sample(frac=0.1, replace=False, random_state=1)\n",
    "        \n",
    "        return Positives, Negatives, X_v\n",
    "        \n",
    "    def uni_svm(X_train, X_valid):\n",
    "        \"\"\" one-class svm by training separately on positives and negatives \"\"\"\n",
    "        \n",
    "        Positives, Negatives, X_v = oneclass_models.data_prepare(X_train, X_valid)\n",
    "        \n",
    "        # Set the parameters by cross-validation\n",
    "        params = [{'kernel': ['rbf'],\n",
    "                   'gamma': [0.01, 0.1, 0.5],\n",
    "                   'nu': [0.01, 0.1, 0.5]}]\n",
    "\n",
    "        clf_P = GridSearchCV(OneClassSVM(), cv=3, param_grid=params, scoring='accuracy', verbose=True)\n",
    "        clf_N = GridSearchCV(OneClassSVM(), cv=3, param_grid=params, scoring='accuracy', verbose=True)\n",
    "        clf_P.fit(X=Positives, y=np.full(len(Positives),1))\n",
    "        print('positive model fit \\n')\n",
    "        clf_N.fit(X=Negatives, y=np.full(len(Negatives),1))\n",
    "        print('negative model fit \\n')\n",
    "        clf_AD_P = OneClassSVM(gamma=clf_P.best_params_['gamma'],\n",
    "                                      kernel=clf_P.best_params_['kernel'], nu=clf_P.best_params_['nu'], verbose=True)\n",
    "        clf_AD_P.fit(Positives)\n",
    "        clf_AD_N = OneClassSVM(gamma=clf_N.best_params_['gamma'],\n",
    "                                      kernel=clf_N.best_params_['kernel'], nu=clf_N.best_params_['nu'], verbose=True)\n",
    "        clf_AD_N.fit(Negatives)\n",
    "\n",
    "        valid_pred_P=clf_AD_P.predict(X_v)\n",
    "        valid_pred_N=clf_AD_N.predict(X_v)\n",
    "        \n",
    "        return valid_pred_P, valid_pred_N, clf_AD_P, clf_AD_N\n",
    "    \n",
    "    def score_table(valid_pred_P, valid_pred_N):\n",
    "        table = pd.DataFrame({'P': valid_pred_P,\n",
    "                              'N': -1*valid_pred_N,\n",
    "                              'O': y_valid})\n",
    "        table['P_N'] = np.where((table['P'] == 1) & (table['N'] == -1), 1, 0)\n",
    "\n",
    "        print(sklearn.metrics.accuracy_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        print(sklearn.metrics.precision_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        print(sklearn.metrics.recall_score(y_pred=table['P_N'], y_true=table['O']))\n",
    "        \n",
    "        return table        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 5,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11427, 318)\n",
      "(572, 318)\n",
      "(5734, 318)\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    4.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive model fit \n",
      "\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   18.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative model fit \n",
      "\n",
      "[LibSVM][LibSVM]"
     ]
    }
   ],
   "source": [
    "p, n, clf_p, clf_n = oneclass_models.uni_svm(X_train=X_train, X_valid=X_valid)\n",
    "\n",
    "table=oneclass_models.score_table(valid_pred_N=n, valid_pred_P=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 9,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsolationForest(bootstrap=False, contamination=0.1, max_features=0.3,\n",
       "        max_samples='auto', n_estimators=200, n_jobs=1, random_state=None,\n",
       "        verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFA=IsolationForest(n_estimators=200, max_features=0.3)\n",
    "IFA.fit(Negatives)\n",
    "\n",
    "train_IFA=IFA.predict(Negatives)\n",
    "test_IFA=IFA.predict(Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return interpretation across methods (specify which or All)\n",
    "\n",
    "class model_interpret():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" this module takes as input the model and train/test datasets to generate interpretations of the\n",
    "        predictions generated by the model\n",
    "        LIME and SHAP methods have been added as a provision currently, treeinterpreter will be added later\n",
    "        \"\"\"\n",
    "    \n",
    "    def lime_interpreter(feat_names, classnames, categindices, categnames, \n",
    "                         kw, num_feature, train, test, n):\n",
    "        explainer = lime.lime_tabular.LimeTabularExplainer(training_data = train.values,\n",
    "                                                   feature_names = list(feat_names),\n",
    "                                                   class_names = classnames,\n",
    "                                                   categorical_features=categindices, \n",
    "                                                   categorical_names=categnames, kernel_width = kw)\n",
    "        xtest = test.values\n",
    "        exp = explainer.explain_instance(xtest[n], model.predict_proba, num_features = num_feature)\n",
    "        return exp.show_in_notebook()\n",
    "    \n",
    "    def shap_interpreter(model, train, test, n, method = 'tree'):\n",
    "        \"\"\" specify n as the prediction/observation you want the interpretation to be returned for \"\"\"\n",
    "        \n",
    "        if method == 'tree':\n",
    "            # create our SHAP explainer\n",
    "            shap_explainer = shap.TreeExplainer(model)\n",
    "            # calculate the shapley values for our test set\n",
    "            shap_values = shap_explainer.shap_values(test.values)\n",
    "        elif method == 'kernel':\n",
    "            # create our SHAP explainer\n",
    "            shap_explainer = shap.KernelExplainer(model.predict_proba, shap.kmeans(train[:100], 5))\n",
    "            shap_values = shap_explainer.shap_values(test.values)\n",
    "            \n",
    "        # load JS in order to use some of the plotting functions from the shap package in the notebook\n",
    "        shap.initjs()\n",
    "        \n",
    "        # plot the explanation for a single prediction\n",
    "        return shap.force_plot(shap_values[n, :], test.iloc[n, :])\n",
    "    \n",
    "    def model_interpreter(interpreter_algo, train, test, shap_method = 'tree', kw = 3, n = 0, model = None,\n",
    "                          feat_names = None, classnames = None,\n",
    "                          categindices = None, categnames = None, num_feature = None):\n",
    "        if interpreter_algo == 'lime':\n",
    "            return model_interpret.lime_interpreter(feat_names, classnames, categindices, categnames,\n",
    "                                                    kw, num_feature, train, test, n)\n",
    "        elif interpreter_algo == 'shap':\n",
    "            return model_interpret.shap_interpreter(model, train, test, n, method = shap_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model_interpret.model_interpreter(interpreter_algo='lime', model = model, train = X_train, test = X_test, feat_names = feat_names,\n",
    "                                  classnames = ['not delayed', 'delayed'], categindices = categ_idx, categnames = categ_names,\n",
    "                                 num_feature = 5, n=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
