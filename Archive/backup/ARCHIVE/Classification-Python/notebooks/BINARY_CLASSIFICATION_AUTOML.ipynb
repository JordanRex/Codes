{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BINARY CLASSIFICATION FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### _The AUTOML based generic framework for the AbInbev TURNOVER projects_ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'BINARY_CLASSIFICATION_AUTOML.ipynb', 'MAIN.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZATION #\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "# add notebook location to syspath (just a safe check)\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "# import all the necessary modules\n",
    "from IMPORT_MODULES import *\n",
    "from HELPER_FUNCS import *\n",
    "from MISS_IMPUTATION import *\n",
    "from ENCODING import *\n",
    "from FEATURE_ENGINEERING import *\n",
    "from FEATURE_SELECTION import *\n",
    "from SAMPLING import *\n",
    "\n",
    "# list all physical objects in current folder\n",
    "print(os.listdir())\n",
    "\n",
    "# matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 5 # number of iterations/parameter sets created towards tuning\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200 # larger number of columns is recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules/packages used in this notebook are listed below\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "['builtins', 'builtins', 'sys', 'os', 'matplotlib.pyplot', 'seaborn', 'collections', 'csv', 'pandas', 'numpy', 'time', 'gc', 'bisect', 're', 'sklearn', 'sklearn.metrics', 'sklearn.preprocessing', 'sklearn.decomposition', 'sklearn.random_projection', 'category_encoders', 'hyperopt.hp', 'hyperopt.tpe', 'xgboost', 'lightgbm', 'sklearn.model_selection', 'shap', 'lime', 'pickle', 'types']"
      ],
      "text/plain": [
       "['builtins',\n",
       " 'builtins',\n",
       " 'sys',\n",
       " 'os',\n",
       " 'matplotlib.pyplot',\n",
       " 'seaborn',\n",
       " 'collections',\n",
       " 'csv',\n",
       " 'pandas',\n",
       " 'numpy',\n",
       " 'time',\n",
       " 'gc',\n",
       " 'bisect',\n",
       " 're',\n",
       " 'sklearn',\n",
       " 'sklearn.metrics',\n",
       " 'sklearn.preprocessing',\n",
       " 'sklearn.decomposition',\n",
       " 'sklearn.random_projection',\n",
       " 'category_encoders',\n",
       " 'hyperopt.hp',\n",
       " 'hyperopt.tpe',\n",
       " 'xgboost',\n",
       " 'lightgbm',\n",
       " 'sklearn.model_selection',\n",
       " 'shap',\n",
       " 'lime',\n",
       " 'pickle',\n",
       " 'types']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield val.__name__\n",
    "print('All modules/packages used in this notebook are listed below')\n",
    "list(imports())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS FLOW\n",
    "\n",
    "#### HELPER FUNCS\n",
    "*helper functions required in the process defined here*\n",
    "- Append datasets (train/valid)\n",
    "- DateTime feature engineering (year, quarter, season, month, week, day -> their corresponding string/numeric representations)\n",
    "- Frequency count of a vector\n",
    "- Deviation encoding of categorical features (not part of feature engineering since implemented before encoding)\n",
    "\n",
    "#### MISS IMPUTATION\n",
    "*missing value treatment functions defined here*\n",
    "- implements multiple methods to perform missing value treatement\n",
    "- simple methods (mean/median/mode) imputation for categorical and numerical features.\n",
    "    - slightly tweak for adding grouped level if needed\n",
    "    - currently built to perform imputation consistent to below process (after encoding)\n",
    "- fancy methods (NNM/KNN/MICE) imputation for numerical features only\n",
    "    - requires encoding to be done prior\n",
    "    \n",
    "#### ENCODING\n",
    "*categorical column encoding functions defined here*\n",
    "- various types of encoding here\n",
    "    - label encoder\n",
    "    - binary encoder\n",
    "    - base encoder\n",
    "    - hashing encoder\n",
    "    - ordinal encoder (similar to label, different implementation)\n",
    "    - one-hot encoder\n",
    "    \n",
    "#### FEATURE ENGINEERING\n",
    "*different types of feature engineering functions defined here*\n",
    "- Decomposition features\n",
    "    - PCA\n",
    "    - ICA\n",
    "    - TSVD\n",
    "    - GRP\n",
    "    - SRP\n",
    "    - ...\n",
    "- Clustering output feaatures\n",
    "    - KMeans\n",
    "    - ...\n",
    "- Deterministic features\n",
    "    - Binning\n",
    "    - ...\n",
    "    \n",
    "#### FEATURE SELECTION\n",
    "*functions to perform feature selection defined here*\n",
    "- near zero variance columns are removed (threshold=0.1)\n",
    "- rf based rfecv with depth=7, column_sampling=0.25, estimators=100 (optional=True/False)\n",
    "\n",
    "#### SAMPLING\n",
    "*sampling functions (for over/under/both sampling) defined here*\n",
    "- Oversampling (ADASYN, SMOTE)\n",
    "- Undersampling (ENN, RENN, AllKNN)\n",
    "- Oversampling and then Undersampling (SMOTE and ENN/TOMEK)\n",
    "\n",
    "*it's okay if you have no idea what the above mean. the only thing that is important is to understand why over/undersampling\n",
    "is done and why or what ratio between*\n",
    "    - why over/under sampling is done in a classification context\n",
    "    - what ratio between the 2 classes is important to You in your context\n",
    "    - how much information loss (or gain) are you willing to tolerate? (create More data than what you have at hand?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### H2O AUTOML ####\n",
    "- automated ML framework by H2O\n",
    "- https://www.h2o.ai/products/h2o/\n",
    "- tuning performed for the sampling parameters as well as whether to balance classes post sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## h2o AUTO_ML grid search framework\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "class h2o_automl():\n",
    "    \n",
    "    def __init__():\n",
    "        \"\"\" module to invoke h2o auto ml and tune on the sampling parameters. \n",
    "        will need to incorporate to super pipeline later \"\"\"\n",
    "        \n",
    "    def automl(os, us, bc, iter, response='response'):\n",
    "        # initializing the h2o cluster\n",
    "        h2o.init()\n",
    "        # Import a sample binary outcome train/test set into H2O\n",
    "        h2o_train = h2o.import_file(str(str(iter) + '_t_h2o.csv'), header=1)\n",
    "        h2o_valid = h2o.import_file(str(str(iter) + '_v_h2o.csv'), header=1)\n",
    "        # Identify the response and set of predictors\n",
    "        x = list(h2o_train.columns)  #if x is defined as all columns except the response, then x is not required\n",
    "        x.remove(response)\n",
    "        # For binary classification, response should be a factor\n",
    "        h2o_train[response] = h2o_train[response].asfactor()\n",
    "        h2o_valid[response] = h2o_valid[response].asfactor()\n",
    "\n",
    "        randomseed = 1\n",
    "        aml = H2OAutoML(max_runtime_secs = 300, stopping_metric='mean_per_class_error', sort_metric='mean_per_class_error',\n",
    "                        class_sampling_factors=[os, us], balance_classes = bc)\n",
    "        aml.train(y = 'response', training_frame = h2o_train)\n",
    "        \n",
    "        # Print Leaderboard (ranked by xval metrics)\n",
    "        print(aml.leaderboard)\n",
    "        # Evaluate performance on a test set\n",
    "        perf = aml.leader.model_performance(h2o_valid)\n",
    "        print('The validation performance (auc) is ', perf.auc())\n",
    "        return aml, h2o_valid\n",
    "        \n",
    "    def get_score(aml, h2o_valid, y_valid, threshold = 0.1):\n",
    "        pred2 = aml.predict(h2o_valid)[:,2]\n",
    "        pred = pred2.as_data_frame().as_matrix()\n",
    "        predict = np.where(pred > threshold, 1, 0)\n",
    "        y_test=y_valid\n",
    "\n",
    "        recall_score = sklearn.metrics.recall_score(y_pred=predict, y_true=y_test)\n",
    "        precision_score = sklearn.metrics.precision_score(y_pred=predict, y_true=y_test)\n",
    "        f1_score = sklearn.metrics.f1_score(y_pred=predict, y_true=y_test)\n",
    "        auc_score = roc_auc_score(y_test, pred)\n",
    "        tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test).ravel()\n",
    "        print(sklearn.metrics.confusion_matrix(y_pred=predict, y_true=y_test), '\\n')\n",
    "        print('recall score is: ', recall_score)\n",
    "        print('precision score is: ', precision_score)\n",
    "        print('f1_score is: ', f1_score)\n",
    "        print('accuracy score: ', sklearn.metrics.accuracy_score(y_true=y_test, y_pred=predict))\n",
    "        print('The final AUC after taking the best params and num_rounds when it stopped is {:.4f}.'.format(auc_score), '\\n')\n",
    "        \n",
    "        return pred2, predict, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 9,
        "hidden": false,
        "row": 166,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "#### MOTHER OF ALL PIPELINES ####\n",
    "- hyperopt based\n",
    "- the process flow is as below\n",
    "    - prepare data\n",
    "    - pre-processing\n",
    "        - missing value imputation\n",
    "        - categorical encoding\n",
    "    - feature engineering\n",
    "    - feature selection\n",
    "    - automl (modelling pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOTHER OF ALL PIPELINES ##\n",
    "# hyperopt based ##\n",
    "\n",
    "class main(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\" random \"\"\"\n",
    "        return None    \n",
    "    \n",
    "    def prepare(self, cols_to_remove, response='label'):\n",
    "        \"\"\" checks first if backup pickles exist in folder already. \n",
    "        if so skips the major computation segment that is already back up \"\"\"\n",
    "        \n",
    "        pickle_files = [x for x in os.listdir() if re.search(pattern='.pickle', string=x)]\n",
    "        trials = {'backup_1': None}\n",
    "        \n",
    "        if len(pickle_files) > 0 :\n",
    "            # opening the first pickle only for now (will later add a loop for all the pickle files)\n",
    "            \n",
    "            print('backups available...hence using them stead of reinventing the wheel \\n')\n",
    "            f = open(pickle_files[0], \"rb\")\n",
    "            train = pickle.load(f)\n",
    "            valid = pickle.load(f)\n",
    "            y_train = pickle.load(f)\n",
    "            y_valid = pickle.load(f)\n",
    "            backup_md = pickle.load(f)\n",
    "            f.close()\n",
    "            print(backup_md)\n",
    "            \n",
    "            trials['backup_1'] = main.backup_optimize(train=train, valid=valid, y_train=y_train, \n",
    "                                                      y_valid=y_valid, backup_md=backup_md)\n",
    "        \n",
    "        else :\n",
    "            # read in the train and validation datasets\n",
    "            # clean column names and remove unwanted columns\n",
    "            # append the (multiple?) train datasets into a single one (simple appending for now)\n",
    "\n",
    "            print('1. Appending the multiple train/valid datasets in the working directory \\n')\n",
    "            train = helper_funcs.append_datasets(string='TRAIN', cols_to_remove=cols_to_remove)\n",
    "            valid = helper_funcs.append_datasets(string='VALID', cols_to_remove=cols_to_remove)\n",
    "            main.removed_cols = cols_to_remove ## attribute\n",
    "\n",
    "            # creating the datetime features from date columns (works only for cols with date in header, modify for other cases)\n",
    "            print('2. Datetime features are being created for the columns (which have \"date\" in their column name) \\n')\n",
    "            train, valid = helper_funcs.datetime_feats(train, valid)\n",
    "\n",
    "            # missing value threshold control (for both rows and columns)\n",
    "            mt = 0.5\n",
    "            print(train.shape, '\\n')\n",
    "            train.dropna(thresh=mt*(train.shape[0]), axis=1, inplace = True)\n",
    "            train.dropna(thresh=mt*(train.shape[1]), axis=0, inplace = True)\n",
    "            print(train.shape, '\\n')\n",
    "            valid = valid[train.columns]\n",
    "            valid.dropna(thresh=mt*(valid.shape[0]), axis=1, inplace = True)\n",
    "            train = train[valid.columns]\n",
    "            main.missing_threshold = mt ## attribute\n",
    "\n",
    "            # reset the index since inplace operations happened earlier\n",
    "            train.index = pd.RangeIndex(len(train.index))\n",
    "            valid.index = pd.RangeIndex(len(valid.index))\n",
    "            # save the global ids for mapping later (forward looking)\n",
    "            valid_ids = valid[['original_id', response]]\n",
    "            main.validation_labels = valid_ids ## attribute\n",
    "            valid_ids.to_csv('test_dfs.csv', index=False)\n",
    "            valid.drop('original_id', axis=1, inplace=True)\n",
    "            train.drop('original_id', axis=1, inplace=True)\n",
    "            X_train = pd.DataFrame(train)\n",
    "            X_valid = pd.DataFrame(valid)\n",
    "            # the class balance in the training dataset for the response\n",
    "            print(helper_funcs.freq_count(X_train[response]), '\\n')\n",
    "            # creating the response vector\n",
    "            y_train = X_train[response].values\n",
    "            y_valid = X_valid[response].values\n",
    "\n",
    "            # categorical columns (names, indices and dtypes)\n",
    "            x = list(X_train.dtypes)\n",
    "            x_1 = [1 if x == 'O' else 0 for x in x]\n",
    "            categorical_idx = [i for i, x in enumerate(x_1) if x == 1]\n",
    "            # Get feature names and their values for categorical data (needed for LIME)\n",
    "            cat_columns = X_train.select_dtypes(include=['object']).columns.values\n",
    "            X_train, X_valid = helper_funcs.categ_feat_eng(X_train, X_valid, cat_columns)\n",
    "\n",
    "            # drop the response\n",
    "            X_train = X_train.drop([response], axis = 1)\n",
    "            X_valid = X_valid.drop([response], axis = 1)\n",
    "\n",
    "            # call the main optimize function that does the whole tuning (inside the nested score function)\n",
    "            trials = main.optimize(train=X_train, valid=X_valid, y_train=y_train, y_valid=y_valid)\n",
    "        return trials\n",
    "    \n",
    "    # function to be minimized and sent to the optimize function of hyperopt\n",
    "    def score(params):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        global ITERATION\n",
    "        ITERATION += 1\n",
    "        \n",
    "        train=main.train\n",
    "        valid=main.valid\n",
    "        y_train=main.y_train\n",
    "        y_valid=main.y_valid\n",
    "        \n",
    "        print('\\n', params, '\\n')\n",
    "        #######################################################################################################\n",
    "        ## ENCODING ##\n",
    "        #######################################################################################################\n",
    "        cat_columns = train.select_dtypes(include=['object']).columns.values\n",
    "        train_cat = train[cat_columns]\n",
    "        num_cols = list(set(train.columns)-set(train_cat.columns))\n",
    "        train_num = train[num_cols]\n",
    "        valid_cat = valid[cat_columns]\n",
    "        valid_num = valid[num_cols]\n",
    "        \n",
    "        train_cat, valid_cat, categorical_names = categ_encoders.encoding(train_cat, valid_cat, y_train, y_valid,\n",
    "                                                                            which=params['encoder'])\n",
    "        train = pd.concat([train_cat.reset_index(drop=True), train_num], axis=1)\n",
    "        valid = pd.concat([valid_cat.reset_index(drop=True), valid_num], axis=1)\n",
    "        print('encoding completed ...', '\\n')\n",
    "        main.categorical_dict = categorical_names ## attribute\n",
    "        #######################################################################################################\n",
    "\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## CORRELATION ANALYSIS ##\n",
    "        #######################################################################################################\n",
    "        # remove highly correlated features to reduce further computation time\n",
    "        print('correlation analysis is happening ...', '\\n')\n",
    "        # Create correlation matrix\n",
    "        corr_matrix = train.corr().abs()\n",
    "        # Select upper triangle of correlation matrix\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "        # Find index of feature columns with correlation greater than 0.75\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]        \n",
    "        # Drop features\n",
    "        #print(to_drop, '\\n')\n",
    "        train.drop(to_drop, axis=1, inplace=True)\n",
    "        valid.drop(to_drop, axis=1, inplace=True)\n",
    "        print('correlation analysis completed ...', '\\n')\n",
    "        main.cor_dropped_vars = to_drop ## attribute\n",
    "        #######################################################################################################\n",
    "    \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## MISSING VALUE IMPUTATION ##\n",
    "        #######################################################################################################\n",
    "        # store all feature names\n",
    "        feat_names = train.columns.values\n",
    "        feat_names2 = valid.columns.values\n",
    "        \n",
    "        if params['miss_treatment'] == 'simple':\n",
    "            miss_enc = DataFrameImputer()\n",
    "            miss_enc.fit(X=train)\n",
    "            train_new = miss_enc.transform(train)\n",
    "            valid_new = miss_enc.transform(valid)\n",
    "        elif params['miss_treatment'] in ['KNN', 'MICE']:\n",
    "            train_new = DataFrameImputer.fancy_impute(train, which_method=params['miss_treatment'])\n",
    "            valid_new = DataFrameImputer.fancy_impute(valid, which_method=params['miss_treatment'])\n",
    "\n",
    "        # returning as pandas dataframes to retain feature names for LIME and feature importance plots\n",
    "        train = pd.DataFrame(data=train_new, columns=feat_names)\n",
    "        valid = pd.DataFrame(data=valid_new, columns=feat_names2)\n",
    "        print('missing value treatment completed ...', '\\n')\n",
    "        #######################################################################################################\n",
    "        \n",
    "\n",
    "        #######################################################################################################\n",
    "        ## STATUS REPORT ##\n",
    "        #######################################################################################################\n",
    "        print('STATUS REPORT \\n')\n",
    "        print(train.shape)\n",
    "        print(valid.shape)\n",
    "        print(y_train.shape)\n",
    "        print(y_valid.shape)\n",
    "        print(collections.Counter(y_train))\n",
    "        print(collections.Counter(y_valid))\n",
    "        #######################################################################################################\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## FEATURE ENGINEERING ##\n",
    "        #######################################################################################################\n",
    "        \"\"\" the feature engineering module \n",
    "            - 1. PCA/ICA/TSVD/GRP/SRP\n",
    "            - 2. KMEANS \"\"\"\n",
    "        \n",
    "        ## 1.\n",
    "        feat_eng_instance = feat_eng()\n",
    "        feat_eng_instance.decomp_various(train, valid, n=int(params['decomp_feats']), which_method=params['scaler'])\n",
    "        train, valid = feat_eng_instance.return_combined(train, valid)\n",
    "        \n",
    "        ## 2.\n",
    "        train, valid = feat_eng.kmeans_feats(train_df=train, valid_df=valid, m=int(params['kmeans_n']))\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## FEATURE SELECTION ##\n",
    "        #######################################################################################################\n",
    "        train, valid = feat_selection.variance_threshold_selector(train=train, valid=valid, threshold=0.1)\n",
    "        \n",
    "        if params['feat_selection'] == 'true':\n",
    "            train, valid = feat_selection.rfecv(train=train, valid=valid, y_train=y_train)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAMPLING ##\n",
    "        #######################################################################################################\n",
    "        \"\"\" oversampling or undersampling or oversampling with undersampling \"\"\"\n",
    "        \n",
    "        if params['sampler']['choice'] == 'yes':\n",
    "            train, y_train = sampler(X_train=train, y_train=y_train, \n",
    "                                     which=params['sampler']['which_method'], \n",
    "                                     frac=params['sampler']['frac'])\n",
    "        else :\n",
    "            print('no sampling done in this pipeline', '\\n')\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## BACKUP ##\n",
    "        #######################################################################################################\n",
    "        backup = str(str(ITERATION) + str(dt.now().strftime('_%H_%M_%d_%m_%Y.pickle')))\n",
    "        f = open(backup, \"wb\")\n",
    "        pickle.dump(train, f)\n",
    "        pickle.dump(valid, f)\n",
    "        pickle.dump(y_train, f)\n",
    "        pickle.dump(y_valid, f)\n",
    "        \n",
    "        backup_md = {'params': params, 'pickle_name': backup, 'randomseed': randomseed}\n",
    "        pickle.dump(backup_md, f)\n",
    "        \n",
    "        f.close()\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAVE AS FLATFILES ##\n",
    "        #######################################################################################################\n",
    "        train['response'] = y_train\n",
    "        valid['response'] = y_valid\n",
    "\n",
    "        train.to_csv(str(str(ITERATION) + '_t_h2o.csv'), index=False)\n",
    "        valid.to_csv(str(str(ITERATION) + '_v_h2o.csv'), index=False)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## H2O AUTOML ##\n",
    "        #######################################################################################################\n",
    "        h2o_os = params['h2o_automl_params']['oversampling']\n",
    "        h2o_us = params['h2o_automl_params']['undersampling']\n",
    "        h2o_bc = params['h2o_automl_params']['balance_classes']\n",
    "        \n",
    "        aml, h2o_valid = h2o_automl.automl(os=h2o_os, bc=h2o_bc, us=h2o_us, iter=ITERATION)\n",
    "        \n",
    "        pred, predict, score = h2o_automl.get_score(aml=aml, h2o_valid=h2o_valid, y_valid=y_valid,\n",
    "                                                    threshold=aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        \n",
    "        setattr(main, str('aml_' + str(ITERATION)), aml)\n",
    "        setattr(main, str('pred_' + str(ITERATION)), pred)\n",
    "        setattr(main, str('predict_' + str(ITERATION)), predict)\n",
    "        setattr(main, str('score_' + str(ITERATION)), score)\n",
    "        setattr(main, str('threshold_' + str(ITERATION)), aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        setattr(main, str('h2o_valid_' + str(ITERATION)), h2o_valid)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        loss = 1 - score\n",
    "        end_time = time.time()\n",
    "        time_taken = timedelta(seconds = round(end_time - start_time))\n",
    "        print(\"Execution took: %s secs (Wall clock time)\" % time_taken)\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK, 'params': params, 'auc': score, 'eval_time': time_taken}\n",
    "    \n",
    "    # function to do hyperparameter tuning with hyperopt (bayesian based method)\n",
    "    def optimize(train, valid, y_train, y_valid):\n",
    "                \n",
    "        main.train = train\n",
    "        main.valid = valid\n",
    "        main.y_train = y_train\n",
    "        main.y_valid= y_valid        \n",
    "        \n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = 0\n",
    "        global trials\n",
    "        trials = Trials()\n",
    "        \n",
    "        # space to be traversed for the hyperopt function\n",
    "        space = {\n",
    "            'encoder': hp.choice('encoder', ['he', 'le']),\n",
    "            'eval_time': time.time(),\n",
    "            'miss_treatment': hp.choice('missing', ['simple']),\n",
    "            'decomp_feats': hp.quniform('n', 2, 5, 1),\n",
    "            'scaler': hp.choice('scaler', ['ss', 'mm']),\n",
    "            'kmeans_n': hp.quniform('m', 2, 3, 1),\n",
    "            'feat_selection': hp.choice('rfecv', ['false', 'false']),\n",
    "            'sampler': hp.choice('sampler', [\n",
    "                {\n",
    "                    'choice': 'no',\n",
    "                    'which_method': hp.choice('sampling', ['smote_enn']),\n",
    "                    'frac': hp.quniform('frac', 0.75, 1, 0.05)\n",
    "                },\n",
    "                {\n",
    "                    'choice': 'no'\n",
    "                }\n",
    "            ]),\n",
    "            'h2o_automl_params': hp.choice('sampling_params', [\n",
    "                {\n",
    "                    'undersampling': hp.uniform('us', 0.1, 1),\n",
    "                    'oversampling': hp.uniform('os', 1, 5),\n",
    "                    'balance_classes': hp.choice('bc', ['False', 'False'])\n",
    "                }\n",
    "            ])\n",
    "        }\n",
    "        \n",
    "        best = fmin(main.score, space, algo=tpe.suggest, trials=trials, max_evals=MAX_EVALS,\n",
    "                    rstate=np.random.RandomState(randomseed))\n",
    "        best = trials.best_trial['result']['params']\n",
    "        \n",
    "        main.best = space_eval(space, trials.argmin)\n",
    "        main.trials = trials\n",
    "        \n",
    "        return trials # results of all the iterations, the best params\n",
    "    \n",
    "    def backup_optimize(train, valid, y_train, y_valid, backup_md):\n",
    "        main.train = train\n",
    "        main.valid = valid\n",
    "        main.y_train = y_train\n",
    "        main.y_valid= y_valid\n",
    "        main.md = backup_md\n",
    "        params = backup_md['params']\n",
    "        \n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "        ITERATION = backup_md['pickle_name'][0]\n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## SAVE AS FLATFILES ##\n",
    "        #######################################################################################################\n",
    "        train['response'] = y_train\n",
    "        valid['response'] = y_valid\n",
    "\n",
    "        train.to_csv(str(str(ITERATION) + '_t_h2o.csv'), index=False)\n",
    "        valid.to_csv(str(str(ITERATION) + '_v_h2o.csv'), index=False)\n",
    "        #######################################################################################################\n",
    "        \n",
    "        \n",
    "        #######################################################################################################\n",
    "        ## H2O AUTOML ##\n",
    "        #######################################################################################################\n",
    "        h2o_os = params['h2o_automl_params']['oversampling']\n",
    "        h2o_us = params['h2o_automl_params']['undersampling']\n",
    "        h2o_bc = params['h2o_automl_params']['balance_classes']\n",
    "        \n",
    "        aml, h2o_valid = h2o_automl.automl(os=h2o_os, bc=h2o_bc, us=h2o_us, iter=ITERATION)\n",
    "        \n",
    "        pred, predict, score = h2o_automl.get_score(aml=aml, h2o_valid=h2o_valid, y_valid=y_valid,\n",
    "                                                    threshold=aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        \n",
    "        setattr(main, str('aml_' + str(ITERATION)), aml)\n",
    "        setattr(main, str('pred_' + str(ITERATION)), pred)\n",
    "        setattr(main, str('predict_' + str(ITERATION)), predict)\n",
    "        setattr(main, str('score_' + str(ITERATION)), score)\n",
    "        setattr(main, str('threshold_' + str(ITERATION)), aml.leader.find_threshold_by_max_metric('min_per_class_accuracy'))\n",
    "        setattr(main, str('h2o_valid_' + str(ITERATION)), h2o_valid)\n",
    "        #######################################################################################################\n",
    "        trials = score\n",
    "        \n",
    "        return trials        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-706f1d1a8029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create an instance of the main class and call it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols_to_remove\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'response'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "# create an instance of the main class and call it\n",
    "x = main()\n",
    "\n",
    "trials = x.prepare(cols_to_remove=['id'], response='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores from the best aml object for our validation set\n",
    "h2o_automl.get_score(aml=x.aml_1, h2o_valid=x.h2o_valid_1, y_valid=x.y_valid, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# use below segment to read in any particular backup as needed (touchpoint is immediately prior to calling automl)\n",
    "import pickle\n",
    "f = open(\"backup_be_simple.pickle\", \"rb\")\n",
    "train = pickle.load(f)\n",
    "valid = pickle.load(f)\n",
    "y_train = pickle.load(f)\n",
    "y_valid = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
