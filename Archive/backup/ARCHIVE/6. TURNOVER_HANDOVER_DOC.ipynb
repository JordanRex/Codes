{"cells":[{"cell_type":"markdown","source":["# _GLOBAL TURNOVER ASSET_\n&nbsp;\n- **Author:** *Varun V*\n- **Team:** *People Analytics*\n- **Location:** *GCC*\n- **Date:** *Oct 2018*\n- **Description:** *Transition Documentation for GCC handover*\n---"],"metadata":{}},{"cell_type":"markdown","source":["## Environment:\n&nbsp;\n1. **OS** - Windows 10\n2. **Python** - Anaconda version 3.6.5\n3. **Packages** - *Described in the requirements file of the project (requirements.txt in the project dir)*\n4. **Deployed Location** - Azure VM; with data in Azure Datalake and production running on an Azure Databricks cluster\n5. **Structure** - Three python notebooks\n    1. Data Preparation\n        1. Raw_ADS_preparation.ipynb\n        2. Final_ADS_preparation.ipynb\n    2. Modelling FW\n        1. Turnover_modelling_FW.ipynb\n    3. Prediction and Factor analysis\n        1. Predict.ipynb\n        2. Factor_analysis.ipynb\n---"],"metadata":{}},{"cell_type":"markdown","source":["## **Contents**\n&nbsp;\n1. Purpose of the Document\n2. Primary Steps in the Asset\n    1. Data Procurement\n    2. Data Preparation\n        1. Monthly/Yearly dataset preparation\n        2. TRAIN and VALID datasets preparation\n    3. Turnover Modelling Framework\n        1. Data Preprocessing\n        2. Classification Module\n    4. Model Interpretation and Deployment\n        1. Prediction\n        2. Factor analysis\n3. Consumption Guidelines\n    1. Click (Stay Interviews)\n    2. Feedback Module\n        1. Adoption Module\n            1. Trends/Insights Generation (High level stakeholder dashboard)\n            2. Feedback Incorporation\n            3. Benefit/ROI calculator\n        2. Health Module\n            1. Data Pollution Module\n            2. Model retrain/refresh cadence determiner\n4. Closure Module\n    1. Transition process\n    2. Leverage Module\n    3. Appendix and Glossary\n    4. References"],"metadata":{}},{"cell_type":"markdown","source":["## **Purpose**\n&nbsp;\n- To ensure seamless transition of the entire pipeline to another entity\n- Complete technical documentation of the entire Turnover process from scratch to final adoption/health monitoring\n- Complete manual processes documentation involved in the pipeline\n- To create modules that can be scaled/leveraged across the organization in other problems\n- To ensure a viable consumption pipeline that maximizes the ROI potential of the whole process\n    - The generic process flow across the various individual modules\n    - ***Perspective***:\n        - **Level** - Actionable steps\n        - **Order** - Chronological order along the time and implementation dimensions (Linear. No parall steps)\n        - **Audience** - Primarily intended for a Data Scientist (few existing steps will be translated to a Data Engineer/BI developer)"],"metadata":{}},{"cell_type":"markdown","source":["## **Primary Steps**\n&nbsp;\n1. **Data Procurement**\n    1. Mappping all the individual data sources\n        1. flatfile reports (from the corresponding data owners; mostly year end reports (headcount, turnover))\n            - manual\n        2. datalake\n            - automated (currently not implemented)\n        3. sharpops/navigate/payroll (target, movement, opr, salary, demographics)\n            - automated (currently not implemented)\n    2. Identification and mapping of the different owners for the above\n        - the manually procured ones\n        - the automated ones\n    3. The sources and owners table is displayed below:\n    4. The cadence for the above data sources are:  \n        1. Every Monday\n            - headcount\n            - movements\n            - PBP mapping\n        2. 1st Monday of every month\n            - headcount\n            - movements\n\n2. **Data Preparation**\n    1. Monthly/Yearly dataset preparation\n        1. Notebook\n            1. Name - Raw_ADS_preparation.ipynb\n            2. Location - Azure VM xxx\n        2. Purpose\n            1. Source in all the different data sources\n                - automated pull or reading in the flatfile reports saved in the Azure Datalake\n            2. Preprocess and clean with business rules\n            3. Aggregate and create the final raw ADS\n                - Level = Employee and Time\n                    - Time level = year (i.e. employee + year)\n                    - Time level = monthly (i.e. employee + year_month)\n        3. Features - Raw base features (may have missing values, violate business rules)\n            - target\n            - opr\n            - salary\n            - personal attributes(age, tenure, hire date, position, location)\n            - manager attributes(same as personal) etc.\n            - ~90 base features\n    2. TRAIN and VALID datasets preparation\n        1. Notebook\n            1. Name - Final_ADS_preparation.ipynb\n            2. Location - Azure VM xxx\n        2. Purpose - Depending on the level\n            1. For yearly datasets - simply appending them\n            2. For monthly datasets - passing them through the Random Sampling module\n        3. Labels - Response variable (0/1) for the two datasets are also applied based on a logic that is user customizable\n            - 0 stands for active, while 1 stands for leaver\n            - Demarcation = Time dimension based (to differentiate between what is TRAIN and VALID)\n                - 2017 Dec 31st\n            - recommended to tune on the definition for labels\n                - for the TRAIN dataset, positive labels could be the last n (n=3/4/5/6 records) of the leavers\n                - for the VALID dataset, positive labels are all the people who left between 1st Jan and 30th June 2018\n                - for both datasets, zero labels are all the remaining records (all for still active, remaining for the leavers)\n        4. Output\n            - A custom logic is applied to select a subset of the monthly datasets\n            - Final TRAIN and VALID datasets are prepared and placed in the Azure Datalake\n\n3. **Turnover Modelling Framework**\n    1. Notebook\n        1. Name - Turnover_modelling_FW.ipynb\n        2. Location - Azure VM xxx\n        3. Content\n            1. Data Preprocessing\n                1. Steps\n                    1. Cleaning\n                        - Table/Field header cleaning\n                        - Missing value treatment module\n                    2. Processing\n                        - Correlation analysis\n                        - Categorical feature encoding\n                        - Feature scaling/transformations\n                2. Output\n                    - The processed TRAIN and VALID datasets are prepared which go into the modelling framework\n            2. Classification module\n                1. Steps\n                    1. Perform baseline logistic regression model\n                        - if the performance of the simple model suffices the modelling pipeline ends here\n                    2. If the performance is not upto the desired mark\n                        - Use the Turnover asset (a much more sophisticated pipeline) to get better results\n                        - Seperate technical documentation for the asset (only for data scientists)\n                2. Output\n                     - A final model object that is used for predictions (1)\n                     - missing value treatment object (1)\n                     - Categorical feature encoding objects/dictionary (1)\n\n4. **Model Integration and Deployment**\n    1. Prediction\n        1. Notebook\n            1. Name - Predict.ipynb\n            2. Location - Azure VM xxx\n        2. Steps\n            1. Open the Predict.ipynb notebook\n            2. Make sure the 3 objects from the previous model creation exercise is present in the folder xxx\n            3. Run the notebook to generate the predictions for all the active people as of that day\n        3. Output\n            - The final predictions (probablities) are generated and saved as a flatfile\n    2. Factor analysis\n        1. Notebook\n            1. Name - Factor_analysis.ipynb\n            2. Location - Azure VM xxx\n        2. Steps\n            1. Open the Factor_analysis.ipynb notebook\n            2. Make sure the required input file (the predictions flatfile from the above step in the pipeline) is present in the location xxx\n            3. Run the notebook to generate the final flatfile with the predictions and the macro factors/questions mapped to each employee\n        3. Output\n            - the final flatfile with everything except for the PBP mapping is generated"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nx=pd.read_csv('/dbfs/mnt/datalake/NAZ/People/TurnoverModel/Data/Handover/DataSourcesMapping.csv')\n\nfrom tabulate import tabulate\nprint(tabulate(x, tablefmt=\"pipe\", headers=\"keys\", showindex=False))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">| Data Source                       | Frequency of upload   | Point of Contact                |\n:----------------------------------|:----------------------|:--------------------------------|\n Headcount                         | weekly                | Scott Keller &amp; RK (GCC) | SFTP  |\n Movement                          | weekly                | Scott Keller &amp; RK (GCC) | SFTP  |\n Demographics (PBP z-relationship) | weekly                | Scott Keller &amp; RK (GCC) | SFTP  |\n Turnover file                     | monthly               | Scott Keller &amp; RK (GCC) | SFTP  |\n Target Achievement (Navigate)     | yearly                | Kayla Johnson                   |\n OPR (Navigate)                    | yearly                | Catherine Sweeting/Greg Bagley  |\n Salary/Bonus                      | quarterly             | Jesse Graftenreed/Haley Bollman |\n Competencies (Navigate)           | yearly                | Catherine Sweeting/Greg Bagley  |\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["## **Consumption Guidelines**\n&nbsp;\n  1. Click (*Stay Interviews*) - this phase details out the steps to be taken once the final prediction dataset with the macro factors mapped is generated\n    1. Steps\n      1. Take the final predictions flatfile with the macro factors mapped out\n      2. Perform the PBP mapping (manual for now, will be automated later down the line)\n        - Steps for the same are:\n          1. ...\n          2. ...\n      3. Create the final output file and export it into the Azure SQL Db\n  2. Feedback Module\n    1. Adoption Module\n      1. Trends/Insights Generation (High level stakeholder dashboard)\n      2. Feedback Incorporation\n      3. Benefit/ROI calculator\n    2. Health Module\n      1. Data Pollution Module\n      2. Model retrain/refresh cadence determiner\n      \n**__in progress__**"],"metadata":{}},{"cell_type":"markdown","source":["## **Closure Module**\n&nbsp;\n  1. Transition process\n  - Leverage Module\n  - Appendix and Glossary\n  - References\n    \n**__in progress__**"],"metadata":{}}],"metadata":{"name":"TURNOVER_HANDOVER_DOC","notebookId":3678653068873080},"nbformat":4,"nbformat_minor":0}
