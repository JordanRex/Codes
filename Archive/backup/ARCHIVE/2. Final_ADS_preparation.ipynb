{"cells":[{"cell_type":"markdown","source":["# Final_ADS_preparation.ipynb\n> Project: **ABI Turnover**  \n> Turnover Process Phase: **2**  \n> Author: **Varun V**  \n> Location: **GCC**  \n> Team: **People Analytics**"],"metadata":{}},{"cell_type":"markdown","source":["## Importing the packages"],"metadata":{}},{"cell_type":"code","source":["## importing the relevant packages:\n\n# clear the workspace\n#%reset -f\n\n# print list of files in directory\nimport os\nprint('The files in the folder are: ', os.listdir())\n\n# the base packages\nimport collections # for the Counter function\nimport csv # for reading/writing csv files\nimport pandas as pd, numpy as np, time, gc, bisect, re\nimport itertools as itertools\n\nrandomseed = 1 # the value for the random state used at various points in the pipeline\npd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\npd.options.display.max_columns = 200\n\n#to display multiple outputs in a cell without using print/display\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The files in the folder are:  [&#39;conf&#39;, &#39;ganglia&#39;, &#39;derby.log&#39;, &#39;logs&#39;, &#39;eventlogs&#39;]\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["## Prepare the train and validation dataset with the labels\n\n- The turnover files ('16 and '17)\n    - returns turnover dataframe with leavers and their termination date\n    - returns the unique list of leavers\n- The raw ads files (from June16 to Dec17 monthly files)\n    - all of them are aggregated\n    - split into two (active/leavers)\n    - 6? most recent records of leavers -> label=1\n    - 2? random records of active + leavers (records older than last 6?) -> label=0\n    \n**P.S: **`This is applicable for the context where you have monthly datasets. if you have yearly datasets then this module does nothing and you can directly use the datasets in the next notebook (Turnover_modelling_FW.ipynb) after adding the labels`"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/OUTPUT/MONTHLY_DATASETS/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201605.csv</td><td>201605.csv</td><td>3736913</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201606.csv</td><td>201606.csv</td><td>3697835</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201607.csv</td><td>201607.csv</td><td>4364859</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201608.csv</td><td>201608.csv</td><td>4282142</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201609.csv</td><td>201609.csv</td><td>4618496</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201610.csv</td><td>201610.csv</td><td>4609998</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201611.csv</td><td>201611.csv</td><td>4554487</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201612.csv</td><td>201612.csv</td><td>4439758</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201701.csv</td><td>201701.csv</td><td>4316376</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201702.csv</td><td>201702.csv</td><td>4282068</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201703.csv</td><td>201703.csv</td><td>4211843</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201704.csv</td><td>201704.csv</td><td>4290405</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201705.csv</td><td>201705.csv</td><td>4216547</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201706.csv</td><td>201706.csv</td><td>4143849</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201707.csv</td><td>201707.csv</td><td>4378680</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201708.csv</td><td>201708.csv</td><td>4289882</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201709.csv</td><td>201709.csv</td><td>4461392</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201710.csv</td><td>201710.csv</td><td>4347563</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201711.csv</td><td>201711.csv</td><td>4431271</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201712.csv</td><td>201712.csv</td><td>4278941</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201801.csv</td><td>201801.csv</td><td>4272668</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201802.csv</td><td>201802.csv</td><td>4202337</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201803.csv</td><td>201803.csv</td><td>4127961</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201804.csv</td><td>201804.csv</td><td>4173175</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201805.csv</td><td>201805.csv</td><td>4077946</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201806.csv</td><td>201806.csv</td><td>4009274</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201807.csv</td><td>201807.csv</td><td>4252480</td></tr><tr><td>dbfs:/mnt/datalake/OUTPUT/MONTHLY_DATASETS/201808.csv</td><td>201808.csv</td><td>4176902</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["class prepare_ads:\n  \"\"\" this class creates the training dataframes and the labels from the raw monthly datasets \"\"\"\n\n  def __init__(self, n=1, m=6, train_start=201601, train_threshold=201801, valid_month=201801, to_threshold=201806):\n    # the dictionary for months and month index mapping (uncomment if necessary)\n    self.dict_of_months = {'jan': 'January', 'feb': 'February', 'mar': 'March', 'april': 'April', 'may': 'May', 'june': 'June', 'july': 'July', \n                      'aug': 'August', 'sep': 'September', 'oct': 'October', 'nov': 'Novembor', 'dec': 'December'}\n    self.dict_of_months_ids = {'jan': 1, 'feb': 2, 'mar': 3, 'april': 4, 'may': 5, 'june': 6, 'july': 7, 'aug': 8, \n                          'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n    self.n = n\n    self.m = m\n    self.trainstart = train_start\n    self.trainthreshold = train_threshold\n    self.validmonth = valid_month\n    self.tothreshold = to_threshold\n\n    self.to_read_all()\n    self.read_all()\n\n    train_name = str('TRAIN_' + str(train_threshold) + '_' + str(n) + '_' + str(m) + '.csv')\n    valid_name = str('VALID_' + str(valid_month) + '_' + str(to_threshold) + '.csv')\n    self.train.to_csv(str('/dbfs/mnt/datalake/OUTPUT/' + train_name), index=False)\n    self.valid.to_csv(str('/dbfs/mnt/datalake/OUTPUT/' + valid_name), index=False)\n    \n  # turnover file creation function\n  def to_read_all(self):\n    # psg bands to filter\n    psg_bands = ['vi_b', 'vii_a', 'vii_b', 'v_a', 'viii_a', 'iii_a', 'v_b', 'vi_a', 'iv_b',\n   'x_a', 'iv_a', 'viii_b', 'ii_b', 'ii_a', 'ix_b', 'ix_a', 'iii_b', 'i_b', 'i_a', 'x_b', 'xi_b', 'xi_a']\n\n    # read in the turnover main file\n    to=pd.read_csv('/dbfs/mnt/datalake/OUTPUT/turnover_labels_df.csv')\n    to=to[['employee_id', 'pay_scale_group', 'name_of_action_type', 'name_of_reason_for_action', 'termination_date_yearmonth', 'year']]\n    to=to.loc[to['name_of_reason_for_action'].isin(['resignation_general', 'resignation _ personal reason', 'resignation _ others', 'mutual agreement', 'early retirement', 'resign _ career growth_opportu'])].reset_index(drop=True)\n    to=to[to['pay_scale_group'].isin(psg_bands)]\n    # below commented lines to be used for another context where you have a different naming convention. use as & when necessary\n    #to['term_year'] = to['termination_date'].dt.year\n    #to['term_month'] = to['termination_date'].dt.month_name()\n    #to['term_monthid'] = to['termination_date'].dt.month\n    to.sort_values(inplace=True, by=['employee_id', 'termination_date_yearmonth'])\n    to.drop_duplicates(inplace=True, subset='employee_id')\n    to_train = to[to['termination_date_yearmonth'] < self.trainthreshold]\n    to_valid = to[(to['termination_date_yearmonth'] > self.validmonth) & (to['termination_date_yearmonth'] <= self.tothreshold)]\n    to_train_ids = to_train['employee_id'].unique()\n    to_valid_ids = to_valid['employee_id'].unique()\n    to.rename(columns={'employee_id': 'employee_personnal_number_pa', 'termination_date_yearmonth': 'yearmonth'}, inplace=True)\n\n    self.to = to\n    self.to_train_ids = to_train_ids\n    self.to_valid_ids = to_valid_ids\n    return None\n\n  # final ads creation function to aggregate all the monthly files\n  # n = number of random records to be taken from the pool of records with label=0\n  # m = number of recent records to be taken from the pool of records of leavers and given label=1\n  def read_all(self):\n    ## reading in the multiple month level files and mapping the labels based on the to files\n    files = dbutils.fs.ls('/mnt/datalake/OUTPUT/MONTHLY_DATASETS/')\n    file_names = {}\n\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n\n    file_names_keys = list(file_names.keys())\n    file_names_keys_values = [s.replace('.csv', '')for s in file_names_keys]\n    train_list = [i for i in file_names_keys_values if ((int(i)>self.trainstart) & (int(i)<self.trainthreshold))]\n    test_list = [i for i in file_names_keys_values if int(i) == self.validmonth]\n\n    dftrain = {}\n    dfvalid = []\n\n    # append the (multiple) datasets into one TRAIN master set\n    iter=0\n    for i in train_list:\n        dftrain[i] = pd.read_csv(str('/dbfs/mnt/datalake/OUTPUT/MONTHLY_DATASETS/' + i + '.csv'))\n        dftrain[i]['yearmonth'] = int(i)\n        # below commented lines to be used for another context where you have a different naming convention. use as & when & if necessary\n        #dftrain[i]['term_year'] = int(i)\n        #dftrain[i]['term_month_temp'] = dftrain[i]['month_file'].apply(lambda s:s.split('_')[0])\n        #dftrain[i]['term_month'] = dftrain[i]['term_month_temp'].map(self.dict_of_months)\n        #dftrain[i]['term_monthid'] = dftrain[i]['term_month_temp'].map(self.dict_of_months_ids)\n        iter = iter+1\n    # create the VALID dataset\n    dfvalid = pd.read_csv(str('/dbfs/mnt/datalake/OUTPUT/MONTHLY_DATASETS/' + str(self.validmonth) + '.csv'))\n    dfvalid['yearmonth'] = self.validmonth\n    dfvalid['label'] = 0\n\n    # combine the TRAIN monthly datasets\n    df_all = pd.concat(dftrain.values(), ignore_index=True)\n    df_all['employee_personnal_number_pa'] = df_all['employee_personnal_number_pa'].astype(int)\n\n    # mapping the leavers (labels) for both TRAIN and VALID datasets\n    df_all_temp=df_all.copy()\n    df_all_temp['flag'] = df_all_temp['employee_personnal_number_pa'].isin(self.to_train_ids).astype(int)\n    dfvalid['label'] = np.where(dfvalid['employee_personnal_number_pa'].isin(self.to_valid_ids), 1, 0).astype(int)\n\n    # taking top m recent records of leavers to flag as labels=1\n    x=df_all_temp.sort_values(ascending=False, by=['yearmonth']).groupby('employee_personnal_number_pa').head(self.m)\n    # taking only The most recent record of leavers for label=1\n    #x=df_all_temp.sort_values(ascending=False, by=['global id', 'term_year', 'term_monthid']).drop_duplicates(['global id'])\n    x=x.loc[x['flag'] == 1].reset_index(drop=True)\n    x=x[['employee_personnal_number_pa', 'yearmonth', 'flag']].reset_index(drop=True)\n    x.rename(columns={'flag': 'label'}, inplace=True)\n\n    df_all_new=pd.merge(df_all, x, how='left', on=['employee_personnal_number_pa', 'yearmonth'])\n    df_all_new['label'].fillna(0, inplace=True)\n    df_all_new['label'] = df_all_new['label'].astype(int)\n    df_all_new.sort_values(by=['employee_personnal_number_pa', 'yearmonth'], inplace=True)\n\n    mask = df_all_new['label'] == 1\n    df_pos_labels = df_all_new[mask]\n    df_neg_labels = df_all_new[~mask]\n\n    df_neg_labels=df_neg_labels.groupby('employee_personnal_number_pa').apply(lambda x: x.sample(self.n, replace=True)).reset_index(drop=True)\n    #df_neg_labels['groupid'] = df_neg_labels.groupby(['global id']).cumcount()+1\n    #df_neg_labels['sample_flag'] = np.where((df_neg_labels['groupid'] % n == 0), 1, 0)\n    #df_neg_labels = df_neg_labels.loc[df_neg_labels['sample_flag'] == 1].reset_index(drop=True)\n    #df_neg_labels.drop(['groupid', 'sample_flag'], axis=1, inplace=True)\n\n    df_all_complete = pd.concat([df_pos_labels.reset_index(drop=True), df_neg_labels], axis=0)\n    #df_all_complete.drop(['yearmonth'], axis=1, inplace=True)\n    self.train = df_all_complete\n    self.valid = dfvalid\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# the mother class\n\nclass final_ads:\n  #put filter to make sure validation timeframe consistent with m\n  def __init__(self):\n    self.n_range = np.arange(1, 3, dtype=int)\n    self.m_range = np.arange(2, 7, dtype=int)\n    self.trainthresh_range = [201801]\n    self.validmonth_range = np.arange(201801, 201809, dtype=int)\n    self.tothresh_range = np.arange(201802, 201812, dtype=int)\n    self.trainstart_range = [201601]\n\n    self.product = list(itertools.product(self.n_range, self.m_range, self.trainstart_range, self.trainthresh_range, self.validmonth_range, self.tothresh_range))\n    self.main()\n    \n  def main(self):\n    # the grid is created in self.product, processed here\n    # iterating through each combination in it and calling the ads_preparation module for each corresponding list of values\n    grid_df = pd.DataFrame(self.product, columns=['nrange', 'mrange', 'trainstart', 'trainthresh', 'validmonth', 'turnoverthresh'])\n    grid_df_filtered = grid_df[grid_df['turnoverthresh'] > (grid_df['validmonth'] + 1)].reset_index(drop=True)\n    grid_df_filtered = grid_df_filtered[(grid_df_filtered['turnoverthresh']-grid_df_filtered['validmonth']) == grid_df_filtered['mrange']].reset_index(drop=True)\n    for i in range(grid_df_filtered.shape[0]):\n      prepare_ads(n=grid_df_filtered.loc[i, 'nrange'],\n                  m=grid_df_filtered.loc[i, 'mrange'],\n                  train_start=grid_df_filtered.loc[i, 'trainstart'],\n                  train_threshold=grid_df_filtered.loc[i, 'trainthresh'],\n                  valid_month=grid_df_filtered.loc[i, 'validmonth'],\n                  to_threshold=grid_df_filtered.loc[i, 'turnoverthresh'])\n    self.grid = grid_df_filtered\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["final_grid = final_ads()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Output\n\n- The ***TRAIN*** and ***VALID*** datasets are written as flatfiles in the working directory\n\n`this marks the completion of the`** Final_ADS_preparation.ipynb **`notebook`"],"metadata":{}}],"metadata":{"name":"2. Final_ADS_preparation","notebookId":246691945831692},"nbformat":4,"nbformat_minor":0}
