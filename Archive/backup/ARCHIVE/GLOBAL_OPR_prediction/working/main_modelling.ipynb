{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Notebook\n",
    "\n",
    "- Dev (cv based iterations on 2017+2018)\n",
    "- Final model and object packing\n",
    "- Prediction (for 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import sys, pandas as pd, numpy as np, inspect, re as re, functools as functools, pickle, glob, warnings, os\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# sklearn packages\n",
    "import sklearn\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.base import clone\n",
    "\n",
    "# some options/variables\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 50 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# to display multiple outputs in a cell without usin print/display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# append the scripts path to pythonpath\n",
    "sys.path.append('./scripts/')\n",
    "\n",
    "# ignore warnings (only if you are the kind that would code when the world is burning)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plot inline\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# # import the various ml modules\n",
    "import xgboost as xgb\n",
    "\n",
    "############################################## import the custom modules ################################\n",
    "import helperfuncs as helper\n",
    "import feateng as fte\n",
    "import misc as miscfun\n",
    "from misc import ce_encodings, DataFrameImputer, scalers\n",
    "\n",
    "# instantiate the classes\n",
    "helpers = helper.helper_funcs()\n",
    "cust_funcs = fte.custom_funcs()\n",
    "feat_sel = miscfun.feat_selection()\n",
    "\n",
    "#############################################################################################################\n",
    "# global function to flatten columns after a grouped operation and aggregation\n",
    "# outside all classes since it is added as an attribute to pandas DataFrames\n",
    "def __my_flatten_cols(self, how=\"_\".join, reset_index=True):\n",
    "    how = (lambda iter: list(iter)[-1]) if how == \"last\" else how\n",
    "    self.columns = [how(filter(None, map(str, levels))) for levels in self.columns.values] \\\n",
    "    if isinstance(self.columns, pd.MultiIndex) else self.columns\n",
    "    return self.reset_index(drop=True) if reset_index else self\n",
    "pd.DataFrame.my_flatten_cols = __my_flatten_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.datalake.store import core, lib, multithread\n",
    "\n",
    "tenant = 'cef04b19-7776-4a94-b89b-375c77a8f936'\n",
    "resource = 'https://datalake.azure.net/'\n",
    "client_id = 'e9aaf06a-9856-42a8-ab3c-c8b0d3a9b110'\n",
    "client_secret = 'DlbuV60szYT2U0CQNjzwRA55EsH42oX92AB7vbD2clk='\n",
    "\n",
    "adlcreds = lib.auth(tenant_id = tenant,\n",
    "                   client_secret = client_secret,\n",
    "                   client_id = client_id,\n",
    "                   resource = resource)\n",
    "\n",
    "subs_id = '73f88e6b-3a35-4612-b550-555157e7059f'\n",
    "adls = 'edhadlsanasagbdev'\n",
    "\n",
    "adlsfsc = core.AzureDLFileSystem(adlcreds, store_name=adls)\n",
    "\n",
    "path = '/root/anasandbox/people/opr10x/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "with adlsfsc.open(path + '/2019/Data/Output_Data/ads/final_ads.pickle', 'rb') as f:\n",
    "    ads = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    6470\n",
       "3.0    4318\n",
       "4.0    1712\n",
       "1.0     902\n",
       "5.0     537\n",
       "0.0     472\n",
       "Name: opr, dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ads.opr.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final PROCESSING\n",
    "\n",
    "# remove opr==1B -> right here, right now\n",
    "ads = ads[ads['opr']>0]\n",
    "ads['opr'] = ads['opr']-1\n",
    "ads.reset_index(drop=True, inplace=True)\n",
    "\n",
    "## split into train+valid/prediction sets\n",
    "traindf = ads[ads['year'].isin([2017, 2018])].copy()\n",
    "preddf = ads[ads['year']==2019].copy()\n",
    "\n",
    "traindf.dropna(subset=['global_id', 'opr'], inplace=True, how='any')\n",
    "preddf.dropna(subset=['global_id'], inplace=True, how='any')\n",
    "\n",
    "traindf.reset_index(drop=True, inplace=True)\n",
    "preddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ytraindf = np.array(traindf.opr)\n",
    "\n",
    "traindf.drop(columns=['opr'], inplace=True)\n",
    "preddf.drop(columns=['opr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 6.920e+02],\n",
       "       [1.000e+00, 4.980e+03],\n",
       "       [2.000e+00, 3.385e+03],\n",
       "       [3.000e+00, 1.317e+03],\n",
       "       [4.000e+00, 4.100e+02]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.unique(ytraindf, return_counts=True)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORDINAL CLASSIFICATION APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['global_id', 'employee_band', 'ebm_level', 'year', 'function',\n",
       "       'mr_pers_compgroup_year_comp_score_mean_functional_competencies',\n",
       "       'mr_pers_compgroup_year_comp_score_mean_leadership_competencies',\n",
       "       'mr_pers_compgroupl1_year_comp_score_mean_leadership_competencies_develop_people',\n",
       "       'mr_pers_compgroupl1_year_comp_score_mean_leadership_competencies_dream_big',\n",
       "       'mr_pers_compgroupl1_year_comp_score_mean_leadership_competencies_live_our_culture',\n",
       "       'net_target', 'teamsize', 'teamsize_delta', 'index_average',\n",
       "       'position_velocity', 'emp_time_in_band1', 'count_of_belts',\n",
       "       'talentpool_renomination', 'talentpool', 'engagement_score',\n",
       "       'manager_effectiveness_score', 'fs_prom', 'fs_ho', 'fs_adherant_perc',\n",
       "       'fs_to_overall', 'dr_prom', 'dr_ho', 'dr_adherant_perc',\n",
       "       'dr_to_overall', 'mean_team_tenure', 'lc_count', 'fc_count',\n",
       "       'position_tenure', 'zone', 'target_delta', 'prev_opr', 'prev_prev_opr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT INTO TRAIN==2017 AND VALID==2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_0to5_2017 = traindf.copy()\n",
    "# train_0to5_2017['response'] = ytraindf\n",
    "# train_0to5_2017 = train_0to5_2017[train_0to5_2017['year'].isin([2017])]\n",
    "# ytrain_0to5_2017 = np.array(train_0to5_2017.response)\n",
    "# train_0to5_2017.drop(columns=['response'], inplace=True)\n",
    "\n",
    "# valid_0to5_2018 = traindf.copy()\n",
    "# valid_0to5_2018['response'] = ytraindf\n",
    "# valid_0to5_2018 = valid_0to5_2018[valid_0to5_2018['year']==2018]\n",
    "# yvalid_0to5_2018 = np.array(valid_0to5_2018.response)\n",
    "# valid_0to5_2018.drop(columns=['response'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "clf\n",
    "\n",
    "params = {'objective': ['binary:logistic'],\n",
    "          'learning_rate': [0.1],\n",
    "          'max_depth': [3,10],\n",
    "          'min_child_weight': [7],\n",
    "          'silent': [1],\n",
    "          'subsample': [0.85],\n",
    "          'colsample_bytree': [0.85],\n",
    "          'n_estimators': [10],\n",
    "          'seed': [1],\n",
    "          'scale_pos_weight': [1,3,5],\n",
    "          'base_score': [0.35, 0.45],\n",
    "          'max_bin': [300],\n",
    "          'gamma': [0.01],\n",
    "          'reg_alpha': [0.01],\n",
    "          'reg_lambda': [0.03]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use below script to iterate on train=2017, valid=2018\n",
    "\n",
    "# %run -i ./scripts/ordinal_classifier_with_valid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category encoding is happening ... \n",
      "\n",
      "category encoding completed \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   19.3s finished\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 25%|█████████████████████                                                               | 1/4 [00:19<00:59, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    9.5s finished\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████████████████████████████████                                          | 2/4 [00:30<00:34, 17.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    9.4s finished\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████████████████████████████████████████████████████████████                     | 3/4 [00:40<00:15, 15.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    9.0s finished\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:50<00:00, 13.35s/it]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## use below script to create cross-validated model on 2017+2018 or 2017 only or 2018 only\n",
    "\n",
    "%run -i ./scripts/ordinal_classifier_without_valid.py\n",
    "\n",
    "xgb_ordinal = OrdinalClassifier(train=traindf.copy(), ytrain=ytraindf.copy(),\n",
    "                               clf=clf, params=params)\n",
    "\n",
    "# fit the binary classifiers\n",
    "xgb_ordinal.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "with adlsfsc.open(path + '/2019/Data/Output_Data/ml_prediction_objects/ordinal_classifier_objects.pickle', 'wb') as f:\n",
    "    pickle.dump(xgb_ordinal, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICLASS APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ./scripts/ml_xgboost_5class.py\n",
    "\n",
    "# xgb_model, xgb_feat_names, xgtrain, xgtest, xgbpred, xgbpredprobs = quick_model_xgb(train=train_0to5_2017.copy(),\n",
    "#                                                                                     valid=valid_0to5_2018.copy(),\n",
    "#                                                                                     ytrain=ytrain_0to5_2017.copy(),\n",
    "#                                                                                     yvalid=yvalid_0to5_2018.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_params(space_size=3, randomseed=123):\n",
    "#     depth = np.arange(15, 20, dtype=int)\n",
    "#     delta_step = np.arange(3, 6, dtype=int)\n",
    "#     base_score = np.arange(0.35, 0.5, 0.05, dtype=float)\n",
    "#     colsample_level = np.arange(0.7, 0.9, 0.05, dtype=float)\n",
    "#     colsample_tree = np.arange(0.7, 0.9, 0.05, dtype=float)\n",
    "#     subsample = np.arange(0.75, 0.9, 0.05, dtype=float)\n",
    "    \n",
    "#     model_params_grid = list(itertools.product(depth, delta_step, base_score, \n",
    "#                                           colsample_level, colsample_tree, subsample))\n",
    "#     model_params_grid = pd.DataFrame(model_params_grid, \n",
    "#                 columns=['maxdepth', 'maxdeltastep', 'basescore', 'colsamplebylevel', 'colsamplebytree', 'subsample'])\n",
    "#     model_params_grid = model_params_grid.sample(space_size, random_state=randomseed).reset_index(drop=True)\n",
    "    \n",
    "#     return model_params_grid\n",
    "\n",
    "# mpg = model_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ./scripts/ml_xgboost_5class_cv.py\n",
    "\n",
    "# xgb_model={}\n",
    "# xgb_featnames={}\n",
    "# xgb_cv_results={}\n",
    "# encoderobj={}\n",
    "# scalerobj={}\n",
    "\n",
    "# for i in tqdm(range(mpg.shape[0])):\n",
    "#     xgb_model[i], xgb_featnames[i], xgb_cv_results[i], encoderobj[i], scalerobj[i] = quick_model_xgb(train=traindf.copy(), \n",
    "#                                                                                         ytrain=ytraindf.copy(),\n",
    "#             params={'objective':'multi:softprob', 'eval_metric':['mlogloss', 'merror'], 'tree_method':'exact', \n",
    "#                     'silent':1, 'nthread':-1, 'num_class':5, 'learning_rate':0.1,  \n",
    "#                     'n_jobs': -1, 'seed':1, 'grow_policy':'lossguide', 'max_bin':500, \n",
    "#                     'alpha': 0.02, 'gamma': 0.03, 'lambda': 0.01, 'min_child_weight': 9,\n",
    "#                     ## params from the param grid are below\n",
    "#                     'max_depth': mpg.loc[i, 'maxdepth'], \n",
    "#                     'max_delta_step': mpg.loc[i, 'maxdeltastep'], \n",
    "#                     'base_score': mpg.loc[i, 'basescore'], \n",
    "#                     'colsample_bylevel': mpg.loc[i, 'colsamplebylevel'],\n",
    "#                     'colsample_bytree': mpg.loc[i, 'colsamplebytree'], \n",
    "#                     'subsample': mpg.loc[i, 'subsample']}, num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb.plot_importance(xgb_model[0], max_num_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -i ./scripts/ml_xgboost_tuning.py\n",
    "\n",
    "# xgbmod = xgbclass(train=train_0to5.copy(), ytrain=ytrain_0to5.copy())\n",
    "# xgbtrials=pd.DataFrame(xgbmod.trials.results)\n",
    "# #xgbmod.get_xgb_imp().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save all objects in a pickle for prediction (run this cell if the modelling code used was xgboost-multiclass)\n",
    "\n",
    "# with adlsfsc.open(path + '/2019/Data/Output_Data/ml_prediction_objects/ml_model_scaler_encoder.pickle', 'wb') as f:\n",
    "#     pickle.dump(xgb_model, f)\n",
    "#     pickle.dump(encoderobj, f)\n",
    "#     pickle.dump(scalerobj, f)\n",
    "#     pickle.dump(xgb_featnames, f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
