{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN ARCHIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## contains all the codes not used on a daily basis on the main OPR modelling pipeline or lost its purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## demographics\n",
    "# demo = bp_full[['global_id', 'date_of_birth', 'original_hire_date']].copy()\n",
    "# demo.drop_duplicates(inplace=True)\n",
    "# # for employee\n",
    "# train_demo1 = demo_fn(demo, '201607', 2016)\n",
    "# train_demo2 = demo_fn(demo, '201707', 2017)\n",
    "# train_demo = pd.concat([train_demo1, train_demo2], ignore_index=True, axis=0)\n",
    "# valid_demo = demo_fn(demo, '201807', 2018)\n",
    "# # for manager\n",
    "# train_demo_mng1 = demo_fn(demo, '201607', 2016, idcol='global_id', mngr_variant='yes')\n",
    "# train_demo_mng2 = demo_fn(demo, '201707', 2017, idcol='global_id', mngr_variant='yes')\n",
    "# train_demo_mng = pd.concat([train_demo_mng1, train_demo_mng2], axis=0, ignore_index=True)\n",
    "# valid_demo_mng = demo_fn(demo, '201807', 2018, idcol='global_id', mngr_variant='yes')\n",
    "\n",
    "# train_demo.drop_duplicates(inplace=True)\n",
    "# valid_demo.drop_duplicates(inplace=True)\n",
    "# train_demo_mng.drop_duplicates(inplace=True)\n",
    "# valid_demo_mng.drop_duplicates(inplace=True)\n",
    "\n",
    "# # merge the demo datasets\n",
    "# train = train.merge(train_demo, how='left', on=['global_id', 'year'])\n",
    "# train = train.merge(train_demo_mng, how='left', on=['direct_manager_emp_id', 'year'])\n",
    "# valid = valid.merge(valid_demo, how='left', on=['global_id', 'year'])\n",
    "# valid = valid.merge(valid_demo_mng, how='left', on=['direct_manager_emp_id', 'year'])\n",
    "# ## create the tenure and age difference features as well\n",
    "# train['emp_mngr_tenure_diff'] = train.emp_tenure_asof_current - train.mngr_tenure_asof_current\n",
    "# train['emp_mngr_dob_diff'] = train.emp_age_asof_current - train.mngr_age_asof_current\n",
    "# valid['emp_mngr_tenure_diff'] = valid.emp_tenure_asof_current - valid.mngr_tenure_asof_current\n",
    "# valid['emp_mngr_dob_diff'] = valid.emp_age_asof_current - valid.mngr_age_asof_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## gmt/gmba older codes\n",
    "# def gm_feats_fn(df, year):\n",
    "#     dfname = df.name\n",
    "#     dfyearcol = str(dfname+'_year')\n",
    "#     temp = df[df[dfyearcol]<=year].copy()\n",
    "#     temp[str(dfname+'_flag')] = np.where(temp['status']=='A', 3,\n",
    "#                                              np.where(temp['status']=='I', 2, 1))\n",
    "#     temp = temp[['global_id', str(dfname+'_flag')]]\n",
    "#     temp['year'] = year\n",
    "#     return temp\n",
    "\n",
    "# gmt_train = gm_feats_fn(gmt, year=2016)\n",
    "# gmba_train = gm_feats_fn(gmba, year=2016)\n",
    "# gmt_valid = gm_feats_fn(gmt, year=2018)\n",
    "# gmba_valid = gm_feats_fn(gmba, year=2018)\n",
    "# gmt_train.reset_index(drop=True, inplace=True)\n",
    "# gmt_valid.reset_index(drop=True, inplace=True)\n",
    "# gmba_train.reset_index(drop=True, inplace=True)\n",
    "# gmba_valid.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # creating the single gm file\n",
    "# gm_train = pd.merge(gmt_train, gmba_train, on=['global_id', 'year'], how='outer')\n",
    "# gm_valid = pd.merge(gmt_valid, gmba_valid, on=['global_id', 'year'], how='outer')\n",
    "# gm_train.fillna(0, inplace=True)\n",
    "# gm_valid.fillna(0, inplace=True)\n",
    "\n",
    "# gm_train['gm_hist_flag'] = np.where((gm_train['gmt_hist_flag']==1) | (gm_train['gmba_hist_flag']==1), 1, 0)\n",
    "# gm_valid['gm_hist_flag'] = np.where((gm_valid['gmt_hist_flag']==1) | (gm_valid['gmba_hist_flag']==1), 1, 0)\n",
    "# gm_train['gm_current_flag'] = np.where((gm_train['gmt_current_flag']==1) | (gm_train['gmba_current_flag']==1), 1, 0)\n",
    "# gm_valid['gm_current_flag'] = np.where((gm_valid['gmt_current_flag']==1) | (gm_valid['gmba_current_flag']==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding the previous year features (competency, target)\n",
    "# ## competency\n",
    "# comp_full_prev = comp_full.copy()\n",
    "# comp_full_prev = comp_full_prev.add_prefix(prefix='prev_')\n",
    "# comp_full_prev.rename({'prev_global_id':'global_id'}, axis=1, inplace=True)\n",
    "# ## target\n",
    "# tar_reshaped_prev = tar_reshaped.copy()\n",
    "# tar_reshaped_prev = tar_reshaped_prev.add_prefix(prefix='prev_')\n",
    "# tar_reshaped_prev.rename({'prev_global_id':'global_id', 'prev_target_year':'target_year'}, axis=1, inplace=True)\n",
    "# tar_reshaped_prev['target_year'] = tar_reshaped_prev['target_year']+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with adlsfsc.open(path + '/2019/Data/Raw_Data/navigate/belts/belts_lauren.xlsx', 'rb') as f:\n",
    "#     belts = helpers.xlsx_read(f, belts_cols)\n",
    "# belts.dropna(inplace=True)\n",
    "# belts = belts[belts['status']=='Certified']\n",
    "# belts['date_acquired'] = pd.to_datetime(belts['date_acquired'])\n",
    "# belts['year'] = belts['date_acquired'].dt.year\n",
    "# belts.reset_index(drop=True, inplace=True)\n",
    "# belts['yearmonth'] = belts['year'].astype(str) + '0701'\n",
    "# belts['monthly_file_date'] = pd.to_datetime(belts['yearmonth'], format='%Y%m%d')\n",
    "# belts['year'] = np.where(belts['date_acquired']>belts['monthly_file_date'], belts['year']+1, belts['year'])\n",
    "# belts.rename(columns={'employee_global_id':'global_id', 'license_certification_name':'belt'}, inplace=True)\n",
    "# belts = cust_funcs.force_numeric(belts, cols=['global_id'])\n",
    "# belts = belts[belts['belt']!='White Belt']\n",
    "# belts.drop_duplicates(subset=['global_id', 'belt'], inplace=True)\n",
    "# belts_1 = belts[belts['date_acquired']>np.datetime64('2018-07-01', format='%Y-%m-%d')]\n",
    "# belts_2 = pd.DataFrame(belts_1.groupby('global_id')['belt'].apply(lambda x: '%s' % ' & '.join(x))).reset_index()\n",
    "# belts_2.to_csv('belts_story.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MOVEMENTS OLDER SCRIPT\n",
    "## process the movement file\n",
    "# mov_cols = ['action_start_date', 'action_type', 'employee_band', 'function', 'global_id', \n",
    "#             'manager_personnel_number', 'position_id', 'reason_for_action']\n",
    "# movm = helpers.csv_read('../input/MOVEMENT/Employee_History_till_date.csv', cols_to_keep=mov_cols)\n",
    "\n",
    "# movm.drop_duplicates(subset=['global_id', 'action_start_date', 'action_type', 'reason_for_action'], inplace=True)\n",
    "# movm['action_start_date_new'] = pd.to_datetime(movm['action_start_date'], format='%d-%m-%Y')\n",
    "# movm = helpers.process_columns(df=movm, cols=['action_type', 'reason_for_action'])\n",
    "\n",
    "# # create flag columns to identify if the record is relevant for us in the timeframe we are working with\n",
    "# movm['flag_2017_curr'] = np.where((movm['action_start_date_new']>np.datetime64('2016-06-30')) & \n",
    "#                          (movm['action_start_date_new']<np.datetime64('2017-07-01')), 1, 0)\n",
    "# movm['flag_2018_curr'] = np.where((movm['action_start_date_new']>np.datetime64('2017-06-30')) & \n",
    "#                          (movm['action_start_date_new']<np.datetime64('2018-07-01')), 1, 0)\n",
    "# movm['flag_2017_hist'] = np.where(movm['action_start_date_new']<np.datetime64('2017-07-01'), 1, 0)\n",
    "# movm['flag_2018_hist'] = np.where(movm['action_start_date_new']<np.datetime64('2018-07-01'), 1, 0)\n",
    "# movm['flag_promotion'] = np.where((movm['reason_for_action'].isin(['promotion', 'promotion_band_up', 'promotion_within_band'])), \n",
    "#                                  1, 0)\n",
    "# movm['flag_gf'] = np.where(movm['reason_for_action'].isin(['end_of_grandfathering', 'grandfathering']), 1, 0)\n",
    "# movm['flag_lm'] = np.where(movm['reason_for_action'].isin(['lateral_move']), 1, 0)\n",
    "# movm['flag_ir'] = np.where(movm['reason_for_action'].isin(['internal_restructuring']), 1, 0)\n",
    "# movm['flag_lt'] = np.where(movm['reason_for_action'].isin(['local_transfers']), 1, 0)\n",
    "# movm['flag_th'] = np.where(movm['reason_for_action'].isin(['technical_hiring']), 1, 0)\n",
    "# movm['flag_np'] = np.where(movm['reason_for_action'].isin(['new_position']), 1, 0)\n",
    "# movm['flag_loa'] = np.where(movm['reason_for_action'].isin(['leave_of_absence']), 1, 0)\n",
    "# movm['flag_rep'] = np.where(movm['reason_for_action'].isin(['replacement']), 1, 0)\n",
    "# movm['flag_mi'] = np.where(movm['reason_for_action'].isin(['merit_increase']), 1, 0)\n",
    "\n",
    "# movm_train = movm[['global_id', 'employee_band', 'flag_2017_curr', 'flag_2017_hist', 'flag_promotion', 'flag_gf', 'flag_lm', \n",
    "#                    'flag_ir', 'flag_lt', 'flag_th', 'flag_np', 'flag_loa', 'flag_rep', 'flag_mi']].copy()\n",
    "# movm_valid = movm[['global_id', 'employee_band', 'flag_2018_curr', 'flag_2018_hist', 'flag_promotion', 'flag_gf', 'flag_lm', \n",
    "#                    'flag_ir', 'flag_lt', 'flag_th', 'flag_np', 'flag_loa', 'flag_rep', 'flag_mi']].copy()\n",
    "# movm_train = movm_train[movm_train['flag_2017_hist']==1]\n",
    "# movm_valid = movm_valid[movm_valid['flag_2018_hist']==1]\n",
    "# movm_train.rename(columns={'flag_2017_curr':'flag_curr', 'flag_2017_hist':'flag_hist'}, inplace=True)\n",
    "# movm_valid.rename(columns={'flag_2018_curr':'flag_curr', 'flag_2018_hist':'flag_hist'}, inplace=True)\n",
    "\n",
    "# def mov_feats(df, year):\n",
    "#     df1 = df[['global_id', 'employee_band']]\n",
    "#     df1.dropna(inplace=True)\n",
    "#     df1.drop_duplicates(inplace=True)\n",
    "#     df1 = helpers.process_columns(df1)\n",
    "#     df1 = df1[df1['employee_band'].isin(psg_bands)]\n",
    "#     df1 = df1.groupby(['global_id']).agg({'employee_band':'nunique'}).reset_index()\n",
    "#     df1.columns = ['global_id', 'hist_band_changes']\n",
    "\n",
    "#     df2 = df[df['flag_curr']==1].copy()\n",
    "#     df2 = df2[['global_id', 'employee_band']]\n",
    "#     df2.dropna(inplace=True)\n",
    "#     df2.drop_duplicates(inplace=True)\n",
    "#     df2 = helpers.process_columns(df2)\n",
    "#     df2 = df2[df2['employee_band'].isin(psg_bands)]\n",
    "#     df2 = df2.groupby(['global_id']).agg({'employee_band':'nunique'}).reset_index()\n",
    "#     df2.columns = ['global_id', 'recent_band_changes']\n",
    "\n",
    "#     df3 = df[['global_id', 'flag_gf', 'flag_promotion', 'flag_lm', 'flag_ir', 'flag_lt', 'flag_th', 'flag_np']].copy()\n",
    "#     df3 = df3.groupby(['global_id'], as_index=False).agg([np.sum]).reset_index()\n",
    "#     df3.columns = df3.columns.get_level_values(0)\n",
    "#     df3 = df3.add_prefix('hist_')\n",
    "#     df3.rename(columns={'hist_global_id':'global_id'}, inplace=True)\n",
    "\n",
    "#     df4 = df[df['flag_curr']==1].copy()\n",
    "#     df4 = df4[['global_id', 'flag_gf', 'flag_promotion', 'flag_lm', 'flag_ir', 'flag_lt', 'flag_th', 'flag_np']].copy()\n",
    "#     df4 = df4.groupby(['global_id'], as_index=False).agg([np.sum]).reset_index()\n",
    "#     df4.columns = df4.columns.get_level_values(0)\n",
    "#     df4 = df4.add_prefix('curr_')\n",
    "#     df4.rename(columns={'curr_global_id':'global_id'}, inplace=True)\n",
    "    \n",
    "#     df = df1.merge(df2, on='global_id', how='outer')\n",
    "#     df = df.merge(df3, on='global_id', how='outer')\n",
    "#     df = df.merge(df4, on='global_id', how='outer')\n",
    "    \n",
    "#     df.fillna(value=0, inplace=True)\n",
    "#     df['year'] = year\n",
    "#     return df\n",
    "\n",
    "# movm_train = mov_feats(movm_train, 2017)\n",
    "# movm_valid = mov_feats(movm_valid, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################\n",
    "### COMPETENCY APPRAISAL\n",
    "################################################################################################################################\n",
    "\n",
    "# compapp = helpers.xlsx_read(file_path=r'../input/Navigate/Comp appraisal.xlsx')\n",
    "\n",
    "# save backup\n",
    "# compapp_backup = open('../working/compapp_backup.pkl','wb')\n",
    "# pickle.dump(compapp, compapp_backup)\n",
    "# compapp_backup.close()\n",
    "# load backup\n",
    "# compapp_backup = open('../working/compapp_backup.pkl', 'rb')\n",
    "# compapp = pickle.load(compapp_backup)\n",
    "# compapp_backup.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################################################################\n",
    "# ### MOBILITY\n",
    "# ################################################################################################################################\n",
    "\n",
    "# mobility = helpers.xlsx_read('../input/Navigate/Mobility.xlsx',\n",
    "#                              cols_to_keep=['employee_global_id', 'mobility'])\n",
    "# mobility = cust_funcs.force_numeric(mobility, cols=['employee_global_id'])\n",
    "# mobility.columns = ['global_id', 'mob']\n",
    "# mobility.dropna(inplace=True)\n",
    "# mobility['global_id'] = mobility['global_id'].astype(int)\n",
    "\n",
    "# mob_dict = {'Mobile Within ABInBev': 5, 'Mobile To Some Zones': 4, 'Mobile Within Zone': 3, \n",
    "#             'Mobile Within Region': 2, 'Mobile Within Country': 1, 'Not Mobile': 0}\n",
    "# mobility['mob'] = mobility['mob'].map(mob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THIS SNIPPET FOR THE XLSB FILES (converted them manually to xlsx because this path is costly/risky)\n",
    "# from pyxlsb import open_workbook\n",
    "# def read_xlsb(path):\n",
    "#     df=[]\n",
    "#     with open_workbook(path) as wb:\n",
    "#         with wb.get_sheet(1) as sheet:\n",
    "#             for row in sheet.rows():\n",
    "#                 df.append([item.v for item in row])\n",
    "#     temp = pd.DataFrame(df[2:], columns=df[1])\n",
    "#     temp.drop(columns=[None], inplace=True)\n",
    "    \n",
    "#     chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'', ':']\n",
    "#     for i in chars_to_remove: temp.columns = temp.columns.str.strip().str.lower().str.replace(i, '_').str.replace('\\_+', '_')\n",
    "    \n",
    "#     cols = [s for s in temp.columns.values if 'date' in s]\n",
    "#     for i in cols:\n",
    "#         temp[col] = pd.to_datetime(temp[i])\n",
    "    \n",
    "#     temp.columns = pd.io.parsers.ParserBase({'names':temp.columns})._maybe_dedup_names(temp.columns)\n",
    "#     return temp\n",
    "\n",
    "# pdp16 = read_xlsb(path='../input/Navigate/PDP/PDP 2016.xlsb')\n",
    "# pdp17 = read_xlsb(path='../input/Navigate/PDP/PDP 2017.xlsb')\n",
    "# pdp18 = helpers.xlsx_read(file_path='../input/Navigate/PDP/PDP 2018.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TIME IN BAND\n",
    "# # load the band change base file\n",
    "# # with adlsfsc.open(path + '/2019/Data/Raw_Data/sharp/movements/Band Change Data-02-05-2019.csv', 'rb') as f:\n",
    "# #     band_changes = helpers.csv_read(f, cols_to_keep=['global_id', 'start_date', 'ps_group'])\n",
    "# #     f.close()\n",
    "\n",
    "# # band_changes['start_date'] = pd.to_datetime(band_changes['start_date'], format='%d-%m-%Y')\n",
    "# # band_changes.sort_values(['global_id', 'start_date'], axis=0, inplace=True, kind='mergesort')\n",
    "# # band_changes.rename(columns={'ps_group':'employee_band'}, inplace=True)\n",
    "# # band_changes.drop_duplicates(subset=['global_id', 'employee_band'], inplace=True, keep='first')\n",
    "# # band_changes.reset_index(inplace=True, drop=True)\n",
    "# # band_changes = helpers.process_columns(df=band_changes, cols=['employee_band'])\n",
    "# # band_changes['employee_band'] = band_changes['employee_band'].map(psg_bands_dict)\n",
    "\n",
    "\n",
    "# # ## save pickle\n",
    "# # with adlsfsc.open(path + '/2019/Data/Raw_Data/pickle_files/Movements/bandchanges.pickle', 'wb') as f:\n",
    "# #     pickle.dump(band_changes, f)\n",
    "# #     f.close()\n",
    "\n",
    "# ## load pickle\n",
    "# with adlsfsc.open(path + '/2019/Data/Raw_Data/pickle_files/Movements/bandchanges.pickle', 'rb') as f:\n",
    "#     band_changes = pickle.load(f)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### old opr train/valid creation\n",
    "\n",
    "\n",
    "# ### split into train and valid\n",
    "# opr_train1 = opr_reshaped.filter(regex='id|14|15|16')\n",
    "# opr_train1['year'] = 2016\n",
    "# opr_train2 = opr_reshaped.filter(regex='id|15|16|17')\n",
    "# opr_train2['year'] = 2017\n",
    "# opr_train1 = opr_train1.reindex(sorted(opr_train1.columns), axis=1)\n",
    "# opr_train2 = opr_train2.reindex(sorted(opr_train2.columns), axis=1)\n",
    "# opr_train1.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'response', 'year']\n",
    "# opr_train2.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'response', 'year']\n",
    "# opr_train = pd.concat([opr_train1, opr_train2], ignore_index=True, axis=0)\n",
    "# opr_valid = opr_reshaped.filter(regex='id|16|17|18')\n",
    "# opr_valid['year'] = 2018\n",
    "# opr_valid.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'response', 'year']\n",
    "# opr_train.dropna(subset=['global_id', 'response'], inplace=True)\n",
    "# opr_valid.dropna(subset=['global_id', 'response'], inplace=True)\n",
    "# opr_train.reset_index(drop=True, inplace=True)\n",
    "# opr_valid.reset_index(drop=True, inplace=True)\n",
    "# opr_train.dropna(subset=['response'], inplace=True)\n",
    "# opr_valid.dropna(subset=['response'], inplace=True)\n",
    "\n",
    "# # prediction opr dataframe\n",
    "# opr_pred = opr_reshaped.filter(regex='id|17|18')\n",
    "# opr_pred['year'] = 2019\n",
    "# opr_pred.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'year']\n",
    "# opr_pred.dropna(subset=['global_id', 'response'], inplace=True)\n",
    "# opr_pred.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################################################################\n",
    "# ### CAREER ASPIRATIONS\n",
    "# ################################################################################################################################\n",
    "\n",
    "# # careerasp = helpers.xlsx_read(file_path=r'../input/Navigate/Career Aspirations.xlsx')\n",
    "\n",
    "# carasp_backup = open('../working/carasp_backup.pkl', 'rb')\n",
    "# careerasp = pickle.load(carasp_backup)\n",
    "# carasp_backup.close()\n",
    "\n",
    "# careerasp.rename(columns={'employee_global_id':'global_id'}, inplace=True)\n",
    "# careerasp = cust_funcs.force_numeric(careerasp, cols=['global_id'])\n",
    "# careerasp = careerasp[['global_id', 'open_to_move_into', 'aspiration___timing', 'aspiration___modified_date']]\n",
    "# careerasp['aspiration___modified_date'] = pd.to_datetime(careerasp['aspiration___modified_date'])\n",
    "# careerasp['stay_flag'] = np.where(careerasp['open_to_move_into']=='Stay In My Function', 1, 0)\n",
    "# careerasp['sales_flag'] = np.where(careerasp['open_to_move_into']=='Sales', 1, 0)\n",
    "# careerasp['supply_flag'] = np.where(careerasp['open_to_move_into']=='Supply', 1, 0)\n",
    "# careerasp['marketing_flag'] = np.where(careerasp['open_to_move_into']=='Marketing', 1, 0)\n",
    "# careerasp['logistics_flag'] = np.where(careerasp['open_to_move_into']=='Logistics', 1, 0)\n",
    "# careerasp['people_flag'] = np.where(careerasp['open_to_move_into']=='People', 1, 0)\n",
    "# careerasp['finance_flag'] = np.where(careerasp['open_to_move_into']=='Finance', 1, 0)\n",
    "\n",
    "# ### split into train and valid\n",
    "# careerasp_train = careerasp[careerasp['aspiration___modified_date']<np.datetime64('2017-07-01')]\n",
    "# careerasp_1 = (careerasp_train[careerasp_train['open_to_move_into']!='Stay In My Function'].\n",
    "#                copy().\n",
    "#                groupby(['global_id', 'aspiration___timing']).\n",
    "#                size().\n",
    "#                to_frame('count_of_aspiration').\n",
    "#                reset_index().\n",
    "#                drop_duplicates().\n",
    "#                pivot(index='global_id', columns='aspiration___timing', values=['count_of_aspiration']).\n",
    "#                reset_index().\n",
    "#                my_flatten_cols().\n",
    "#                fillna(0))\n",
    "# careerasp_train = careerasp_train.merge(careerasp_1, how='left')\n",
    "# careerasp_train = careerasp_train.groupby('global_id').sum().reset_index()\n",
    "# careerasp_valid = careerasp[careerasp['aspiration___modified_date']<np.datetime64('2018-07-01')]\n",
    "# careerasp_2 = (careerasp_valid[careerasp_valid['open_to_move_into']!='Stay In My Function'].\n",
    "#                copy().\n",
    "#                groupby(['global_id', 'aspiration___timing']).\n",
    "#                size().\n",
    "#                to_frame('count_of_aspiration').\n",
    "#                reset_index().\n",
    "#                drop_duplicates().\n",
    "#                pivot(index='global_id', columns='aspiration___timing', values=['count_of_aspiration']).\n",
    "#                reset_index().\n",
    "#                my_flatten_cols().\n",
    "#                fillna(0))\n",
    "# careerasp_valid = careerasp_valid.merge(careerasp_2, how='left')\n",
    "# careerasp_valid = careerasp_valid.groupby('global_id').sum().reset_index()\n",
    "\n",
    "# careerasp_train['year'] = 2017\n",
    "# careerasp_valid['year'] = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mean band change time taken in days ###\n",
    "# band_changes_df = band_changes.copy()\n",
    "# band_changes_df.dropna(subset=['employee_band'], inplace=True)\n",
    "\n",
    "# band_changes_df['flag_2017'] = np.where(band_changes_df['start_date']<=np.datetime64('2017-07-01'), 1, 0)\n",
    "# band_changes_df['flag_2018'] = np.where(band_changes_df['start_date']<=np.datetime64('2018-07-01'), 1, 0)\n",
    "\n",
    "# band_changes_df_train = band_changes_df[band_changes_df['flag_2017']==1]\n",
    "# band_changes_df_valid = band_changes_df[band_changes_df['flag_2018']==1]\n",
    "\n",
    "# band_changes_df_train['time_2017'] = (np.datetime64('2017-07-01')-band_changes_df_train['start_date']).astype('timedelta64[D]')\n",
    "# band_changes_df_valid['time_2018'] = (np.datetime64('2018-07-01')-band_changes_df_valid['start_date']).astype('timedelta64[D]')\n",
    "\n",
    "# # to make train df\n",
    "# band_changes_df_train_agg1 = band_changes_df_train.groupby(['global_id']).size().to_frame('hist_band_changes_count').reset_index()\n",
    "# band_changes_df_train['mean_band_change_time'] = (band_changes_df_train.\n",
    "#                                       groupby(['global_id'])['time_2017'].\n",
    "#                                       transform(lambda x: x.diff().fillna(0)))\n",
    "# mask = band_changes_df_train.groupby(['global_id'])['global_id'].transform(mask_first).astype(bool)\n",
    "# band_changes_df_train_agg2 = band_changes_df_train.loc[mask]\n",
    "# band_changes_df_train_agg3 = (band_changes_df_train_agg2.\n",
    "#                               groupby(['global_id']).\n",
    "#                               agg({'mean_band_change_time':neg_mean}).\n",
    "#                               reset_index())\n",
    "# band_changes_df_train_agg3['year'] = 2017\n",
    "\n",
    "# # to make valid df\n",
    "# band_changes_df_valid_agg1 = band_changes_df_valid.groupby(['global_id']).size().to_frame('hist_band_changes_count').reset_index()\n",
    "# band_changes_df_valid['mean_band_change_time'] = (band_changes_df_valid.\n",
    "#                                       groupby(['global_id'])['time_2018'].\n",
    "#                                       transform(lambda x: x.diff().fillna(0)))\n",
    "# mask = band_changes_df_valid.groupby(['global_id'])['global_id'].transform(mask_first).astype(bool)\n",
    "# band_changes_df_valid_agg2 = band_changes_df_valid.loc[mask]\n",
    "# band_changes_df_valid_agg3 = (band_changes_df_valid_agg2.\n",
    "#                               groupby(['global_id']).\n",
    "#                               agg({'mean_band_change_time':neg_mean}).\n",
    "#                               reset_index())\n",
    "# band_changes_df_valid_agg3['year'] = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy1 = bp_2018.filter(regex='local_entity|analysis_block|inbev', axis=1)\n",
    "# dummy1.drop_duplicates(inplace=True)\n",
    "# dummy2 = bp_2017.filter(regex='local_entity|analysis_block|inbev', axis=1)\n",
    "# dummy2.drop_duplicates(inplace=True)\n",
    "# dummy3 = bp_2016.filter(regex='local_entity|analysis_block|inbev', axis=1)\n",
    "# dummy3.drop_duplicates(inplace=True)\n",
    "\n",
    "# dummy = pd.concat([dummy1.reset_index(drop=True), dummy2.reset_index(drop=True)], axis=0)\n",
    "# #dummy = pd.concat([dummy.reset_index(drop=True), dummy3.reset_index(drop=True)], axis=0)\n",
    "# dummy.drop_duplicates(inplace=True)\n",
    "# dummy.shape\n",
    "\n",
    "# dummy.to_csv('all_entities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## missing value custom\n",
    "\n",
    "# # Create table for missing data analysis\n",
    "# def draw_missing_data_table(df):\n",
    "#     total = df.isnull().sum().sort_values(ascending=False)\n",
    "#     percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "#     missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "#     return missing_data\n",
    "\n",
    "# def df_missing_fill(df, cols, value):\n",
    "#     df[cols].fillna(value=value, inplace=True)\n",
    "#     return df\n",
    "\n",
    "# train = df_missing_fill(train, cols=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### missing value threshold control (for both rows and columns)\n",
    "\n",
    "# # add the response to make it easier for the filtering of the high missing records\n",
    "# train_dummy = train.copy()\n",
    "# train_dummy['response'] = ytrain.reset_index(drop=True)\n",
    "# valid_dummy = valid.copy()\n",
    "# valid_dummy['response'] = yvalid.reset_index(drop=True)\n",
    "\n",
    "# mt = 0.6\n",
    "# print(train_dummy.shape, '\\n')\n",
    "# train_dummy.dropna(thresh=mt*(train_dummy.shape[0]), axis=1, inplace = True)\n",
    "# train_dummy.dropna(thresh=mt*(train_dummy.shape[1]), axis=0, inplace = True)\n",
    "# print(train_dummy.shape, '\\n')\n",
    "# valid_dummy = valid_dummy[train_dummy.columns]\n",
    "# valid_dummy.dropna(thresh=mt*(valid_dummy.shape[0]), axis=1, inplace = True)\n",
    "# train_dummy = train_dummy[valid_dummy.columns]\n",
    "\n",
    "# ytrain_dummy = train_dummy['response'].values\n",
    "# yvalid_dummy = valid_dummy['response'].values\n",
    "\n",
    "# train_dummy.drop(columns=['response'], inplace=True)\n",
    "# valid_dummy.drop(columns=['response'], inplace=True)\n",
    "\n",
    "# #pd.DataFrame(np.array(np.unique(ytrain_dummy, return_counts=True)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## function to make deviation encoding features\n",
    "# def categ_feat_eng(train_df, valid_df, cat_columns, response):\n",
    "#     print('categorical feature engineering is happening ...', '\\n')\n",
    "#     global iter\n",
    "#     iter = 0\n",
    "#     for i in tqdm(cat_columns):\n",
    "#         grouped_df = pd.DataFrame(train_df.groupby([i])[response].agg(['mean', 'count'])).reset_index()\n",
    "#         grouped_df.rename(columns={'mean': str('mean_' + cat_columns[iter]),\n",
    "#                                    'count': str('count_' + cat_columns[iter])}, inplace=True)\n",
    "#         train_df = pd.merge(train_df, grouped_df, how='left')\n",
    "#         valid_df = pd.merge(valid_df, grouped_df, how='left')\n",
    "#         iter += 1\n",
    "#     return train_df, valid_df\n",
    "\n",
    "# train_dummy = train.copy()\n",
    "# train_dummy['response'] = ytrain\n",
    "# valid_dummy = valid.copy()\n",
    "# valid_dummy['response'] = yvalid\n",
    "# #cat_columns = train_dummy.select_dtypes(include=['object']).columns.values\n",
    "# cat_columns = ['employee_band', 'global_job_code', 'macro_entity_l1_code', 'zone', 'country_code']\n",
    "\n",
    "# train_dummy, valid_dummy = categ_feat_eng(train_dummy, valid_dummy, cat_columns=cat_columns, response='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove highly correlated features to reduce further computation time\n",
    "# print('correlation analysis is happening ...', '\\n')\n",
    "# # Create correlation matrix\n",
    "# corr_matrix = train.corr().abs()\n",
    "# # Select upper triangle of correlation matrix\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# # Find index of feature columns with correlation greater than 0.9\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]        \n",
    "# # Drop features\n",
    "# #print(to_drop, '\\n')\n",
    "# train.drop(to_drop, axis=1, inplace=True)\n",
    "# valid.drop(to_drop, axis=1, inplace=True)\n",
    "# print('correlation analysis completed ...', '\\n')\n",
    "# main.cor_dropped_vars = to_drop ## attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## belts old function to derive the time_since_belts\n",
    "\n",
    "# def belts_fn(df, cols, year, string):\n",
    "#     tempdf = df[cols]\n",
    "#     tempdf = tempdf[tempdf[str('flag_'+str(year))]==1]\n",
    "#     tempdf['year'] = year\n",
    "#     tempdf['yearmonth'] = tempdf['year'].astype(str) + string\n",
    "#     tempdf['monthly_file_date'] = pd.to_datetime(tempdf['yearmonth'], format='%Y%m%d')\n",
    "#     tempdf['time_since_belt'] = (tempdf.monthly_file_date - tempdf.date_acquired).astype('timedelta64[D]')\n",
    "#     tempdf = tempdf[['global_id', 'belt', 'time_since_belt']]\n",
    "#     tempdf_new = tempdf.pivot(index='global_id', columns='belt', \n",
    "#                                         values=['time_since_belt']).reset_index().my_flatten_cols()\n",
    "#     tempdf_new.columns.name = None\n",
    "#     tempdf_new['belt_flag'] = 1\n",
    "#     tempdf_new['year'] = year\n",
    "#     return tempdf_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ZONE MODELLING\n",
    "\n",
    "# zone_dict = train_dummy.zone.unique()\n",
    "\n",
    "# #create a data frame dictionary to store your data frames\n",
    "# train_dict = {x: pd.DataFrame for x in zone_dict}\n",
    "# valid_dict = {x: pd.DataFrame for x in zone_dict}\n",
    "\n",
    "# for key in train_dict.keys():\n",
    "#     train_dict[key] = train_dummy[:][train_dummy.zone == key]\n",
    "#     valid_dict[key] = valid_dummy[:][valid_dummy.zone == key]\n",
    "    \n",
    "# class zone_models():\n",
    "    \n",
    "#     def __init__(self, train_dict, valid_dict):\n",
    "#         self.td = train_dict\n",
    "#         self.vd = valid_dict\n",
    "#         self.zones = train_dict.keys()\n",
    "#         self.allzone_results = {}\n",
    "#         self.main()\n",
    "      \n",
    "#     def imp_plot(self, model, train):\n",
    "#         imp = model.feature_importances_\n",
    "#         imp = pd.DataFrame(imp, index=train.columns, columns=['Importance'])\n",
    "#         imp.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "#         imp['Std'] = np.std([tree.feature_importances_\n",
    "#                                 for tree in model.estimators_], axis=0)\n",
    "#         x = range(imp.shape[0])\n",
    "#         y = imp.ix[:, 0]\n",
    "#         yerr = imp.ix[:, 1]\n",
    "#         plt.bar(x, y, yerr=yerr, align=\"center\")\n",
    "#         plt.savefig('importance.png')\n",
    "#         plt.show()\n",
    "#         return imp\n",
    "    \n",
    "#     def main(self):\n",
    "#         for zone, zone_df in self.td.items():\n",
    "#             train_temp = zone_df.copy()\n",
    "#             valid_temp = self.vd[zone].copy()\n",
    "            \n",
    "#             print(zone)\n",
    "#             print(train_temp.shape)\n",
    "#             print(valid_temp.shape)\n",
    "            \n",
    "#             ytrain = train_temp.response\n",
    "#             yvalid = valid_temp.response\n",
    "            \n",
    "#             # drop unnecessary columns\n",
    "#             train_temp.drop(['global_id', 'year', 'response'], inplace=True, axis=1)\n",
    "#             valid_temp.drop(['global_id', 'year', 'response'], inplace=True, axis=1)\n",
    "\n",
    "#             # encoding and other preprocessing\n",
    "#             cat_columns = train_temp.select_dtypes(include=['object']).columns.values\n",
    "\n",
    "#             ## for categorical\n",
    "#             ### split\n",
    "#             train_cat = train_temp[cat_columns]\n",
    "#             valid_cat = valid_temp[cat_columns]\n",
    "#             ### fillna\n",
    "#             train_cat.fillna(value='none', axis=1,  inplace=True)\n",
    "#             valid_cat.fillna(value='none', axis=1,  inplace=True)\n",
    "#             ### encoding\n",
    "#             encoding='oe'\n",
    "#             if encoding in ['be', 'bne', 'he', 'oe', 'ohe']:\n",
    "#                 train_df_cat, valid_df_cat, enc = ce_encodings(train_cat, valid_cat, encoding)\n",
    "#             else :\n",
    "#                 print('Not supported. Use one of [be, bne, he, oe, ohe]', '\\n')\n",
    "\n",
    "#             ## for numerical\n",
    "#             ### split\n",
    "#             num_cols = list(set(train_temp.columns)-set(train_cat.columns))\n",
    "#             train_num = train_temp[num_cols]\n",
    "#             valid_num = valid_temp[num_cols]\n",
    "\n",
    "#             # reset all indices (better safe than sorry)\n",
    "#             train_df_cat.reset_index(drop=True, inplace=True)\n",
    "#             valid_df_cat.reset_index(drop=True, inplace=True)\n",
    "#             train_num.reset_index(drop=True, inplace=True)\n",
    "#             valid_num.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#             ### combine with *_cat dfs\n",
    "#             train_new = pd.concat([train_df_cat, train_num], axis=1)\n",
    "#             valid_new = pd.concat([valid_df_cat, valid_num], axis=1)\n",
    "\n",
    "#             ### missing value treatment\n",
    "#             miss = DataFrameImputer()\n",
    "#             train_temp = train_new.fillna(value=-10)\n",
    "#             valid_temp = valid_new.fillna(value=-10)\n",
    "\n",
    "#             #train_temp, valid_temp = feat_sel.variance_threshold_selector(train=train_temp, valid=valid_temp, threshold=0.1)\n",
    "#             train_new, valid_new = scalers(train_temp, valid_temp, 'ss')\n",
    "\n",
    "#             model = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, class_weight={0:100, 1:50, 2:1, 3:1, 4:50, 5:100}, \n",
    "#                                          random_state=1, max_features=50)\n",
    "#             model.fit(train_new, ytrain)\n",
    "#             print(model.score(valid_new, yvalid))\n",
    "#             print(model.score(train_new, ytrain))\n",
    "\n",
    "#             pred=model.predict(valid_new)\n",
    "#             pred_probs=model.predict_proba(valid_new)\n",
    "\n",
    "#             imp = self.imp_plot(model, train_new)\n",
    "#             print(imp.head(10))\n",
    "#             #imp.to_csv('importance_new.csv', index=True)\n",
    "#             conf = skm.confusion_matrix(y_true=yvalid, y_pred=pred)\n",
    "#             self.allzone_results[zone] = model.score(valid_new, yvalid)\n",
    "#         return None\n",
    "    \n",
    "# dummy = zone_models(train_dict, valid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opr_dict = {5.:'4A', 4.:'4B', 3.:'3A', 2.:'3B', 1.:'1A', 0.:'1B'}\n",
    "\n",
    "# preddf = valid['global_id'].copy()\n",
    "# preddf = pd.concat([preddf, pd.DataFrame(pred1)], ignore_index=True, axis=1)\n",
    "# preddf = pd.concat([preddf, pd.DataFrame(yvalid)], ignore_index=True, axis=1)\n",
    "# preddf = pd.concat([preddf, pd.DataFrame(pred_df1.score)], ignore_index=True, axis=1)\n",
    "# preddf.columns = ['global_id', 'predictions', 'actuals', 'score']\n",
    "# preddf['predictions'] = preddf['predictions'].map(opr_dict)\n",
    "# # preddf['actuals'] = preddf['actuals'].map(opr_dict)\n",
    "# pred_df1 = pd.DataFrame(pred_probs1)\n",
    "# pred_df1['score'] = (pred_df1[0]*1)+(pred_df1[1]*2)+(pred_df1[2]*3)+(pred_df1[3]*4)+(pred_df1[4]*5)+(pred_df1[5]*6)\n",
    "# preddf.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # multi-class classification/regression superclass\n",
    "# class opr_model():\n",
    "    \n",
    "#     \"\"\" Algorithms in each function are as follows:\n",
    "#     1. opr_multiclass_inherant\n",
    "#     2. opr_multiclass_oneVS\n",
    "#     3. opr_regression\n",
    "#     4. opr_ordinal\n",
    "#     5. opr_neuralnets\n",
    "#     6. ...\n",
    "    \n",
    "#     P.S: Not good enough algos commented\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, train_df, valid_df, ytrain_vec, yvalid_vec, algo=0):\n",
    "#         # initialize the arguments\n",
    "#         self.train = train_df\n",
    "#         self.valid = valid_df\n",
    "#         self.ytrain = ytrain_vec\n",
    "#         self.yvalid = yvalid_vec\n",
    "#         self.algo = algo\n",
    "        \n",
    "#         # print stats\n",
    "#         print(train_df.shape, '\\n', valid_df.shape, '\\n', ytrain_vec.shape, '\\n', yvalid_vec.shape)\n",
    "#         print('the class values are:', yvalid_vec.unique(), '\\n')\n",
    "        \n",
    "#         # run the various algorithm functions\n",
    "#         self.opr_multiclass_inherant()\n",
    "#         self.opr_neuralnets()\n",
    "#         self.opr_multiclass_oneVS()\n",
    "#         self.main()\n",
    "#         self.ensemble()\n",
    "        \n",
    "#     def main(self):\n",
    "#         names = ['Logit', 'AdaBoost', 'ExtraTrees', 'LinearSVC', 'Ridge',\n",
    "#                 'GBM', 'OneVsRest', 'OneVsOne', 'OutputCode']\n",
    "#         classifiers = [self.logr, self.adab, self.extt, self.lsvc, self.rdgc,\n",
    "#                       self.rfcf, self.gbmc, self.ovrc, self.ovoc, self.occf]\n",
    "        \n",
    "#         #testnames = ['Logit', 'ExtraTrees', 'MLP']\n",
    "#         #testclassifiers = [self.logr, self.extt, self.mlpc]\n",
    "        \n",
    "#         # iterate over classifiers\n",
    "#         clf_scores = {}\n",
    "#         clf_probs = {}\n",
    "#         clf_predictions = {}\n",
    "#         clf_insample = {}\n",
    "#         for name, clf in zip(names, classifiers):\n",
    "#             print(name, 'is happening \\n')\n",
    "#             clf.fit(self.train.values, self.ytrain)\n",
    "#             clf_scores[name] = clf.score(self.valid.values, self.yvalid)\n",
    "#             print(clf_scores[name], '\\n')\n",
    "#             # predict probs\n",
    "#             if hasattr(clf, \"predict_proba\"):\n",
    "#                 clf_probs[name] = clf.predict_proba(self.valid.values)\n",
    "#                 clf_insample[name] = clf.predict_proba(self.train.values)\n",
    "#             else:\n",
    "#                 clf_probs[name] = clf.predict(self.valid.values)\n",
    "#                 clf_insample[name] = clf.predict(self.train.values)\n",
    "#             # predictions as well\n",
    "#             clf_predictions[name] = clf.predict(self.valid.values)\n",
    "        \n",
    "#         self.scores = clf_scores\n",
    "#         self.probs = clf_probs\n",
    "#         self.predictions = clf_predictions\n",
    "#         self.insample = clf_insample\n",
    "#         return None\n",
    "        \n",
    "#     def opr_multiclass_inherant(self):\n",
    "#         # all sklearn native algorithms\n",
    "#         self.logr = LogisticRegressionCV(random_state=1, multi_class='ovr', max_iter=1000, penalty='l2') #has probs\n",
    "#         self.adab = AdaBoostClassifier(DecisionTreeClassifier(max_depth=12, presort=True), \n",
    "#                                        n_estimators=100, learning_rate=0.1) #has probs\n",
    "#         self.extt = ExtraTreesClassifier(n_estimators=200, max_depth=12, n_jobs=-1) #has probs\n",
    "#         #self.knnc = KNeighborsClassifier(n_neighbors=3, weights='distance') #has probs\n",
    "#         #self.ldac = LinearDiscriminantAnalysis() #has probs\n",
    "#         #self.qdac = QuadraticDiscriminantAnalysis() #has probs\n",
    "#         self.lsvc = LinearSVC(multi_class='ovr', random_state=1) #multiclass='crammer_singer' another setting #no probs\n",
    "#         self.rdgc = RidgeClassifierCV(cv=5) #no probs\n",
    "#         self.rncf = RadiusNeighborsClassifier(n_jobs=-1, radius=2, outlier_label=[2091]) #no probs\n",
    "#         self.rfcf = RandomForestClassifier(n_estimators=200, max_depth=10, n_jobs=-1, random_state=1, \n",
    "#                                        class_weight={0:1, 1:1, 2:1, 3:1, 4:3, 5:3}) #has probs\n",
    "#         #self.nsvc = NuSVC(kernel='linear', nu=0.7, probability=True, class_weight={0:2, 1:1, 2:1, 3:1, 4:2, 5:2}, random_state=1) #has probs\n",
    "#         #self.ksvc = SVC(kernel='poly', probability=True, class_weight={0:1, 1:1, 2:1, 3:1, 4:3, 5:3}, random_state=1) #has probs\n",
    "#         #self.gpcf = GaussianProcessClassifier(random_state=1, multi_class='one_vs_rest', n_jobs=-1) #has probs\n",
    "#         self.gbmc = GradientBoostingClassifier(learning_rate=0.01, max_depth=12, n_estimators=200, subsample=0.8, \n",
    "#                                            max_features=0.8, random_state=1) #has probs\n",
    "#         self.sgdc = SGDClassifier(loss='log', penalty='elasticnet', max_iter=20, n_jobs=-1, early_stopping=True,\n",
    "#                              class_weight={0:2, 1:2, 2:1, 3:1, 4:3, 5:4}) #loss ='modified_huber' #has probs\n",
    "#         return None\n",
    "        \n",
    "#     def opr_multiclass_oneVS(self):\n",
    "#         \"\"\" best algorithms found from the opr_multiclass_inherant will be used as the starting base estimator for the \n",
    "#         methods below. a separate tuning framework to find the best base estimator for oneVS methods will be \n",
    "#         implemented later \"\"\"\n",
    "#         self.ovrc = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=200, max_depth=10, n_jobs=-1), n_jobs=-1) #has probs\n",
    "#         self.ovoc = OneVsOneClassifier(xgb.XGBClassifier(learning_rate=0.01, n_estimators=200, colsample_bytree=0.7, subsample=0.7, \n",
    "#                     scale_pos_weight=2, objective='multi:softmax', max_depth=10, num_class=6)) #no probs\n",
    "#         self.occf = OutputCodeClassifier(ExtraTreesClassifier(n_estimators=200, max_depth=10, n_jobs=-1),\n",
    "#                                          code_size=5, random_state=1) #no probs\n",
    "#         return None\n",
    "        \n",
    "#     def opr_regression(self):\n",
    "#         return None\n",
    "    \n",
    "#     def opr_ordinal(self):\n",
    "#         return None\n",
    "    \n",
    "#     def opr_neuralnets(self):\n",
    "#         ### mlp classifier ###\n",
    "#         self.mlpc = MLPClassifier(hidden_layer_sizes=(10,))\n",
    "#         return None\n",
    "    \n",
    "#     def ensemble(self):\n",
    "#         train_output = self.ytrain.copy()\n",
    "#         valid_output = self.yvalid.copy()\n",
    "#         for k,v in self.insample.items():\n",
    "#             df_insample = pd.DataFrame(self.insample[k])\n",
    "#             df_valid = pd.DataFrame(self.probs[k])\n",
    "#             df_insample.columns = [k+str(i) for i in df_insample.columns.values.tolist()]\n",
    "#             df_valid.columns = [k+str(i) for i in df_valid.columns.values.tolist()]\n",
    "#             train_output = pd.concat([train_output, df_insample], axis=1, ignore_index=False)\n",
    "#             valid_output = pd.concat([valid_output, df_valid], axis=1, ignore_index=False)\n",
    "        \n",
    "#         ens_ytrain = train_output.response.values\n",
    "#         ens_yvalid = valid_output.response.values\n",
    "#         self.ens_train = train_output.drop(['response'], axis=1, inplace=False)\n",
    "#         self.ens_valid = valid_output.drop(['response'], axis=1, inplace=False)\n",
    "        \n",
    "#         ens_model = ExtraTreesClassifier(n_estimators=100, max_depth=5, n_jobs=-1)\n",
    "#         ens_model.fit(self.ens_train, ens_ytrain)\n",
    "#         print('ensemble score is:', ens_model.score(self.ens_valid, ens_yvalid))\n",
    "#         self.ensmod = ens_model\n",
    "#         return None\n",
    "\n",
    "# oprmod = opr_model(train_new, valid_new, ytrain, yvalid)\n",
    "# pred = oprmod.ensmod.predict(oprmod.ens_valid)\n",
    "# sklearn.metrics.confusion_matrix(y_pred=pred, y_true=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAZ ADHOC ##\n",
    "\n",
    "# saz_backup = open('../working/saz_backup.pkl', 'rb')\n",
    "# train_new = pickle.load(saz_backup)\n",
    "# valid_new = pickle.load(saz_backup)\n",
    "# saz_ytrain = pickle.load(saz_backup)\n",
    "# saz_yvalid = pickle.load(saz_backup)\n",
    "# saz_backup.close()\n",
    "\n",
    "# # train['response'] = ytrain\n",
    "# # valid['response'] = yvalid\n",
    "\n",
    "# # saz_train_to = pd.read_excel('../working/2017 in-sample prediction.csv.xlsx')\n",
    "# # saz_train_to['riskcat'] = np.where(saz_train_to['decile'].isin([1,2,3]), 2,\n",
    "# #                                   np.where(saz_train_to['decile'].isin([4,5,6,7]), 1, 0))\n",
    "# # saz_train_to = saz_train_to[['global_id', 'riskcat']]\n",
    "\n",
    "# # saz_valid_to = pd.read_excel('../working/Turnover Prediction for SAZ Test Population.xlsx')\n",
    "# # saz_valid_to['riskcat'] = np.where(saz_valid_to['decile'].isin([1,2,3]), 2,\n",
    "# #                                   np.where(saz_valid_to['decile'].isin([4,5,6,7]), 1, 0))\n",
    "# # saz_valid_to = saz_valid_to[['global_id', 'riskcat']]\n",
    "\n",
    "# # train = pd.merge(train, saz_train_to, how='left')\n",
    "# # valid = pd.merge(valid, saz_valid_to, how='left')\n",
    "\n",
    "# # saz_train = train[train['macro_entity_l2_code']==10002169].copy()\n",
    "# # saz_valid = valid[valid['macro_entity_l2_code']==10002169].copy()\n",
    "# # saz_ytrain = saz_train.response\n",
    "# # saz_yvalid = saz_valid.response\n",
    "# # saz_train.drop(['response', 'global_id', 'year', 'macro_entity_l2_code'], axis=1, inplace=True)\n",
    "# # saz_valid_ids = saz_valid.global_id.reset_index(drop=True).values\n",
    "# # saz_valid.drop(['response', 'global_id', 'year', 'macro_entity_l2_code'], axis=1, inplace=True)\n",
    "# # saz_train.shape, saz_valid.shape\n",
    "\n",
    "# # # encoding and other preprocessing\n",
    "# # cat_columns = saz_train.select_dtypes(include=['object']).columns.values\n",
    "\n",
    "# # ## for categorical\n",
    "# # ### split\n",
    "# # train_cat = saz_train[cat_columns]\n",
    "# # valid_cat = saz_valid[cat_columns]\n",
    "# # ### fillna\n",
    "# # train_cat.fillna(value='none', axis=1,  inplace=True)\n",
    "# # valid_cat.fillna(value='none', axis=1,  inplace=True)\n",
    "# # ### encoding\n",
    "# # encoding='oe'\n",
    "# # if encoding in ['be', 'bne', 'he', 'oe', 'ohe']:\n",
    "# #     train_df_cat, valid_df_cat, enc = ce_encodings(train_cat, valid_cat, encoding)\n",
    "# # else :\n",
    "# #     print('Not supported. Use one of [be, bne, he, oe, ohe]', '\\n')\n",
    "\n",
    "# # ## for numerical\n",
    "# # ### split\n",
    "# # num_cols = list(set(saz_train.columns)-set(train_cat.columns))\n",
    "# # train_num = saz_train[num_cols]\n",
    "# # valid_num = saz_valid[num_cols]\n",
    "\n",
    "# # # reset all indices (better safe than sorry)\n",
    "# # train_df_cat.reset_index(drop=True, inplace=True)\n",
    "# # valid_df_cat.reset_index(drop=True, inplace=True)\n",
    "# # train_num.reset_index(drop=True, inplace=True)\n",
    "# # valid_num.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # ### combine with *_cat dfs\n",
    "# # train_new = pd.concat([train_df_cat, train_num], axis=1)\n",
    "# # valid_new = pd.concat([valid_df_cat, valid_num], axis=1)\n",
    "\n",
    "# # ### missing value treatment\n",
    "# # miss = DataFrameImputer()\n",
    "# # saz_train = train_new.fillna(value=-1)\n",
    "# # saz_valid = valid_new.fillna(value=-1)\n",
    "\n",
    "# # saz_train.shape, saz_valid.shape\n",
    "# # saz_train, saz_valid = feat_sel.variance_threshold_selector(train=saz_train, valid=saz_valid, threshold=0.01)\n",
    "# # saz_train.shape, saz_valid.shape\n",
    "\n",
    "# # train_new, valid_new = scalers(saz_train, saz_valid, 'ss')\n",
    "\n",
    "\n",
    "# # import pickle\n",
    "# # # save backup\n",
    "# # saz_backup = open('../working/saz_backup.pkl','wb')\n",
    "# # pickle.dump(train_new, saz_backup)\n",
    "# # pickle.dump(valid_new, saz_backup)\n",
    "# # pickle.dump(saz_ytrain, saz_backup)\n",
    "# # pickle.dump(saz_yvalid, saz_backup)\n",
    "# # saz_backup.close()\n",
    "# # load backup\n",
    "# saz_backup = open('../working/saz_backup.pkl', 'rb')\n",
    "# train_new = pickle.load(saz_backup)\n",
    "# valid_new = pickle.load(saz_backup)\n",
    "# saz_ytrain = pickle.load(saz_backup)\n",
    "# saz_yvalid = pickle.load(saz_backup)\n",
    "# saz_backup.close()\n",
    "\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# mod1 = ExtraTreesClassifier(n_estimators=200, max_depth=10, n_jobs=-1, max_features=45, \n",
    "#                             class_weight={0:3, 1:2, 2:0.5, 3:1, 4:2, 5:3}, random_state=1)\n",
    "# #mod1 = ExtraTreesClassifier(n_estimators=100, max_depth=20, n_jobs=-1, max_features=50, random_state=1, bootstrap=True)\n",
    "\n",
    "# mod1.fit(train_new, saz_ytrain)\n",
    "# mod1.score(valid_new, saz_yvalid)\n",
    "# mod1.score(train_new, saz_ytrain)\n",
    "# pred1=mod1.predict(valid_new)\n",
    "# pred_probs1=mod1.predict_proba(valid_new)\n",
    "\n",
    "# #pred1_df = pd.DataFrame({'global_id':saz_valid_ids, 'saz_modpred_class':pred1})\n",
    "# #pred1_df.to_csv('saz_pred_aftermod.csv', index=False)\n",
    "\n",
    "# ## plot the importance matrix\n",
    "# %matplotlib inline\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# importance = mod1.feature_importances_\n",
    "# importance = pd.DataFrame(importance, index=train_new.columns, \n",
    "#                           columns=[\"Importance\"])\n",
    "# importance.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# importance[\"Std\"] = np.std([tree.feature_importances_ for tree in mod1.estimators_], axis=0)\n",
    "# x = range(importance.shape[0])\n",
    "# y = importance.ix[:, 0]\n",
    "# yerr = importance.ix[:, 1]\n",
    "# plt.bar(x, y, yerr=yerr, align=\"center\")\n",
    "# plt.savefig('importance.png')\n",
    "# plt.show()\n",
    "\n",
    "# importance[0:10]\n",
    "# importance.to_csv('importance.csv', index=True)\n",
    "\n",
    "# rev_dep_dict = {5.:'4A', 4.:'4B', 3.:'3A', 2.:'3B', 1.:'1A', 0.:'1B'}\n",
    "\n",
    "# saz_to_file = pd.read_excel('../working/Turnover Prediction for SAZ Test Population.xlsx')\n",
    "# saz_to_file = saz_to_file[['global_id', 'decile']]\n",
    "# saz_complete = pd.merge(pred1_df, saz_to_file, how='left')\n",
    "# saz_complete['riskcat'] = np.where(saz_complete['decile'].isin([1,2,3]), '2_high',\n",
    "#                                   np.where(saz_complete['decile'].isin([4,5,6,7]), '1_medium', '0_low'))\n",
    "# saz_complete['saz_modpred_class'] = saz_complete['saz_modpred_class'].map(rev_dep_dict)\n",
    "# saz_complete['oprcat'] = np.where(saz_complete['saz_modpred_class'].isin(['1A', '1B']), '0_low',\n",
    "#                                  np.where(saz_complete['saz_modpred_class'].isin(['3A', '3B']), '1_medium', '2_high'))\n",
    "# saz_complete = saz_complete[['global_id', 'oprcat', 'riskcat']]\n",
    "\n",
    "# df = pd.crosstab(saz_complete.oprcat, saz_complete.riskcat)\n",
    "# idx = df.columns.union(df.index)\n",
    "# df = df.reindex(index = idx, columns=idx, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## shape module\n",
    "\n",
    "# class opr_shape():\n",
    "#     \"\"\" precedence => 4A -> 4B -> 1B -> 1A -> 3A -> 3B (5 > 4 > 0 > 1 > 3 > 2) \"\"\"\n",
    "    \n",
    "#     def __init__(self, probs, ytrain):\n",
    "#         self.probs = probs\n",
    "#         self.ytrain = ytrain\n",
    "#         self.shapevec()\n",
    "#         self.main()\n",
    "#         self.finaldf.to_csv('finaldf.csv', index=False)\n",
    "        \n",
    "#     def shapevec(self):\n",
    "#         shape_vec = pd.DataFrame(self.ytrain.value_counts()/self.ytrain.shape[0])\n",
    "#         shape_vec['class'] = shape_vec.index\n",
    "#         shape_vec.sort_values('class', inplace=True)\n",
    "#         #shape_vec['response'] = shape_vec.response.cumsum()\n",
    "#         #print('the shape vector is: ', shape_vec)\n",
    "#         self.shape_vec = shape_vec\n",
    "#         return None\n",
    "\n",
    "#     def main(self):\n",
    "#         self.probs_df = {}\n",
    "#         for i in range(self.probs.shape[1]):\n",
    "#             self.probs_df[i] = pd.DataFrame({str('prob_'+str(i)): self.probs[:, i]})\n",
    "#             self.probs_df[i]['true_index'] = self.probs_df[i].index\n",
    "#             self.probs_df[i].sort_values(by=[str('prob_'+str(i))], inplace=True, kind='mergesort', ascending=False)\n",
    "#             self.probs_df[i].reset_index(inplace=True, drop=True)\n",
    "#             self.probs_df[i][str(str(i)+'_new_index')] = self.probs_df[i].index\n",
    "#             self.probs_df[i][str(str(i)+'_index_perc')] = self.probs_df[i][str(str(i)+'_new_index')].rank(pct=True)\n",
    "#             self.probs_df[i][str(str(i)+'_flag')] = np.where(self.probs_df[i][str(str(i)+'_index_perc')]<self.shape_vec.iloc[i, 0], 1, 0)\n",
    "            \n",
    "#         merge = functools.partial(pd.merge, left_index=False, right_index=False, how='inner', on='true_index')\n",
    "#         finaldf = functools.reduce(merge, self.probs_df.values())\n",
    "        \n",
    "#         finaldf['class'] = np.where(finaldf['5_flag']==1, 5, np.where(finaldf['4_flag']==1, 4, np.where(finaldf['0_flag']==1, 0, \n",
    "#                             np.where(finaldf['1_flag']==1, 1, np.where(finaldf['2_flag']==1, 2, 3)))))\n",
    "#         self.finaldf = finaldf\n",
    "#         return None\n",
    "\n",
    "# oprs = opr_shape(probs=pred_probs1, ytrain=yvalid)\n",
    "# shaped_df = oprs.finaldf[['true_index', 'class']]\n",
    "# shaped_df.set_index('true_index', inplace=True)\n",
    "# shaped_df.sort_index(inplace=True)\n",
    "# shaped_df.reset_index(drop=True, inplace=True)\n",
    "# sklearn.metrics.accuracy_score(y_true=yvalid, y_pred=shaped_df['class'])\n",
    "# sklearn.metrics.confusion_matrix(y_true=yvalid, y_pred=shaped_df['class'])\n",
    "# shaped_df.columns = ['shape_class']\n",
    "# shaped_df = pd.concat([shaped_df.reset_index(drop=True), validids], axis=1)\n",
    "# #shaped_df.to_csv('pred_aftershape.csv', index=False)\n",
    "\n",
    "# pred_df1 = pd.DataFrame(pred_probs1)\n",
    "# pred_df1['score'] = (pred_df1[0]*1)+(pred_df1[1]*2)+(pred_df1[2]*3)+(pred_df1[3]*4)+(pred_df1[4]*5)+(pred_df1[5]*6)\n",
    "# pred_df1['true_index'] = pred_df1.index\n",
    "# pred_df1.sort_values('score', inplace=True)\n",
    "# pred_df1.reset_index(inplace=True, drop=True)\n",
    "# pred_df1=pred_df1[['score', 'true_index']]\n",
    "# pred_df1['new_index'] = (pred_df1.index+1)/pred_df1.shape[0]\n",
    "# pred_df1['class'] = np.where(pred_df1['new_index']<0.031877, 0, \n",
    "#                              (np.where(pred_df1['new_index']<0.100751, 1, \n",
    "#                                        (np.where(pred_df1['new_index']<0.582359, 2, \n",
    "#                                                  (np.where(pred_df1['new_index']<0.872194, 3, \n",
    "#                                                            (np.where(pred_df1['new_index']<0.970513, 4, 5)))))))))\n",
    "# dummy_df = pred_df1.copy()\n",
    "# dummy_df = dummy_df[['true_index', 'class']]\n",
    "# dummy_df.set_index(keys='true_index', inplace=True)\n",
    "# dummy_df.sort_index(inplace=True)\n",
    "# dummy_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# x1=dummy_df['class']\n",
    "# sklearn.metrics.accuracy_score(y_true=yvalid, y_pred=x1)\n",
    "# sklearn.metrics.confusion_matrix(y_pred=x1, y_true=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## custom validation class\n",
    "\n",
    "# class validation():\n",
    "    \n",
    "#     def __init__(self, actual, pred, probs, df, name):\n",
    "#         self.actual = actual\n",
    "#         self.pred = pred\n",
    "#         self.predprobs = pd.DataFrame(probs)\n",
    "#         self.pop = len(actual)\n",
    "#         self.df = df\n",
    "#         self.dfname = name\n",
    "        \n",
    "#         self.score_dict = {'actuals': 0, \n",
    "#                            'pred':  0, \n",
    "#                            'dev1': 0.75, \n",
    "#                            'dev2': 0, \n",
    "#                            'dev3': -0.5, \n",
    "#                            'dev4': -0.75, \n",
    "#                            'dev5': -1, \n",
    "#                            'nodev': 1}\n",
    "#         self.main()\n",
    "#         self.temp_withprobs.to_csv(str(self.dfname + 'prediction_probs.csv'), index=False)\n",
    "        \n",
    "#     def main(self):\n",
    "#         temp = pd.DataFrame({'actuals': self.actual, 'pred': self.pred})\n",
    "#         self.temp_withprobs = pd.concat([temp.reset_index(drop=True), pd.DataFrame(self.predprobs)], axis=1)\n",
    "#         self.temp_withprobs = pd.concat([self.df.reset_index(drop=True), self.temp_withprobs], axis=1)\n",
    "#         temp['dev1'] = np.where(abs(temp['actuals']-temp['pred'])==1.0, 1, 0)\n",
    "#         temp['dev2'] = np.where(abs(temp['actuals']-temp['pred'])==2.0, 1, 0)\n",
    "#         temp['dev3'] = np.where(abs(temp['actuals']-temp['pred'])==3.0, 1, 0)\n",
    "#         temp['dev4'] = np.where(abs(temp['actuals']-temp['pred'])==4.0, 1, 0)\n",
    "#         temp['dev5'] = np.where(abs(temp['actuals']-temp['pred'])==5.0, 1, 0)\n",
    "#         temp['nodev'] = np.where(abs(temp['actuals']-temp['pred'])==0.0, 1, 0)\n",
    "#         self.fulltable = temp\n",
    "        \n",
    "#         temp_df = (pd.DataFrame({'sum': temp.sum()})).reset_index()\n",
    "#         temp_df['weights'] = temp_df['index'].map(self.score_dict)\n",
    "#         temp_df['score'] = temp_df['sum']*temp_df['weights']\n",
    "#         temp_df = temp_df[~temp_df['index'].isin(['actuals', 'pred'])].reset_index(drop=True)\n",
    "#         temp_df['perc'] = temp_df['sum']/self.pop\n",
    "        \n",
    "#         self.table = temp_df\n",
    "#         self.score = temp_df.score.sum()/self.pop\n",
    "#         self.strict_classrate = temp_df[temp_df['index']=='nodev'].perc.sum()\n",
    "#         self.flex_classrate = temp_df[temp_df['index'].isin(['nodev', 'dev1'])].perc.sum()\n",
    "#         return print(self.score)\n",
    "\n",
    "# val=validation(yvalid, pred1, pred_probs1, valid_new, 'valid')\n",
    "# 'the strict classification rate is: ', val.strict_classrate\n",
    "# 'the relaxed classification rate is: ', val.flex_classrate\n",
    "# val.table\n",
    "# df1 = val.fulltable.copy()\n",
    "# df1_summ = df1.groupby(['actuals']).agg({'nodev':'sum', 'dev1':'sum', 'dev2':'sum', 'dev3':'sum', 'dev4':'sum', 'dev5':'sum'})\n",
    "# df1_summ.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## competency comments language detection\n",
    "\n",
    "# from langdetect import detect, detect_langs\n",
    "\n",
    "# compapp_backup = open('../working/compapp_backup.pkl', 'rb')\n",
    "# compapp = pickle.load(compapp_backup)\n",
    "# compapp_backup.close()\n",
    "\n",
    "# dummy = compapp[['employee_global_id', 'manager_comments']]\n",
    "# dummy.drop_duplicates(inplace=True)\n",
    "# dummy.dropna(inplace=True)\n",
    "# dummy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# dummy['manager_comments'] = dummy['manager_comments'].str.lower()\n",
    "# dummy['manager_comments'] = dummy['manager_comments'].str.strip()\n",
    "\n",
    "# dummy['manager_comments'] = dummy['manager_comments'].str.replace('\\d+', '')\n",
    "# dummy.dropna(inplace=True)\n",
    "# dummy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# p = re.compile(r'[^\\w\\s]+')\n",
    "# dummy['manager_comments'] = [p.sub('', x) for x in dummy['manager_comments'].tolist()]\n",
    "# dummy.dropna(inplace=True)\n",
    "# dummy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# dummy.to_csv('compcomments.csv')\n",
    "# dummy.manager_comments[4]\n",
    "# detect_langs(dummy.manager_comments[4])\n",
    "\n",
    "# for index, row in dummy['manager_comments'].iteritems():\n",
    "#     try:\n",
    "#         lang = detect(row)\n",
    "#         dummy.loc[index, 'lang'] = lang\n",
    "#     except:\n",
    "#         dummy.loc[index, 'lang'] = 'error'\n",
    "#         pass\n",
    "\n",
    "# dummy['lang'] = dummy['manager_comments'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rem_cols = ['drgr',\n",
    "# 'drplus1gr',\n",
    "# 'fsgr',\n",
    "# 'lmanddrlevelgr',\n",
    "# 'lmgr',\n",
    "# 'slmandlmlevelgr',\n",
    "#             #####\n",
    "# 'emp_age_asof_current',\n",
    "# 'emp_mngr_dob_diff',\n",
    "# 'emp_mngr_tenure_diff',\n",
    "# 'emp_tenure_asof_current',\n",
    "# 'mngr_age_asof_current',\n",
    "# 'mngr_tenure_asof_current',\n",
    "#             #####\n",
    "# 'analysis_block_id',\n",
    "# 'analysis_block_l1_code',\n",
    "# 'analysis_block_l2_code',\n",
    "# 'analysis_block_l3_code',\n",
    "# 'analysis_block_l4_code',\n",
    "# # 'analysis_block_l5_code',\n",
    "# # 'analysis_block_l6_code',\n",
    "#             ###\n",
    "# 'ab_inbev_entity_id',\n",
    "# # 'inbev_entity_l1',\n",
    "# # 'inbev_entity_l2_code',\n",
    "# # 'inbev_entity_l3_code',\n",
    "# # 'inbev_entity_l4_code',\n",
    "#             ###\n",
    "# 'local_entity_id',\n",
    "# 'local_entity_l1_code',\n",
    "# 'local_entity_l2_code',\n",
    "# 'local_entity_l3_code',\n",
    "# 'local_entity_l4_code',\n",
    "# # 'local_entity_l5_code',\n",
    "# # 'local_entity_l6_code',\n",
    "#             ###\n",
    "# 'macro_entity_id',\n",
    "# 'macro_entity_l1_code',\n",
    "# 'macro_entity_l3_code',\n",
    "# 'macro_entity_l4_code',\n",
    "# # 'macro_entity_l5_code',\n",
    "# # 'macro_entity_l6_code',\n",
    "#             #####\n",
    "# # 'cost_center',\n",
    "# 'country_grouping_p_',\n",
    "# 'company_code',\n",
    "# 'pay_scale_area_code',\n",
    "# 'pay_scale_level',\n",
    "# 'pay_scale_type',\n",
    "#             #####\n",
    "# 'direct_manager_emp_id',\n",
    "# # 'direct_manager_position_desc',\n",
    "# # 'direct_manager_position_id',\n",
    "# # 'hierarchy_manager_emp_id',\n",
    "# # 'parent_org_unit_manager_personnel_no',\n",
    "# # 'organizational_unit',\n",
    "#             #####\n",
    "# 'job_family_code',\n",
    "# 'functional_area_id',\n",
    "# 'global_job_code',\n",
    "# 'personnel_area',\n",
    "# 'personnel_subarea',\n",
    "# 'physical_work_location_code',\n",
    "# 'position_id',\n",
    "# 'position_start_date_om_dayofyear',\n",
    "# 'position_title',\n",
    "# 'talent_type']\n",
    "\n",
    "# train_new = train_0to5.drop(columns=rem_cols)\n",
    "# valid_new = valid_0to5.drop(columns=rem_cols)\n",
    "\n",
    "# train_new.shape, ytrain_0to5.shape\n",
    "# valid_new.shape, yvalid_0to5.shape\n",
    "\n",
    "# train_newnew = train.drop(columns=rem_cols)\n",
    "# valid_newnew = valid.drop(columns=rem_cols)\n",
    "\n",
    "# train_newnew.shape, ytrain.shape\n",
    "# valid_newnew.shape, yvalid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating manager level features\n",
    "\n",
    "# def mngr_feats(df, cols):\n",
    "#     dftemp = df[cols].copy()\n",
    "#     ids = df.direct_manager_emp_id.unique()\n",
    "#     dftemp = dftemp[dftemp['global_id'].isin(ids)]\n",
    "#     dftemp = dftemp.add_prefix('manager_')\n",
    "#     dftemp.rename(columns={'manager_global_id':'direct_manager_emp_id', 'manager_year':'year'}, inplace=True)\n",
    "#     print('old shape: ', df.shape)\n",
    "#     df = df.merge(dftemp, on=['direct_manager_emp_id', 'year'], how='left')\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     print('new shape: ', df.shape)\n",
    "#     return df\n",
    "\n",
    "# mngr_cols = ['global_id', 'opr_prev', 'year', 'time_in_band']\n",
    "# train = mngr_feats(train, cols=mngr_cols)\n",
    "# valid = mngr_feats(valid, cols=mngr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def groupby_count(df, grp_cols, count_col):\n",
    "#     for i in grp_cols:\n",
    "#         colname = str('count_' + count_col + '_' + i)\n",
    "#         grp_cols_full = [i, 'year']\n",
    "#         dftemp = df.groupby(grp_cols_full)[count_col].nunique().to_frame(name=colname).reset_index()\n",
    "#         dftemp.sort_values(grp_cols_full, inplace=True)\n",
    "#         dftemp.reset_index(inplace=True, drop=True)\n",
    "#         diff_colname = str('countdiff_' + count_col + '_' + i)\n",
    "#         dftemp[diff_colname] = dftemp.groupby(i)[colname].diff().fillna(0)\n",
    "#         df = df.merge(dftemp, how='left')\n",
    "        \n",
    "#         means_stds = df.groupby(grp_cols_full)[colname].agg(['mean','std']).reset_index()\n",
    "#         df = df.merge(means_stds, on=grp_cols_full, how='left')\n",
    "#         df[colname] = (df[colname] - df['mean']) / df['std']\n",
    "#         df.drop(columns=['mean', 'std'], inplace=True)\n",
    "#     return df\n",
    "\n",
    "# # count features (get count of employees for each level from train and valid)\n",
    "# # combining the two to create the features and then splitting again\n",
    "# train_0to5['response'] = ytrain_0to5\n",
    "# valid_0to5['response'] = yvalid_0to5\n",
    "\n",
    "# ads_full = pd.concat([train_0to5, valid_0to5], axis=0)\n",
    "\n",
    "# # TEAM FEATURES\n",
    "# ads_full = groupby_count(df=ads_full.copy(),\n",
    "#                             grp_cols=['direct_manager_emp_id'],\n",
    "#                             count_col = 'global_id')\n",
    "\n",
    "# train_0to5.reset_index(drop=True, inplace=True)\n",
    "# valid_0to5.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# ytrain_0to5 = np.array(train_0to5.response)\n",
    "# yvalid_0to5 = np.array(valid_0to5.response)\n",
    "\n",
    "# train_0to5.drop(columns=['response'], inplace=True)\n",
    "# valid_0to5.drop(columns=['response'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # index features \n",
    "# def index_feats(df1, df2, grp_cols, ind_cols):\n",
    "#     for i in grp_cols:\n",
    "#         for j in ind_cols:\n",
    "#             colname = str('Rank of individuals based on ' + i + ' across ' + j)\n",
    "#             df1[colname] = df1.groupby(i)[j].rank(ascending=False, method='dense')\n",
    "#             df2[colname] = df2.groupby(i)[j].rank(ascending=False, method='dense')\n",
    "            \n",
    "#             means_stds = df1.groupby(i)[colname].agg(['mean','std']).reset_index()\n",
    "#             df1 = df1.merge(means_stds, on=i, how='left')\n",
    "#             df1[colname] = (df1[colname] - df1['mean']) / df1['std']\n",
    "#             df1.drop(columns=['mean', 'std'], inplace=True)\n",
    "            \n",
    "#             means_stds = df2.groupby(i)[colname].agg(['mean','std']).reset_index()\n",
    "#             df2 = df2.merge(means_stds, on=i, how='left')\n",
    "#             df2[colname] = (df2[colname] - df2['mean']) / df2['std']\n",
    "#             df2.drop(columns=['mean', 'std'], inplace=True)\n",
    "#     return df1, df2\n",
    "\n",
    "# # train, valid = index_feats(df1=train.copy(), df2=valid.copy(), \n",
    "# #                     grp_cols=['local_entity_l1_code', 'opr_prev', 'macro_entity_l1_code', 'local_entity_l2_code', \n",
    "# #                               'inbev_entity_l1', 'global_job_code', 'analysis_block_l1_code', 'zone'],\n",
    "# #                    ind_cols=['mr_pers_year_comp_score_mean', 'time_in_band', 'position_tenure', 'mrs', 'compare_ratio',\n",
    "# #                             'net_target', 'mod_salary', 'mob'])\n",
    "\n",
    "# train_0to5, valid_0to5 = index_feats(df1=train_0to5.copy(), df2=valid_0to5.copy(), \n",
    "#                     grp_cols=['local_entity_l1_code', 'opr_prev', 'macro_entity_l1_code', 'local_entity_l2_code', \n",
    "#                               'global_job_code', 'analysis_block_l1_code', 'zone'],\n",
    "#                    ind_cols=['mr_pers_year_comp_score_mean', 'compare_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### OLDER TARGET SCRIPT\n",
    "\n",
    "# ## target script\n",
    "\n",
    "# def target_year_fn(filepath, cols, year):\n",
    "#     df = helpers.xlsx_read(file_path=filepath)\n",
    "#     df.drop_duplicates(inplace=True)\n",
    "#     df = df[cols]\n",
    "#     df.columns = ['global_id', 'timerate', 'target']\n",
    "#     df = cust_funcs.force_numeric(df)\n",
    "#     df.dropna(inplace=True)\n",
    "#     df['target'] = np.where(df['target']>100, 100, df['target'])\n",
    "#     df.drop_duplicates(inplace=True, keep='last', subset=['global_id', 'timerate'])\n",
    "#     dfnew = df[(df['timerate']>0) & (df['target']>0)].copy()\n",
    "#     dfnew.reset_index(drop=True, inplace=True)\n",
    "#     dfnew['timerate'] = np.where(dfnew['timerate']==0, 0.5, dfnew['timerate'])\n",
    "#     dfnew['sum_tr'] = dfnew.groupby(['global_id'])['timerate'].transform(np.sum)\n",
    "#     dfnew['sum_nt'] = dfnew.groupby(['global_id'])['target'].transform(np.sum)\n",
    "#     dfsize = dfnew.groupby('global_id').size().to_frame('size').reset_index()\n",
    "#     dfnew = dfnew.merge(dfsize, on='global_id', how='left')\n",
    "#     dfnew['target_final'] = np.where(dfnew['size']==1, \n",
    "#                                      dfnew['target'],\n",
    "#                                      dfnew['target']*dfnew['timerate'])\n",
    "#     dfnew['target_year'] = year\n",
    "#     dfnew['target_final_agg'] = dfnew.groupby(['global_id'])['target_final'].transform(np.sum)\n",
    "    \n",
    "#     # just to ensure no exception cases went through\n",
    "#     dfnew['target_final_agg'] = np.where(dfnew['target_final_agg']>100, \n",
    "#                                  100, \n",
    "#                                  np.where(dfnew['target_final_agg']<0, 0, dfnew['target_final_agg']))\n",
    "#     return dfnew[['global_id', 'target_final_agg', 'target_year']]\n",
    "\n",
    "# # read in the base files for target - 2016 and 2017\n",
    "# tar16 = target_year_fn('../input/TARGET/target_2016_new.xlsx',\n",
    "#                        cols=['employee_global_id', 'time_dedication_rate', 'target_value'],\n",
    "#                       year=2016)\n",
    "# tar17 = target_year_fn('../input/TARGET/target_2017_new.xlsx',\n",
    "#                        cols=['employee_global_id', 'time_dedication_rate', 'target_value'],\n",
    "#                       year=2017)\n",
    "# tar1617 = pd.concat([tar16.reset_index(drop=True), tar17.reset_index(drop=True)], axis=0)\n",
    "\n",
    "# ## read in the target files\n",
    "# tar_cols_to_keep = ['globalid', 'ta_2014', 'ta_2015', 'ta_2016', 'ta_2017']\n",
    "# tar_full = helpers.csv_read(file_path='../input/TARGET/TA_14_17_Consolidated_Edited.csv', cols_to_keep=tar_cols_to_keep)\n",
    "# tar_full.columns = ['global_id', '2014', '2015', '2016', '2017']\n",
    "# tar_full = tar_full[pd.to_numeric(tar_full['global_id'], errors='coerce').notnull()]\n",
    "# tar_full['global_id'] = tar_full['global_id'].astype(int)\n",
    "# tar_full.set_index('global_id', inplace=True)\n",
    "# tar_reshaped = tar_full.unstack().reset_index()\n",
    "# tar_reshaped.columns = ['target_year', 'global_id', 'net_target']\n",
    "# tar_reshaped['target_year'] = tar_reshaped['target_year'].astype(int)\n",
    "# tar_reshaped = tar_reshaped[pd.to_numeric(tar_reshaped['global_id'], errors='coerce').notnull()]\n",
    "# tar_reshaped.dropna(subset=['global_id'], inplace=True)\n",
    "\n",
    "# tar1617['global_id'] = tar1617['global_id'].astype(int)\n",
    "# tar = tar1617.merge(tar_reshaped, how='outer', on=['global_id', 'target_year'])\n",
    "# tar['net_target'] = np.where(tar['target_final_agg'].isna(), tar['net_target'], tar['target_final_agg'])\n",
    "\n",
    "# # just to ensure no exception cases went through\n",
    "# tar['net_target'] = np.where(tar['net_target']>100, \n",
    "#                              100, \n",
    "#                              np.where(tar['net_target']<0, 0, tar['net_target']))\n",
    "# tar = cust_funcs.group_cummean_feats(tar, group_cols=['global_id'], \n",
    "#                              cummean_col='net_target', sort_col='target_year')\n",
    "# tar = tar[['global_id', 'target_year', 'net_target', 'cummean_global_id_net_target']]\n",
    "# tar.drop_duplicates(subset=['global_id', 'target_year'], inplace=True, keep='last')\n",
    "# tar.dropna(subset=['global_id', 'net_target'], inplace=True, how='any')\n",
    "\n",
    "# # save backup\n",
    "# tarpickle = open('../working/tar_backup.pkl','wb')\n",
    "# pickle.dump(tar, tarpickle)\n",
    "# tarpickle.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
