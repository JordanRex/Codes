{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL OPR NLP MODEL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder files:  ['BP', 'career_velocity', 'COMP', 'Engagement', 'MOVEMENT', 'Navigate', 'OPR', 'org_chart_feats', 'PDI', 'Rule_based_module_output', 'SCRIPTS', 'TARGET'] \n",
      "\n",
      "envir variables: \n",
      "InteractiveShell\t collections\t np\t os\t pd\t randomseed\t re\t skm\t warnings\t \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "import collections # for the Counter function\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "randomseed = 1 # the value for the random state used at various points in the pipeline\n",
    "pd.options.display.max_rows = 10 # specify if you want the full output in cells rather the truncated list\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "# to display multiple outputs in a cell without usin print/display\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# ignore warnings (only if you are the kind that would code when the world is burning)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# display wd files\n",
    "import os as os\n",
    "print('folder files: ', os.listdir('../input/'), '\\n')\n",
    "print('envir variables: ')\n",
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS CLASS #\n",
    "\n",
    "# global function to flatten columns after a grouped operation and aggregation\n",
    "# outside all classes since it is added as an attribute to pandas DataFrames\n",
    "def __my_flatten_cols(self, how=\"_\".join, reset_index=True):\n",
    "    how = (lambda iter: list(iter)[-1]) if how == \"last\" else how\n",
    "    self.columns = [how(filter(None, map(str, levels))) for levels in self.columns.values] \\\n",
    "    if isinstance(self.columns, pd.MultiIndex) else self.columns\n",
    "    return self.reset_index(drop=True) if reset_index else self\n",
    "pd.DataFrame.my_flatten_cols = __my_flatten_cols\n",
    "\n",
    "class helper_funcs():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" list down the various functions defined here \"\"\"\n",
    "    \n",
    "    def csv_read(self, file_path, cols_to_keep=None, dtype=None, drop_dup=None):\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        if dtype is None:\n",
    "            x=pd.read_csv(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], encoding='latin-1', low_memory=False)\n",
    "        else:\n",
    "            x=pd.read_csv(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], encoding='latin-1', low_memory=False, dtype=dtype)\n",
    "        chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'', ':']\n",
    "        for i in chars_to_remove:\n",
    "            x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "        if cols_to_keep is not None: x = x[cols_to_keep]\n",
    "        if drop_dup is not None: x.drop_duplicates(inplace=True)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def txt_read(self, file_path, cols_to_keep=None, sep='|', skiprows=1, dtype=None, drop_dup=None):\n",
    "        # currently only supports salary files with the default values (need to implement dynamic programming for any generic txt)\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        if dtype is None:\n",
    "            x=pd.read_table(file_path, sep=sep, skiprows=skiprows, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'])\n",
    "        else:\n",
    "            x=pd.read_table(file_path, sep=sep, skiprows=skiprows, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], dtype=dtype)\n",
    "        chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'', ':']\n",
    "        for i in chars_to_remove:\n",
    "            x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "        if cols_to_keep is not None: x = x[cols_to_keep]\n",
    "        if drop_dup is not None: x.drop_duplicates(inplace=True)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "\n",
    "    def xlsx_read(self, file_path, cols_to_keep=None, sheet_name=0, dtype=None, drop_dup=None):\n",
    "        self.cols_to_keep = cols_to_keep\n",
    "        if dtype is None:\n",
    "          x=pd.read_excel(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], sheet_name=sheet_name)\n",
    "        else:\n",
    "          x=pd.read_excel(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], sheet_name=sheet_name, dtype=dtype)\n",
    "        chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'', ':']\n",
    "        for i in chars_to_remove:\n",
    "            x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n",
    "        if cols_to_keep is not None: x = x[cols_to_keep]\n",
    "        if drop_dup is not None: x.drop_duplicates(inplace=True)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def process_columns(self, df, cols=None):\n",
    "        if cols is None:\n",
    "            df = df.apply(lambda x: x.str.lower() if (x.dtype == 'object') else x)\n",
    "            df = df.apply(lambda x: x.str.strip() if (x.dtype == 'object') else x)\n",
    "            df = df.apply(lambda x: x.str.replace('\\s+|\\s', '_', regex=True) if (x.dtype == 'object') else x)\n",
    "            df = df.apply(lambda x: x.str.replace('[^\\w+\\s+]', '_', regex=True) if (x.dtype == 'object') else x)\n",
    "            df = df.apply(lambda x: x.str.replace('\\_+', '_', regex=True) if (x.dtype == 'object') else x)\n",
    "        else:\n",
    "            df = df.apply(lambda x: x.str.lower() if x.name in cols else x)\n",
    "            df = df.apply(lambda x: x.str.strip() if x.name in cols else x)\n",
    "            df = df.apply(lambda x: x.str.replace('\\s+|\\s', '_', regex=True) if x.name in cols else x)\n",
    "            df = df.apply(lambda x: x.str.replace('[^\\w+\\s+]', '_', regex=True) if x.name in cols else x)\n",
    "            df = df.apply(lambda x: x.str.replace('\\_+', '_', regex=True) if x.name in cols else x)\n",
    "        return df\n",
    "  \n",
    "    def nlp_process_columns(self, df, nlp_cols):\n",
    "        df = df.apply(lambda x: x.str.replace('_', ' ') if x.name in nlp_cols else x)\n",
    "        df = df.apply(lambda x: x.str.replace('\\s+', ' ', regex=True) if x.name in nlp_cols else x)\n",
    "        df = df.apply(lambda x: x.str.replace('crft', 'craft') if x.name in nlp_cols else x)\n",
    "        return df\n",
    "    \n",
    "    def retrieve_name(var):\n",
    "        \"\"\"\n",
    "        Gets the name of var. Does it from the out most frame inner-wards.\n",
    "        :param var: variable to get name from.\n",
    "        :return: string\n",
    "        \"\"\"\n",
    "        for fi in reversed(inspect.stack()):\n",
    "            names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "            if len(names) > 0:\n",
    "                return names[0]\n",
    "\n",
    "helpers = helper_funcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## read in the BP files\n",
    "\n",
    "# bp_nlp_cols = ['company_name', 'contract_text', 'cost_center_description', 'functional_area_name', 'global_job_description', \n",
    "#                'inbev_description', 'inbev_entity_l2_desc', 'inbev_entity_l3_desc', 'inbev_entity_l4_desc', 'global_id',\n",
    "#                'job_family_description', 'macro_entity_desc', 'macro_entity_l2_desc', 'macro_entity_l3_desc', \n",
    "#                'macro_entity_l4_desc', 'macro_entity_l5_desc', 'macro_entity_l6_desc', 'org_unit_description', 'year',\n",
    "#                'pay_scale_area_text', 'pers_subarea_text', 'personnel_area_text', 'position_type_text', 'position_title', \n",
    "#                'position_title', 'direct_manager_position_desc', 'org_unit_description', 'global_job_description',\n",
    "#               'payroll_area_text', 'local_entity_description', 'local_entity_l1_desc', 'local_entity_l2_desc', \n",
    "#                'local_entity_l3_desc', 'local_entity_4_desc', 'local_entity_l5_desc', 'local_entity_l6_desc']\n",
    "\n",
    "# import pickle\n",
    "# # load backup\n",
    "# bp_files = open('../working/bp_backup.pkl', 'rb')\n",
    "# bp_2016 = pickle.load(bp_files)\n",
    "# bp_2017 = pickle.load(bp_files)\n",
    "# bp_2018 = pickle.load(bp_files)\n",
    "# bp_files.close()\n",
    "\n",
    "# bp_2016 = bp_2016[bp_2016['employment_status']=='Active']\n",
    "# bp_2017 = bp_2017[bp_2017['employment_status']=='Active']\n",
    "# bp_2018 = bp_2018[bp_2018['employment_status']=='Active']\n",
    "\n",
    "# # # merge the files and do some pre-processing on the columns\n",
    "# bp_full = bp_2016.append(bp_2017, ignore_index=True)\n",
    "# bp_full = bp_full.append(bp_2018, ignore_index=True)\n",
    "\n",
    "# bp_full.drop_duplicates(subset=['global_id', 'year'], keep='last', inplace=True)\n",
    "# bp_full = bp_full[bp_nlp_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OPR\n",
    "\n",
    "# # global dictionaries and lists\n",
    "# dep_dict = {'4A': 5, '4B': 4, '3A': 3, '3B': 2, '1A': 1, '1B': 0}\n",
    "\n",
    "# ## read the input opr files\n",
    "# opr_2015 = helpers.csv_read(file_path='../input/OPR/global_opr_2015.csv', drop_dup='yes')\n",
    "# opr_2016 = helpers.csv_read(file_path='../input/OPR/global_opr_2016.csv', drop_dup='yes')\n",
    "# opr_2017 = helpers.csv_read(file_path='../input/OPR/global_opr_2017.csv', drop_dup='yes')\n",
    "# opr_2018 = helpers.csv_read(file_path='../input/OPR/global_opr_2018.csv', drop_dup='yes')\n",
    "# #opr_2018.head(2)\n",
    "\n",
    "# ## pre-processing the input files and appending them. not dynamic since cadence/structure can change\n",
    "# required_cols = ['employee_global_id', 'year', 'opr_rating_scale']\n",
    "# opr_2015 = opr_2015[required_cols]\n",
    "# opr_2016 = opr_2016[required_cols]\n",
    "# opr_2017 = opr_2017[required_cols]\n",
    "# opr_2018 = opr_2018[required_cols]\n",
    "\n",
    "# ## create the full set\n",
    "# opr_full = opr_2015.append(opr_2016, ignore_index=True)\n",
    "# opr_full = opr_full.append(opr_2017, ignore_index=True)\n",
    "# opr_full = opr_full.append(opr_2018, ignore_index=True)\n",
    "# opr_full.columns = ['global_id', 'year', 'opr']\n",
    "# opr_full.drop_duplicates(inplace=True, subset=['global_id', 'year'])\n",
    "# opr_full.dropna(how='any', inplace=True)\n",
    "# opr_full = opr_full[opr_full['year'] > 2014]\n",
    "# opr_full.reset_index(inplace=True, drop=True)\n",
    "# opr_full = opr_full[opr_full['opr']!='2']\n",
    "# opr_full['opr'] = opr_full['opr'].map(dep_dict)\n",
    "# opr_full.to_csv('../working/opr_full.csv', index=False)\n",
    "\n",
    "# ## reshaping and creating the pivot version\n",
    "# opr_reshaped = opr_full.pivot(index='global_id', columns='year', values=['opr']).reset_index().my_flatten_cols()\n",
    "# opr_reshaped.columns.name = None\n",
    "# #opr_reshaped.head(5)\n",
    "# opr_reshaped[['opr_2015', 'opr_2016', 'opr_2017', 'opr_2018']] = opr_reshaped[['opr_2015', 'opr_2016', 'opr_2017', 'opr_2018']].apply(pd.to_numeric, errors='coerce')\n",
    "# opr_reshaped = helpers.process_columns(df=opr_reshaped)\n",
    "\n",
    "# ## split into train and valid\n",
    "# opr_train = opr_reshaped.filter(regex='id|15|16|17')\n",
    "# opr_train['year'] = 2017\n",
    "# opr_train.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'response', 'year']\n",
    "# opr_valid = opr_reshaped.filter(regex='id|16|17|18')\n",
    "# opr_valid['year'] = 2018\n",
    "# opr_valid.columns = ['global_id', 'opr_prev_prev', 'opr_prev', 'response', 'year']\n",
    "# opr_train.dropna(subset=['global_id', 'response'], inplace=True)\n",
    "# opr_valid.dropna(subset=['global_id', 'response'], inplace=True)\n",
    "# opr_train.reset_index(drop=True, inplace=True)\n",
    "# opr_valid.reset_index(drop=True, inplace=True)\n",
    "# 'opr train shape is: ', opr_train.shape\n",
    "# 'opr valid shape is: ', opr_valid.shape\n",
    "# opr_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_opr_train = opr_train[['global_id', 'response', 'year']]\n",
    "# nlp_opr_valid = opr_valid[['global_id', 'response', 'year']]\n",
    "# nlp_opr_train.shape, nlp_opr_valid.shape\n",
    "\n",
    "# nlp_opr_train = pd.merge(nlp_opr_train.reset_index(drop=True), bp_full, on=['global_id', 'year'], how='left')\n",
    "# nlp_opr_valid = pd.merge(nlp_opr_valid.reset_index(drop=True), bp_full, on=['global_id', 'year'], how='left')\n",
    "# nlp_opr_train.shape, nlp_opr_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PDP data\n",
    "\n",
    "# # load backup\n",
    "# pdpbackup = open('../working/pdpbackup.pkl', 'rb')\n",
    "# pdp16 = pickle.load(pdpbackup)\n",
    "# pdp17 = pickle.load(pdpbackup)\n",
    "# pdp18 = pickle.load(pdpbackup)\n",
    "# pdpbackup.close()\n",
    "\n",
    "# pdp17 = pdp17[['employee_global_id', 'organization_&_hierarchy', 'action', 'development_objective_title',\n",
    "#               'employee_progress_comments', 'manager_progress_comments']]\n",
    "# pdp18 = pdp18[['employee_global_id', 'organization_&_hierarchy', 'action', 'development_objective_title',\n",
    "#               'employee_progress_comments', 'manager_progress_comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_Columns_Into_New_Column(DF, columns_To_Combine, new_Column_Name):\n",
    "#     DF[new_Column_Name] = ''\n",
    "#     for Col in columns_To_Combine:\n",
    "#         DF[new_Column_Name] += DF[Col].map(str) + ' '\n",
    "#     DF = DF.drop(columns_To_Combine, axis=1)\n",
    "#     DF = DF.groupby(by=['employee_global_id']).sum()\n",
    "#     DF.reset_index(drop=False, inplace=True)\n",
    "#     return DF\n",
    "\n",
    "# def pdp_process(df):\n",
    "#     dfagg = df[['employee_global_id', 'organization_&_hierarchy']].copy()\n",
    "#     dfagg.drop_duplicates(inplace=True)\n",
    "#     dfagg.fillna(value='', inplace=True)\n",
    "\n",
    "#     dfagg2 = df[['employee_global_id', 'action', 'development_objective_title', 'employee_progress_comments', 'manager_progress_comments']].copy()\n",
    "#     dfagg2.fillna(value='', inplace=True)\n",
    "#     dfagg2 = combine_Columns_Into_New_Column(dfagg2, ['action', 'development_objective_title', 'employee_progress_comments', 'manager_progress_comments'], \n",
    "#                                                  'all_strings')\n",
    "#     df = dfagg.merge(dfagg2, on='employee_global_id', how='outer')\n",
    "#     df.columns = ['global_id', 'orghierarchy', 'allstrings']\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdp17full = pdp_process(pdp17)\n",
    "# pdp18full = pdp_process(pdp18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_opr_train = nlp_opr_train.merge(pdp17full, on='global_id', how='left')\n",
    "# nlp_opr_valid = nlp_opr_valid.merge(pdp18full, on='global_id', how='left')\n",
    "\n",
    "# nlp_opr_train_ids = nlp_opr_train.global_id\n",
    "# nlp_opr_valid_ids = nlp_opr_valid.global_id\n",
    "\n",
    "# nlp_opr_train.drop(columns=['global_id', 'year'], inplace=True)\n",
    "# nlp_opr_valid.drop(columns=['global_id', 'year'], inplace=True)\n",
    "\n",
    "# text_cols = nlp_opr_train.columns\n",
    "# text_cols = text_cols.delete(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input/ouput files\n",
    "\n",
    "# nlp_opr_train['string_all'] = nlp_opr_train[text_cols].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "# nlp_opr_valid['string_all'] = nlp_opr_valid[text_cols].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "# y_train = nlp_opr_train[['response']]\n",
    "# y_test = nlp_opr_valid[['response']]\n",
    "# y_train = y_train.astype(int)\n",
    "# y_test = y_test.astype(int)\n",
    "# train_text = nlp_opr_train[['string_all']]\n",
    "# test_text = nlp_opr_valid[['string_all']]\n",
    "# train_text = train_text.replace('\\n','', regex=True)\n",
    "# test_text = test_text.replace('\\n','', regex=True)\n",
    "# train_text = helpers.nlp_process_columns(train_text, nlp_cols=['string_all'])\n",
    "# test_text = helpers.nlp_process_columns(test_text, nlp_cols=['string_all'])\n",
    "\n",
    "# train_final = pd.concat([y_train.reset_index(drop=True), train_text.reset_index(drop=True)], axis=1)\n",
    "# test_final = pd.concat([y_test.reset_index(drop=True), test_text.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save backup\n",
    "# # nlp_pickle = open('../working/nlppickle.pkl','wb')\n",
    "# # pickle.dump(train_final, nlp_pickle)\n",
    "# # pickle.dump(test_final, nlp_pickle)\n",
    "# # nlp_pickle.close()\n",
    "\n",
    "# # # load backup\n",
    "# nlp_pickle = open('../working/nlppickle.pkl', 'rb')\n",
    "# train_final = pickle.load(nlp_pickle)\n",
    "# test_final = pickle.load(nlp_pickle)\n",
    "# nlp_pickle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# #nltk.download('stopwords')\n",
    "\n",
    "# def clean_text(string):\n",
    "#     string = re.sub(r'['+string.punctuation+']', ' ', string)\n",
    "#     string = re.sub(r'[0-9]+', ' ', string)\n",
    "#     string = re.sub('\\s+', ' ', string).strip()\n",
    "#     string = string.lower()\n",
    "    \n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "\n",
    "# train_final['stopwords'] = train_final['string_all'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "# train_final[['string_all','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3549, 3)\n",
      "(3908, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>global_id</th>\n",
       "      <th>comments</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001929</td>\n",
       "      <td>communication across the departments to elimin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002651</td>\n",
       "      <td>hoj great owner of the ABI culture hoj high le...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   global_id                                           comments  response\n",
       "0    1001929  communication across the departments to elimin...         0\n",
       "1    1002651  hoj great owner of the ABI culture hoj high le...         2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## comp comments\n",
    "\n",
    "train = helpers.csv_read('../input/Navigate/comp_appraisal_comments/train_filtered.csv')\n",
    "valid = helpers.csv_read('../input/Navigate/comp_appraisal_comments/valid_filtered.csv')\n",
    "\n",
    "ytrain = np.array(train.response)\n",
    "yvalid = np.array(valid.response)\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['string'] = '__label__' + train['response'].astype('str') + ' ' + train['comments']\n",
    "valid['string'] = '__label__' + valid['response'].astype('str') + ' ' + valid['comments']\n",
    "\n",
    "train.drop(['global_id', 'response', 'comments'], inplace=True, axis=1)\n",
    "valid.drop(['global_id', 'response', 'comments'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.txt', encoding='utf-8', index=False, header=False, sep='\\t')\n",
    "valid.to_csv('test.txt', encoding='utf-8', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__0 communication across the department...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2 hoj great owner of the ABI culture ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              string\n",
       "0  __label__0 communication across the department...\n",
       "1  __label__2 hoj great owner of the ABI culture ..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastText import train_supervised\n",
    "\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_supervised(input = 'train.txt', epoch=100, lr=1, wordNgrams=5, \n",
    "                         verbose=10, minCount=10, loss='softmax', minn=5, dim=50, ws=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t3908\n",
      "P@1\t0.167\n",
      "R@1\t1.000\n"
     ]
    }
   ],
   "source": [
    "print_results(*model.test('test.txt', k=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = valid.shape[0]\n",
    "predict_list = {}\n",
    "for i in range(test_len):\n",
    "    x=model.predict(valid.iloc[i][0], k=6)\n",
    "    y=pd.DataFrame(list(x))\n",
    "    y.columns=y.iloc[0]\n",
    "    y=y.reindex(y.index.drop(0))\n",
    "    predict_list[i] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat(predict_list.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__label__0</th>\n",
       "      <th>__label__1</th>\n",
       "      <th>__label__2</th>\n",
       "      <th>__label__3</th>\n",
       "      <th>__label__4</th>\n",
       "      <th>__label__5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.803385</td>\n",
       "      <td>0.087291</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.827874</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.013256</td>\n",
       "      <td>0.221778</td>\n",
       "      <td>0.763705</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.076031</td>\n",
       "      <td>0.922190</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015996</td>\n",
       "      <td>0.084058</td>\n",
       "      <td>0.519280</td>\n",
       "      <td>0.193349</td>\n",
       "      <td>0.169685</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.297013</td>\n",
       "      <td>0.677778</td>\n",
       "      <td>0.019855</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.027596</td>\n",
       "      <td>0.401129</td>\n",
       "      <td>0.461924</td>\n",
       "      <td>0.095287</td>\n",
       "      <td>0.011604</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.051684</td>\n",
       "      <td>0.943123</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>0.991873</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.013631</td>\n",
       "      <td>0.957588</td>\n",
       "      <td>0.028489</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   __label__0  __label__1  __label__2  __label__3  __label__4  __label__5  \\\n",
       "0    0.000601    0.094422    0.803385    0.087291    0.013991    0.000370   \n",
       "1    0.002893    0.035064    0.827874    0.100877    0.030609    0.002743   \n",
       "2    0.000029    0.013256    0.221778    0.763705    0.000690    0.000602   \n",
       "3    0.000418    0.076031    0.922190    0.001369    0.000038    0.000014   \n",
       "4    0.015996    0.084058    0.519280    0.193349    0.169685    0.017692   \n",
       "5    0.003540    0.297013    0.677778    0.019855    0.001496    0.000377   \n",
       "6    0.002520    0.027596    0.401129    0.461924    0.095287    0.011604   \n",
       "7    0.000453    0.051684    0.943123    0.004557    0.000216    0.000028   \n",
       "8    0.000084    0.003114    0.991873    0.004719    0.000257    0.000012   \n",
       "9    0.000028    0.013631    0.957588    0.028489    0.000309    0.000015   \n",
       "\n",
       "   label  \n",
       "0      2  \n",
       "1      2  \n",
       "2      3  \n",
       "3      2  \n",
       "4      2  \n",
       "5      2  \n",
       "6      3  \n",
       "7      2  \n",
       "8      2  \n",
       "9      2  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred.apply(pd.to_numeric) # convert all columns of DataFrame\n",
    "#minmax = StandardScaler()\n",
    "#pred=pd.DataFrame(minmax.fit_transform(pred))\n",
    "pred['label'] = pred.idxmax(axis=1)\n",
    "pred = pred.replace('\\_\\_label\\_\\_','', regex=True)\n",
    "pred['label'] = pred['label'].astype(int)\n",
    "pred.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 2, 0, 1, 2, 2, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yvalid[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred['actuals'] = yvalid\n",
    "pred.to_csv('nlppred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44779938587512796"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skm.accuracy_score(y_pred=pred.label, y_true=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,  101,   26,    4,    0],\n",
       "       [   0,    0,  187,   48,    0,    0],\n",
       "       [   1,    5, 1324,  366,   26,    1],\n",
       "       [   0,    5,  671,  410,   34,    1],\n",
       "       [   0,    1,  287,  226,   16,    0],\n",
       "       [   0,    2,   86,   76,    4,    0]], dtype=int64)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skm.confusion_matrix(y_pred=pred.label, y_true=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__label__0</th>\n",
       "      <th>__label__1</th>\n",
       "      <th>__label__2</th>\n",
       "      <th>__label__3</th>\n",
       "      <th>__label__4</th>\n",
       "      <th>__label__5</th>\n",
       "      <th>label</th>\n",
       "      <th>actuals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.803385</td>\n",
       "      <td>0.087291</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.827874</td>\n",
       "      <td>0.100877</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   __label__0  __label__1  __label__2  __label__3  __label__4  __label__5  \\\n",
       "0    0.000601    0.094422    0.803385    0.087291    0.013991    0.000370   \n",
       "1    0.002893    0.035064    0.827874    0.100877    0.030609    0.002743   \n",
       "\n",
       "   label  actuals  \n",
       "0      2        1  \n",
       "1      2        2  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = pred.iloc[:, 0:6]\n",
    "predictions = pred.label\n",
    "yvalid = pred.actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shape module\n",
    "\n",
    "import functools\n",
    "\n",
    "class opr_shape():\n",
    "    \"\"\" precedence => 4A -> 4B -> 1B -> 1A -> 3A -> 3B (5 > 4 > 0 > 1 > 3 > 2) \"\"\"\n",
    "    \n",
    "    def __init__(self, probs, ytrain):\n",
    "        self.probs = probs\n",
    "        self.ytrain = ytrain\n",
    "        self.shapevec()\n",
    "        self.main()\n",
    "        \n",
    "    def shapevec(self):\n",
    "        shape_vec = pd.DataFrame(self.ytrain.value_counts()/self.ytrain.shape[0])\n",
    "        shape_vec['class'] = shape_vec.index\n",
    "        shape_vec.sort_values('class', inplace=True)\n",
    "        #shape_vec['response'] = shape_vec.response.cumsum()\n",
    "        #print('the shape vector is: ', shape_vec)\n",
    "        self.shape_vec = shape_vec\n",
    "        return None\n",
    "\n",
    "    def main(self):\n",
    "        self.probs_df = {}\n",
    "        for i in range(self.probs.shape[1]):\n",
    "            self.probs_df[i] = pd.DataFrame({str('prob_'+str(i)): self.probs.iloc[:, i]})\n",
    "            self.probs_df[i]['true_index'] = self.probs_df[i].index\n",
    "            self.probs_df[i].sort_values(by=[str('prob_'+str(i))], inplace=True, kind='mergesort', ascending=False)\n",
    "            self.probs_df[i].reset_index(inplace=True, drop=True)\n",
    "            self.probs_df[i][str(str(i)+'_new_index')] = self.probs_df[i].index\n",
    "            self.probs_df[i][str(str(i)+'_index_perc')] = self.probs_df[i][str(str(i)+'_new_index')].rank(pct=True)\n",
    "            self.probs_df[i][str(str(i)+'_flag')] = np.where(self.probs_df[i][str(str(i)+'_index_perc')]<self.shape_vec.iloc[i, 0], 1, 0)\n",
    "            \n",
    "        merge = functools.partial(pd.merge, left_index=False, right_index=False, how='inner', on='true_index')\n",
    "        finaldf = functools.reduce(merge, self.probs_df.values())\n",
    "        \n",
    "        finaldf['class'] = np.where(finaldf['5_flag']==1, 5, np.where(finaldf['4_flag']==1, 4, np.where(finaldf['0_flag']==1, 0, \n",
    "                            np.where(finaldf['1_flag']==1, 1, np.where(finaldf['2_flag']==1, 2, 3)))))\n",
    "        self.finaldf = finaldf\n",
    "        return None\n",
    "\n",
    "oprs = opr_shape(probs=probs, ytrain=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38792221084953943"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[  7,  13,  57,  32,  13,   9],\n",
       "       [  7,  22, 112,  69,  19,   6],\n",
       "       [ 32,  86, 872, 523, 161,  49],\n",
       "       [ 13,  20, 364, 517, 145,  62],\n",
       "       [  8,  16, 141, 242,  90,  33],\n",
       "       [  2,   7,  42,  82,  27,   8]], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_df = oprs.finaldf[['true_index', 'class']]\n",
    "shaped_df.set_index('true_index', inplace=True)\n",
    "shaped_df.sort_index(inplace=True)\n",
    "shaped_df.reset_index(drop=True, inplace=True)\n",
    "skm.accuracy_score(y_true=yvalid, y_pred=shaped_df['class'])\n",
    "skm.confusion_matrix(y_true=yvalid, y_pred=shaped_df['class'])\n",
    "shaped_df.columns = ['shape_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_probs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-161-008a3146f0e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpred_df1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpred_df1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'true_index'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_df1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_probs1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pred_df1 = pd.DataFrame(probs)\n",
    "pred_df1['score'] = (pred_df1[0]*1)+(pred_df1[1]*2)+(pred_df1[2]*3)+(pred_df1[3]*4)+(pred_df1[4]*5)+(pred_df1[5]*6)\n",
    "pred_df1['true_index'] = pred_df1.index\n",
    "pred_df1.sort_values('score', inplace=True)\n",
    "pred_df1.reset_index(inplace=True, drop=True)\n",
    "pred_df1=pred_df1[['score', 'true_index']]\n",
    "pred_df1['new_index'] = (pred_df1.index+1)/pred_df1.shape[0]\n",
    "pred_df1['class'] = np.where(pred_df1['new_index']<0.031877, 0, \n",
    "                             (np.where(pred_df1['new_index']<0.100751, 1, \n",
    "                                       (np.where(pred_df1['new_index']<0.582359, 2, \n",
    "                                                 (np.where(pred_df1['new_index']<0.872194, 3, \n",
    "                                                           (np.where(pred_df1['new_index']<0.970513, 4, 5)))))))))\n",
    "dummy_df = pred_df1.copy()\n",
    "dummy_df = dummy_df[['true_index', 'class']]\n",
    "dummy_df.set_index(keys='true_index', inplace=True)\n",
    "dummy_df.sort_index(inplace=True)\n",
    "dummy_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "x1=dummy_df['class']\n",
    "sklearn.metrics.accuracy_score(y_true=yvalid, y_pred=x1)\n",
    "sklearn.metrics.confusion_matrix(y_pred=x1, y_true=yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
