{"cells":[{"cell_type":"markdown","source":["# Raw_ADS_preparation.ipynb\n> Project: **ABI Turnover**  \n> Turnover Process Phase: **1**  \n> Author: **Varun V**  \n> Location: **GCC**  \n> Team: **People Analytics**"],"metadata":{}},{"cell_type":"markdown","source":["###Pending steps\n- add target combined file creation functions from the prior setup to the target_class\n- missing value imputation module\n- package as seperate py classes file and make the notebook compact"],"metadata":{}},{"cell_type":"code","source":["## importing the relevant packages:\n\n# clear the workspace\n# %reset -f\n\n# print list of files in directory\nimport os\nprint(os.listdir())\n\n# the base packages\nimport collections # for the Counter function\nimport csv # for reading/writing csv files\nimport pandas as pd, numpy as np, time, gc, bisect, re\nfrom datetime import datetime as dt\n\n# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\nimport sklearn\nfrom sklearn import metrics, preprocessing\n\nrandomseed = 1 # the value for the random state used at various points in the pipeline\npd.options.display.max_rows = 1000 # specify if you want the full output in cells rather the truncated list\npd.options.display.max_columns = 200\npd.options.mode.chained_assignment = None  # default='warn'\n\n# to display multiple outputs in a cell without usin print/display\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;conf&#39;, &#39;ganglia&#39;, &#39;logs&#39;, &#39;eventlogs&#39;, &#39;derby.log&#39;]\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["__The Parent folder structure of the ADLS where the invidual files are stored__\n\n*classes/functions follow the order as dictated by the folder structure*"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/</td><td>BLUEPRINTS/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/</td><td>COMPETENCY/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MISC/</td><td>MISC/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/</td><td>MOVEMENT/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/OPR/</td><td>OPR/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/SALARY/</td><td>SALARY/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/TARGET/</td><td>TARGET/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/TURNOVER/</td><td>TURNOVER/</td><td>0</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/test/</td><td>test/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# GLOBAL VARIABLES/DEFINITIONS\n\n# for blueprint (main and nlp/text columns)\nbp_cols_to_keep = ['org_unit_id_of_position_om', 'position_id_om', 'position_name_om', 'macro_entity_level_3_om', 'macro_entity_level_4_om',  'ab__inbev_entity_level_2_om', 'ab__inbev_entity_level_3_om', 'ab__inbev_entity_level_4_om', 'employee_global_id_pa', 'employee_personnal_number_pa', 'pay_grade_group_pa', 'physical_work_location_code_pa', 'physical_work_location_description_pa', 'physical_work_location_city_pa', 'global_job_om', 'job_family_om', 'functional_area_om', 'position_start_date_om', 'position_band_om', 'position_work_location_code_om', 'position_work_location_country_om', 'manager_s_global_id_pa', 'manager_s_position_id_om', 'manager_s_position_name_om', 'ebm_level_of_the_job_om', 'company_code_om', 'employee_group_code_om', 'employee_subgroup_code_om', 'cost_center_id_om', 'local_entity_code', 'employment_status_pa']\nbp_cols_for_nlp = ['org_unit_name_of_position_om', 'position_name_om', 'company_name_om', 'global_job_om', 'job_family_om', 'functional_area_om', 'position_work_location_code_om', 'manager_s_position_name_om', 'cost_center_text_om', 'local_entity_description', 'employee_global_id_pa', 'employee_personnal_number_pa', 'position_id_om']\n\n# for competency\ncp_cols_to_keep = ['employee_global_id', 'personnel_number', 'year', 'competency_group', 'competency', 'manager_rating___scale_value', 'manager_rating___numeric_value']\n\n# for movement\nmov_cols_to_keep = ['pers_no_', 'position', 'name_of_manager_om_', 'ps_group', 'start_date', 'end_date', 'reason_for_action']\n\n# for opr\nopr_cols_to_keep = ['personnel_number', 'year', 'opr_rating_scale']\n\n# for salary\nsal_cols_to_keep = ['pers_no_', 'name_of_employee_or_applicant', 'for_period', 'amount', 'crcy']\n\n# for target\ntarget_cols_to_keep = ['appraisee_id', 'appraiser_id', 'net_target', 'year']\n\n# for turnover\nto_cols_to_keep = ['employee_id', 'global_id', 'pay_scale_group', 'name_of_action_type', 'name_of_reason_for_action', 'last_day', 'termination_date']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# HELPER FUNCTIONS CLASS #\n\n\nclass helper_funcs(object):\n  \n  def __init__(self):\n    \"\"\" list down the various functions defined here \"\"\"\n  \n  def csv_read(self, file_path, cols_to_remove=None, dtype=None):\n    self.cols_to_remove = cols_to_remove\n    if dtype is None:\n      x=pd.read_csv(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], encoding='latin-1', low_memory=False)\n    else:\n      x=pd.read_csv(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], encoding='latin-1', low_memory=False, dtype=dtype)\n    print(x.shape)\n    if cols_to_remove is not None: x.drop(cols_to_remove, axis = 1, inplace = True)\n    chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'']\n    for i in chars_to_remove:\n        x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n    return x\n  \n  def txt_read(self, file_path, cols_to_remove=None, sep='|', skiprows=1, dtype=None):\n    # currently only supports salary files with the default values (need to implement dynamic programming for any generic txt)\n    self.cols_to_remove = cols_to_remove\n    if dtype is None:\n      x=pd.read_table(file_path, sep=sep, skiprows=skiprows, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'])\n    else:\n      x=pd.read_table(file_path, sep=sep, skiprows=skiprows, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], dtype=dtype)\n    print(x.shape)\n    if cols_to_remove is not None: x.drop(cols_to_remove, axis = 1, inplace = True)\n    chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'']\n    for i in chars_to_remove:\n        x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n    return x\n\n  def xlsx_read(self, file_path, cols_to_remove=None, sheet_name=0, dtype=None):\n    self.cols_to_remove = cols_to_remove\n    if dtype is None:\n      x=pd.read_excel(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], sheet_name=sheet_name)\n    else:\n      x=pd.read_excel(file_path, na_values=['No Data', ' ', 'UNKNOWN', '', 'Not Rated', 'Not Applicable'], sheet_name=sheet_name, dtype=dtype)\n    print(x.shape)\n    if cols_to_remove is not None: x.drop(cols_to_remove, axis = 1, inplace = True)\n    chars_to_remove = [' ', '.', '(', ')', '__', '-', '/', '\\'']\n    for i in chars_to_remove:\n        x.columns = x.columns.str.strip().str.lower().str.replace(i, '_')\n    return x\n  \n  def process_columns(self, df):\n    df = df.apply(lambda x: x.str.lower() if (x.dtype == 'object') else x)\n    df = df.apply(lambda x: x.str.strip() if (x.dtype == 'object') else x)\n    df = df.apply(lambda x: x.str.replace('[^\\w\\s]', '_', regex=True) if (x.dtype == 'object') else x)\n    return df\n  \n  def nlp_process_columns(self, df, nlp_cols):\n    df = df.apply(lambda x: x.str.replace('_', ' ') if x.name in nlp_cols else x)\n    df = df.apply(lambda x: x.str.replace('\\s+', ' ', regex=True) if x.name in nlp_cols else x)\n    df = df.apply(lambda x: x.str.replace('crft', 'craft') if x.name in nlp_cols else x)\n    return df"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# initialize the helper functions class instance\n\nhelpers = helper_funcs()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["#BLUEPRINT"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/BLUEPRINTS"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/01-2017.xlsx</td><td>01-2017.xlsx</td><td>9141149</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/01-2018.xlsx</td><td>01-2018.xlsx</td><td>15701971</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/02-2017.xlsx</td><td>02-2017.xlsx</td><td>9974343</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/02-2018.xlsx</td><td>02-2018.xlsx</td><td>15826020</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/03-2017.xlsx</td><td>03-2017.xlsx</td><td>10171703</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/03-2018.xlsx</td><td>03-2018.xlsx</td><td>15900764</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/04-2017.xlsx</td><td>04-2017.xlsx</td><td>10156939</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/04-2018.xlsx</td><td>04-2018.xlsx</td><td>16046382</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/05-2016.xlsx</td><td>05-2016.xlsx</td><td>7657060</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/05-2017.xlsx</td><td>05-2017.xlsx</td><td>10231479</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/05-2018.xlsx</td><td>05-2018.xlsx</td><td>15940415</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/06-2016.xlsx</td><td>06-2016.xlsx</td><td>7766965</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/06-2017.xlsx</td><td>06-2017.xlsx</td><td>10297592</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/06-2018.xlsx</td><td>06-2018.xlsx</td><td>15832373</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/07-2016.xlsx</td><td>07-2016.xlsx</td><td>10381074</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/07-2017.xlsx</td><td>07-2017.xlsx</td><td>10388105</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/07-2018.xlsx</td><td>07-2018.xlsx</td><td>16163577</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/08-2016.xlsx</td><td>08-2016.xlsx</td><td>9786222</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/08-2017.xlsx</td><td>08-2017.xlsx</td><td>10392838</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/08-2018.xlsx</td><td>08-2018.xlsx</td><td>15892407</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/09-2016.xlsx</td><td>09-2016.xlsx</td><td>9752613</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/09-2017.xlsx</td><td>09-2017.xlsx</td><td>10429362</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/10-2016.xlsx</td><td>10-2016.xlsx</td><td>9956785</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/10-2017.xlsx</td><td>10-2017.xlsx</td><td>10229109</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/11-2016.xlsx</td><td>11-2016.xlsx</td><td>10091960</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/11-2017.xlsx</td><td>11-2017.xlsx</td><td>10213954</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/12-2016.xlsx</td><td>12-2016.xlsx</td><td>10037410</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/BLUEPRINTS/12-2017.xlsx</td><td>12-2017.xlsx</td><td>10223429</td></tr></tbody></table></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# BLUEPRINT CLASS #\n\n\nclass blueprint(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep, cols_for_nlp):\n    \"\"\" \n    this class does multiple things:\n      - reads in the multiple blueprint reports\n      - subset only necessary columns and create a seperate dataset for NLP\n      - do some basic preprocessing steps on the column names and content\n      - ...\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    self.cols_for_nlp = cols_for_nlp\n  \n  def main(self):\n    ## list the existing blueprint reports and store the list of file names in file_names_keys\n    files = dbutils.fs.ls('/mnt/datalake/INPUT/BLUEPRINTS/')\n    file_names = {}\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n    file_names_keys = list(file_names.keys())\n    self.file_names_keys = file_names_keys\n\n    # splitting the list of files on the basis of the year it belongs to\n    df16 = [x for x in file_names_keys if re.search(pattern='-2016.xlsx', string=x)]\n    df17 = [x for x in file_names_keys if re.search(pattern='-2017.xlsx', string=x)]\n    df18 = [x for x in file_names_keys if re.search(pattern='-2018.xlsx', string=x)]\n    \n    dict16_keys = [s.replace('.xlsx', '') for s in df16]\n    dict17_keys = [s.replace('.xlsx', '') for s in df17]\n    dict18_keys = [s.replace('.xlsx', '') for s in df18]\n\n    # the dict to store the individual datasets (per year) and the nlp datasets of the corresponding files\n    dict16 = {}\n    dict17 = {}\n    dict18 = {}\n    nlp_dict16 = {}\n    nlp_dict17 = {}\n    nlp_dict18 = {}\n    \n    # read in the files and store them in the year dictionaries\n    iter=0\n    for i in df16:\n        dict16[dict16_keys[iter]] = self.helpers.xlsx_read(str('/dbfs/mnt/datalake/INPUT/BLUEPRINTS/' + dict16_keys[iter] + '.xlsx'))\n        nlp_dict16[dict16_keys[iter]] = dict16[dict16_keys[iter]][self.cols_for_nlp]\n        dict16[dict16_keys[iter]] = dict16[dict16_keys[iter]][self.cols_to_keep]\n        dict16[dict16_keys[iter]] = self.helpers.process_columns(dict16[dict16_keys[iter]])\n        dict16[dict16_keys[iter]]['month_file'] = dict16_keys[iter]\n        dict16[dict16_keys[iter]]['term_year'] = 2016\n        dict16[dict16_keys[iter]]['term_month_temp'] = dict16[dict16_keys[iter]]['month_file'].apply(lambda s:s.split('-')[0])\n        iter = iter+1\n\n    iter=0\n    for i in df17:\n        dict17[dict17_keys[iter]] = self.helpers.xlsx_read(str('/dbfs/mnt/datalake/INPUT/BLUEPRINTS/' + dict17_keys[iter] + '.xlsx'))\n        nlp_dict17[dict17_keys[iter]] = dict17[dict17_keys[iter]][self.cols_for_nlp]\n        dict17[dict17_keys[iter]] = dict17[dict17_keys[iter]][self.cols_to_keep]\n        dict17[dict17_keys[iter]] = self.helpers.process_columns(dict17[dict17_keys[iter]])\n        dict17[dict17_keys[iter]]['month_file'] = dict17_keys[iter]\n        dict17[dict17_keys[iter]]['term_year'] = 2017\n        dict17[dict17_keys[iter]]['term_month_temp'] = dict17[dict17_keys[iter]]['month_file'].apply(lambda s:s.split('-')[0])\n        iter = iter+1\n    \n    iter=0\n    for i in df18:\n        dict18[dict18_keys[iter]] = self.helpers.xlsx_read(str('/dbfs/mnt/datalake/INPUT/BLUEPRINTS/' + dict18_keys[iter] + '.xlsx'), \n                                                           sheet_name='Blueprint-Data')\n        nlp_dict18[dict18_keys[iter]] = dict18[dict18_keys[iter]][self.cols_for_nlp]\n        dict18[dict18_keys[iter]] = dict18[dict18_keys[iter]][self.cols_to_keep]\n        dict18[dict18_keys[iter]] = self.helpers.process_columns(dict18[dict18_keys[iter]])\n        dict18[dict18_keys[iter]]['month_file'] = dict18_keys[iter]\n        dict18[dict18_keys[iter]]['term_year'] = 2018\n        dict18[dict18_keys[iter]]['term_month_temp'] = dict18[dict18_keys[iter]]['month_file'].apply(lambda s:s.split('-')[0])\n        iter = iter+1\n        \n    self.dict16 = dict16\n    self.dict17 = dict17\n    self.dict18 = dict18\n    self.nlp_dict16 = nlp_dict16\n    self.nlp_dict17 = nlp_dict17\n    self.nlp_dict18 = nlp_dict18\n    \n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["bp = blueprint(helpers, bp_cols_to_keep, bp_cols_for_nlp)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"code","source":["bp.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(21057, 75)\n(21362, 75)\n(28658, 77)\n(26714, 77)\n(26827, 77)\n(27074, 77)\n(27092, 78)\n(27323, 77)\n(26582, 86)\n(27076, 77)\n(27189, 77)\n(27095, 77)\n(27284, 77)\n(27441, 77)\n(27671, 77)\n(27601, 77)\n(27757, 77)\n(27320, 77)\n(27242, 77)\n(27420, 77)\n(27601, 85)\n(27603, 85)\n(27783, 85)\n(27913, 85)\n(27810, 85)\n(27700, 85)\n(27677, 86)\n(27560, 85)\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["#COMPETENCY"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/COMPETENCY"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/NAZ_2018 Competency Results.xlsx</td><td>NAZ_2018 Competency Results.xlsx</td><td>15903629</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/competency_2015.csv</td><td>competency_2015.csv</td><td>23309365</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/competency_2016.csv</td><td>competency_2016.csv</td><td>27261788</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/competency_2017.csv</td><td>competency_2017.csv</td><td>27045442</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/competency_2018.csv</td><td>competency_2018.csv</td><td>39162901</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/COMPETENCY/competency_combined.csv</td><td>competency_combined.csv</td><td>378720</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# COMPETENCY CLASS #\n\nclass competency(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class does multiple things:\n      - reads in the multiple competency reports\n      - subset only necessary columns and create a seperate dataset for NLP\n      - do some basic preprocessing steps on the column names and content\n      - ...\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    ## list the existing competency files and store the list of file names in file_names_keys\n    files = dbutils.fs.ls('/mnt/datalake/INPUT/COMPETENCY/')\n    file_names = {}\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n    file_names_keys = list(file_names.keys())\n    self.file_names_keys = file_names_keys\n\n    # splitting the list of files on the basis of the year it belongs to\n    comp = [x for x in file_names_keys if re.search(pattern='competency_20', string=x)]\n    comp_keys = [s.replace('.csv', '') for s in comp]\n\n    # the dict to store the individual datasets (per year)\n    competency = {}\n    comp_years = [i.split('_')[1] for i in comp_keys]\n\n    # read in the files and store them in the year dictionaries\n    iter=0\n    for i in comp:\n        competency[comp_keys[iter]] = self.helpers.csv_read(str('/dbfs/mnt/datalake/INPUT/COMPETENCY/' + comp_keys[iter] + '.csv'))\n        competency[comp_keys[iter]] = competency[comp_keys[iter]][self.cols_to_keep]\n        competency[comp_keys[iter]] = self.helpers.process_columns(competency[comp_keys[iter]])\n        competency[comp_keys[iter]]['year'] = comp_years[iter]\n        iter = iter+1\n    self.competency = competency\n    \n    competency_combined = pd.concat(competency.values(), ignore_index=True)\n    print('competency combined original shape: ', competency_combined.shape)\n    competency_combined.drop_duplicates(inplace=True)\n    print('competency combined new shape after distinct: ', competency_combined.shape)\n    self.competency_combined = competency_combined\n\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["comp = competency(helpers, cp_cols_to_keep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["comp.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(106713, 15)\n(135468, 15)\n(131866, 15)\n(118650, 17)\ncompetency combined original shape:  (492697, 7)\ncompetency combined new shape after distinct:  (356170, 7)\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["#MISC"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/MISC"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/MISC/DEMOGRAPHICS_2017.xlsx</td><td>DEMOGRAPHICS_2017.xlsx</td><td>2420029</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MISC/NAZ_Headcountreport_20181109220859.csv</td><td>NAZ_Headcountreport_20181109220859.csv</td><td>8268833</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# MISCELLANEOUS CLASS #\n\nclass misc_class(object):\n  \n  def __init__(self, helper_funcs_instance):\n    \"\"\"\n    this class is for reading in the multiple miscellaneous files\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    return None\n  \n  def main(self):\n    # read in the files\n    demographics = self.helpers.xlsx_read('/dbfs/mnt/datalake/INPUT/MISC/DEMOGRAPHICS_2017.xlsx')\n    self.demo = demographics\n    \n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["misc = misc_class(helpers)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":22},{"cell_type":"code","source":["misc.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(67895, 4)\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["#MOVEMENT"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/MOVEMENT"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/Movements_2014.csv</td><td>Movements_2014.csv</td><td>2247865</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/Movements_2015.csv</td><td>Movements_2015.csv</td><td>1701558</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/Movements_2016.csv</td><td>Movements_2016.csv</td><td>1787575</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/Movements_2017.csv</td><td>Movements_2017.csv</td><td>2955452</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/Movements_2018.csv</td><td>Movements_2018.csv</td><td>3903004</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/MOVEMENT/NAZ_Movementsreport_20181109215959.csv</td><td>NAZ_Movementsreport_20181109215959.csv</td><td>34167938</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"code","source":["# MOVEMENT CLASS #\n\nclass movement_class(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class does multiple things:\n      - reads in the multiple movement reports\n      - subset only necessary columns\n      - do some basic preprocessing steps on the column names and content\n      - ...\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    ## list the existing movement files and store the list of file names in file_names_keys\n    files = dbutils.fs.ls('/mnt/datalake/INPUT/MOVEMENT/')\n    file_names = {}\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n    file_names_keys = list(file_names.keys())\n    self.file_names_keys = file_names_keys\n\n    # splitting the list of files on the basis of the year it belongs to\n    mov = [x for x in file_names_keys if re.search(pattern='Movements_2', string=x)]\n    mov_keys = [s.replace('.csv', '') for s in mov]\n\n    # the dict to store the individual datasets (per year)\n    movement = {}\n    movement_years = [i.split('_')[1] for i in mov_keys]\n\n    # read in the files and store them in the year dictionaries\n    iter=0\n    for i in mov:\n        movement[mov_keys[iter]] = self.helpers.csv_read(str('/dbfs/mnt/datalake/INPUT/MOVEMENT/' + mov_keys[iter] + '.csv'))\n        movement[mov_keys[iter]] = movement[mov_keys[iter]][self.cols_to_keep]\n        movement[mov_keys[iter]] = self.helpers.process_columns(movement[mov_keys[iter]])\n        movement[mov_keys[iter]]['year'] = movement_years[iter]\n        iter = iter+1\n    self.movement = movement\n    \n    movement_combined = pd.concat(movement.values(), ignore_index=True)\n    print('movement combined original shape: ', movement_combined.shape)\n    movement_combined.drop_duplicates(inplace=True)\n    print('movement combined new shape after distinct: ', movement_combined.shape)\n    self.movement_combined = movement_combined\n\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"code","source":["movement = movement_class(helpers, cols_to_keep = mov_cols_to_keep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"code","source":["movement.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(4975, 51)\n(3559, 51)\n(4413, 51)\n(11052, 21)\n(9110, 50)\nmovement combined original shape:  (33109, 8)\nmovement combined new shape after distinct:  (32184, 8)\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["#OPR"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/OPR"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/OPR/opr_2016.csv</td><td>opr_2016.csv</td><td>6291794</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/OPR/opr_2017.csv</td><td>opr_2017.csv</td><td>6938386</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/OPR/opr_combined.csv</td><td>opr_combined.csv</td><td>228941</td></tr></tbody></table></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# OPR CLASS #\n\nclass opr_class(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class does multiple things:\n      - reads in the multiple opr files\n      - subset only necessary columns\n      - do some basic preprocessing steps on the column names and content\n      - ...\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    ## list the existing opr files and store the list of file names in file_names_keys\n    files = dbutils.fs.ls('/mnt/datalake/INPUT/OPR/')\n    file_names = {}\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n    file_names_keys = list(file_names.keys())\n    self.file_names_keys = file_names_keys\n\n    # splitting the list of files on the basis of the year it belongs to\n    opr = [x for x in file_names_keys if re.search(pattern='opr_2', string=x)]\n    opr_keys = [s.replace('.csv', '') for s in opr]\n\n    # the dict to store the individual datasets (per year)\n    opr_list = {}\n    opr_years = [i.split('_')[1] for i in opr_keys]\n\n    # read in the files and store them in the year dictionaries\n    iter=0\n    for i in opr:\n        opr_list[opr_keys[iter]] = self.helpers.csv_read(str('/dbfs/mnt/datalake/INPUT/OPR/' + opr_keys[iter] + '.csv'))\n        opr_list[opr_keys[iter]] = opr_list[opr_keys[iter]][self.cols_to_keep]\n        opr_list[opr_keys[iter]] = self.helpers.process_columns(opr_list[opr_keys[iter]])\n        iter = iter+1\n    self.opr_list = opr_list\n    \n    opr_processed = self.helpers.csv_read('/dbfs/mnt/datalake/INPUT/OPR/opr_combined.csv')\n    opr_processed = self.helpers.process_columns(opr_processed)\n    self.opr_processed = opr_processed\n    \n    opr_combined = pd.concat(opr_list.values(), ignore_index=True)\n    print('opr combined original shape: ', opr_combined.shape)\n    opr_combined.drop_duplicates(inplace=True)\n    print('opr combined new shape after distinct: ', opr_combined.shape)\n    self.opr_combined = opr_combined\n\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["opr = opr_class(helpers, opr_cols_to_keep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"code","source":["opr.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(63647, 12)\n(70055, 12)\n(7778, 12)\nopr combined original shape:  (133702, 3)\nopr combined new shape after distinct:  (70051, 3)\n</div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["#SALARY"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/SALARY"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/SALARY/salary_bonus_CA_2018.xlsx</td><td>salary_bonus_CA_2018.xlsx</td><td>687042</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/SALARY/salary_bonus_US_2018.txt</td><td>salary_bonus_US_2018.txt</td><td>34760201</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/SALARY/salary_main_NAZ_2018.xlsx</td><td>salary_main_NAZ_2018.xlsx</td><td>6880592</td></tr></tbody></table></div>"]}}],"execution_count":35},{"cell_type":"code","source":["# SALARY CLASS #\n\nclass salary_class(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class does multiple things:\n      - reads in the multiple salary files (base salary and bonus, perhaps different files for USA and CANADA)\n      - subset only necessary columns\n      - do some basic preprocessing steps on the column names and content\n      - ...\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    ## list the existing salary files and store the list of file names in file_names_keys\n#     files = dbutils.fs.ls('/mnt/datalake/INPUT/SALARY/')\n#     file_names = {}\n#     iter = 0\n#     for i in files:\n#       file_names[i.name] = i.path\n#       iter = iter+1\n#     file_names_keys = list(file_names.keys())\n#     self.file_names_keys = file_names_keys\n\n#     # splitting the list of files on the basis of the year it belongs to\n#     sal_main = [x for x in file_names_keys if re.search(pattern='salary_main(.*?).xlsx', string=x)]\n#     sal_bonus = [x for x in file_names_keys if re.search(pattern='salary_bonus(.*?).txt', string=x)]\n#     sal_main_keys = [s.replace('.xlsx', '') for s in sal_main]\n#     sal_bonus_keys = [s.replace('.txt', '') for s in sal_bonus]\n\n#     # the dict to store the individual datasets (per year)\n#     salmain = {}\n#     salbonus = {}\n#     sal_main_years = [i.split('_')[3] for i in sal_main_keys]\n#     sal_bonus_years = [i.split('_')[3] for i in sal_bonus_keys]\n#     sal_main_zones = [i.split('_')[2] for i in sal_main_keys]\n#     sal_bonus_zones = [i.split('_')[2] for i in sal_bonus_keys]\n\n    # read in the files and store them in the main and bonus dictionaries\n    # here there is only one file for main salary and two bonus files in two formats. hence making it static codes.\n    # recommended to define proper governance around salary files and tweak the salary class accordingly\n    \n    # naz combined main file\n    salmain = self.helpers.xlsx_read('/dbfs/mnt/datalake/INPUT/SALARY/salary_main_NAZ_2018.xlsx')\n    \n    # usa bonus file\n    salbonus_naz = self.helpers.txt_read('/dbfs/mnt/datalake/INPUT/SALARY/salary_bonus_US_2018.txt', skiprows=37)\n    salbonus_naz = salbonus_naz.iloc[1:, 1:]\n    salbonus_naz.drop(salbonus_naz.columns[[-1,]], axis=1, inplace=True)\n    salbonus_naz = salbonus_naz[self.cols_to_keep]\n    salbonus_naz['year'] = 2018\n    salbonus_naz['zone'] = 'usa'\n    salbonus_naz.dropna(inplace=True, axis=0, how='all')\n    salbonus_naz.drop_duplicates(inplace=True)\n    salbonus_naz = salbonus_naz[salbonus_naz['pers_no_'] != 'Pers.No.']\n    salbonus_naz.pers_no_ = salbonus_naz['pers_no_'].astype(float)\n    \n    # canada bonus file\n    sal_bonus_can_excel = pd.ExcelFile('/dbfs/mnt/datalake/INPUT/SALARY/salary_bonus_CA_2018.xlsx')\n    sheets = len(sal_bonus_can_excel.sheet_names)\n    sal_bonus_can = {}\n    for i in range(sheets):\n      sal_bonus_can[i] = self.helpers.xlsx_read('/dbfs/mnt/datalake/INPUT/SALARY/salary_bonus_CA_2018.xlsx', sheet_name=i)\n      sal_bonus_can[i]['year'] = sal_bonus_can_excel.sheet_names[i]\n    sal_bonus_can_df = pd.concat(sal_bonus_can.values(), ignore_index=True, sort=False)\n    sal_bonus_can_df = sal_bonus_can_df[['employee_number', 'pay_run_earning_committed_amount_sum', 'period_start', 'year']]\n    sal_bonus_can_df['yearmonth'] = pd.to_datetime(sal_bonus_can_df['period_start']).dt.strftime('%Y%m')\n    salbonus_ca = sal_bonus_can_df.groupby(['employee_number', 'yearmonth'])['pay_run_earning_committed_amount_sum'].sum().to_frame(name='bonus').reset_index()\n       \n    # finally\n    self.salary_combined = salmain\n    self.salbonus_naz = salbonus_naz\n    self.salbonus_ca = salbonus_ca\n\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["salary = salary_class(helpers, sal_cols_to_keep)\nsalary.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(187756, 6)\n(89807, 24)\n(4163, 10)\n(3025, 10)\n(5042, 10)\n</div>"]}}],"execution_count":37},{"cell_type":"markdown","source":["#TARGET"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/TARGET"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/TARGET/target_combined.csv</td><td>target_combined.csv</td><td>1953820</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/TARGET/target_complete.xlsx</td><td>target_complete.xlsx</td><td>6090150</td></tr></tbody></table></div>"]}}],"execution_count":39},{"cell_type":"code","source":["# TARGET CLASS #\n\nclass target_class(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class is for reading in the target files\n    the target combined file read here is a prior-processed one. the function to generate it from \n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    # read in the files\n    target = pd.ExcelFile('/dbfs/mnt/datalake/INPUT/TARGET/target_complete.xlsx')\n    sheets = len(target.sheet_names)\n    self.target = target\n    \n    target_combined = self.helpers.csv_read('/dbfs/mnt/datalake/INPUT/TARGET/target_combined.csv')\n    target_combined = target_combined[self.cols_to_keep]\n    target_combined = self.helpers.process_columns(target_combined)\n    self.target_combined = target_combined\n    \n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"code","source":["target = target_class(helpers, target_cols_to_keep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["target.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(17656, 15)\n</div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["#TURNOVER FILES"],"metadata":{}},{"cell_type":"code","source":["%fs\n\nls /mnt/datalake/INPUT/TURNOVER/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/datalake/INPUT/TURNOVER/turnover-2016.csv</td><td>turnover-2016.csv</td><td>606800</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/TURNOVER/turnover_2017.csv</td><td>turnover_2017.csv</td><td>4790559</td></tr><tr><td>dbfs:/mnt/datalake/INPUT/TURNOVER/turnover_2018.csv</td><td>turnover_2018.csv</td><td>9033569</td></tr></tbody></table></div>"]}}],"execution_count":44},{"cell_type":"code","source":["# TURNOVER CLASS #\n\nclass to_class(object):\n  \n  def __init__(self, helper_funcs_instance, cols_to_keep):\n    \"\"\" \n    this class is for reading in the target files\n    \"\"\"\n    self.helpers = helper_funcs_instance\n    self.cols_to_keep = cols_to_keep\n    return None\n  \n  def main(self):\n    # read in the files\n    files = dbutils.fs.ls('/mnt/datalake/INPUT/TURNOVER/')\n    file_names = {}\n    iter = 0\n    for i in files:\n      file_names[i.name] = i.path\n      iter = iter+1\n    file_names_keys = list(file_names.keys())\n    self.file_names_keys = file_names_keys\n\n    # splitting the list of files on the basis of the format it has\n    to_csv = [x for x in file_names_keys if re.search(pattern='.csv', string=x)]\n    to_xlsx = [x for x in file_names_keys if re.search(pattern='.xlsx', string=x)]    \n    to_csv_keys = [re.sub(pattern='turnover_|turnover-|.csv', repl = '', string=s) for s in to_csv]\n    to_xlsx_keys = [re.sub(pattern='turnover_|turnover-|.xlsx', repl = '', string=s) for s in to_xlsx]\n\n    # the dict to store the individual datasets to append for later\n    to = {}\n\n    # read in the files\n    iter=0\n    for i in to_csv:\n        to[to_csv_keys[iter]] = self.helpers.csv_read(str('/dbfs/mnt/datalake/INPUT/TURNOVER/' + to_csv[iter]), dtype=str)\n        to[to_csv_keys[iter]] = to[to_csv_keys[iter]][self.cols_to_keep]\n        to[to_csv_keys[iter]] = self.helpers.process_columns(to[to_csv_keys[iter]])\n        to[to_csv_keys[iter]]['year'] = to_csv_keys[iter]\n        iter = iter+1\n    iter=0\n    for i in to_xlsx:\n        to[to_xlsx_keys[iter]] = self.helpers.xlsx_read(str('/dbfs/mnt/datalake/INPUT/TURNOVER/' + to_xlsx[iter]), dtype=str)\n        to[to_xlsx_keys[iter]] = to[to_xlsx_keys[iter]][self.cols_to_keep]\n        to[to_xlsx_keys[iter]] = self.helpers.process_columns(to[to_xlsx_keys[iter]])\n        to[to_xlsx_keys[iter]]['year'] = to_xlsx_keys[iter]\n        iter = iter+1\n    self.to = to\n    \n    to_combined = pd.concat(to.values(), ignore_index=True)\n    print('turnover combined original shape: ', to_combined.shape)\n    to_combined.drop_duplicates(inplace=True, subset=['employee_id'])\n    to_combined['employee_id'] = to_combined['employee_id'].astype(int)\n    print('turnover combined new shape after distinct: ', to_combined.shape)\n    self.to_combined = to_combined\n    \n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"code","source":["turnover = to_class(helpers, to_cols_to_keep)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"code","source":["turnover.main()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(1399, 42)\n(6160, 96)\n(10805, 102)\nturnover combined original shape:  (18364, 8)\nturnover combined new shape after distinct:  (12172, 8)\n</div>"]}}],"execution_count":47},{"cell_type":"markdown","source":["# END OF DATA PROCUREMENT\n&nbsp;\n_all the invidiual class instances are loaded into a super class that contains as attributes All the invidual files/list of files_"],"metadata":{}},{"cell_type":"code","source":["# global function to flatten columns after a grouped operation and aggregation\n# outside all classes since it is added as an attribute to pandas DataFrames\ndef __my_flatten_cols(self, how=\"_\".join, reset_index=True):\n  how = (lambda iter: list(iter)[-1]) if how == \"last\" else how\n  self.columns = [how(filter(None, map(str, levels))) for levels in self.columns.values] \\\n  if isinstance(self.columns, pd.MultiIndex) else self.columns\n  return self.reset_index(drop=True) if reset_index else self\npd.DataFrame.my_flatten_cols = __my_flatten_cols\n\n\n# the super class\nclass all_files:\n  \n  def __init__(self, blueprint, competency, opr, target, movement, salary, turnover, misc, bp_nlp_cols, helpers):\n    # initialize the various class instances into attributes of the super class\n    self.bpt = blueprint\n    self.cpy = competency\n    self.trg = target\n    self.opr = opr\n    self.mov = movement\n    self.sal = salary\n    self.tvr = turnover\n    self.misc = misc\n    self.bpt_nlp_cols = bp_nlp_cols\n    self.helper_functions = helpers\n    \n    # declare global variables\n    self.ps_group_list = ['vi_b', 'vii_a', 'vii_b', 'v_a', 'viii_a', 'iii_a', 'v_b', 'vi_a', 'iv_b',\n       'x_a', 'iv_a', 'viii_b', 'ii_b', 'ii_a', 'ix_b', 'ix_a', 'iii_b', 'i_b', 'i_a', 'x_b', 'xi_b', 'xi_a']\n    self.opr_dict = {'4a': 7, '4b': 6, '3b': 5, '3a': 4, '2': 3, '1a': 2, '1b': 1}\n    self.opr_dict_bucket = {'4a': 'high', '4b': 'high', '3b': 'medium', '3a': 'high', '2': 'medium', '1a': 'low', '1b': 'low'}\n    self.mov_rfa_good = ['promotion _ band up', 'promotion within band']\n    self.mov_rfa_neutral = ['lateral move', 'internal restructuring', 'local transfers', 'return to base location']\n    self.mov_rfa_bad = ['demotion', 'grandfathering', 'position reevaluation', 'end of grandfathering']\n    self.crcy_converter =  {'USD': 1, 'CNY': 0.15, 'TWD': 0.032, 'GBP': 1.26, 'HKD': 0.13, 'EUR': 1.13, 'MXN': 0.05, 'ARS': 0.026, 'SGD': 0.73,\n       'CAD': 0.75, 'BRL': 0.26}  # average ratios based on last 5 years of currency changes\n\n    \n    # call the various feature creation classes (adds the features as attributes of the class instance)\n    self.competency_features()\n    self.target_features()\n    self.opr_features()\n    self.movement_features()\n    self.blueprint_features()\n    self.turnover()\n    \n    # final function call\n    self.main()\n  ###########################################################################################################################\n  \n  # some helper class functions\n  def group_agg_feats(self, df, group_cols, agg_col, new_cols):\n    df_grp = df.groupby(group_cols, as_index=False).agg({agg_col: ['sum', 'mean']}).my_flatten_cols()\n    df_grp.columns = new_cols\n    df = df.merge(df_grp, how='left')\n    return df\n  \n  def group_cumsum_feats(self, df, group_cols, cumsum_cols, sort_col):\n    df_temp = df.sort_values(sort_col).groupby(group_cols, sort=False).sum().groupby(level=[0])[cumsum_cols].cumsum().reset_index().my_flatten_cols()\n    group_cols.extend(list(['cumsum_' + s for s in cumsum_cols]))\n    df_temp.columns = group_cols\n    df = df.merge(df_temp, how='left')\n    return df\n  \n  def group_cummean_feats(self, df, group_cols, cummean_col, sort_col):\n    for i in group_cols:\n      df.sort_values([i, sort_col], ascending = [True, True], inplace=True)\n      df.reset_index(drop=True, inplace=True)\n      new_col = str('cummean_' + i + '_' + cummean_col)\n      df[new_col] = df.groupby(i, sort=False)[cummean_col].expanding().mean().reset_index(drop=True)\n    return df\n\n  # below function to be tweaked. incorporate cummax/cummin version\n  def group_topn_feats(self, df, group_cols, sort_col, subset_cols, new_cols, npwhere_col, npwhere_list, which, flag_col, n=1):\n    # which = ['top', 'bottom']\n    df[sort_col] = df[sort_col].astype(float)\n    if which=='top':\n      #df['new'] = df.groupby('id').value.cummax()\n      df_temp = df.groupby(group_cols)[sort_col].nlargest(n)\n    elif which=='bottom':\n      df_temp = df.groupby(group_cols)[sort_col].nsmallest(n)\n    df_temp = df[subset_cols].loc[df_temp.index.get_level_values(1)].reset_index(drop=True)\n    df_temp.columns = new_cols\n    df_temp[flag_col] = np.where(df_temp[npwhere_col].isin(npwhere_list), 1, 0)\n    df_temp.drop(npwhere_col, inplace=True, axis=1)\n    df = df.merge(df_temp, how='left')\n    return df\n  \n  def date_converter(self, df):\n    date_cols = [col for col in df.columns if 'date' in col]\n    for i in date_cols:\n      df[i] = df[i].astype(str)\n      df[i] = df[i].str.replace(' +', ' ', regex=True)\n      df[i] = df[i].str.split(' ').str[0].astype(str)\n      df[i] = df[i].str.replace('[^\\w\\s]', '_', regex=True)\n      df[i+'_year'] = df[i].str.split('_').str[2].astype(str).apply(lambda s: '20'+s if len(s) == 2 else s)\n      df = df.loc[df[i+'_year'] != '0000']\n      df[i+'_month'] = df[i].astype(str).str.split('_').str[0].astype(str).apply(lambda s: '0'+s if len(s) == 1 else s)\n      df[i+'_day'] = df[i].astype(str).str.split('_').str[1].astype(float)\n      df[i+'_month'] = np.where(df[i+'_year'].astype(float).isin([99, 9999, 2099]), 12, df[i+'_month'])\n      df[i+'_day'] = np.where(df[i+'_year'].astype(float).isin([99, 9999, 2099]), 31, df[i+'_day'])\n\n      df[i+'_year'] = np.where(df[i+'_year'].astype(float).isin([99, 9999, 2099]), 2019, df[i+'_year'])\n      df[i+'_yearmonth'] = df[i+'_year'].map(str) + df[i+'_month'].astype(str)\n      df[i+'_year'] = df[i+'_year'].astype(float)\n      df[i+'_month'] = df[i+'_month'].astype(float)\n    return df\n  \n  def groupby_count(self, df, grp1_col, grp2_cols, count_col):\n    for i in grp2_cols:\n      col_name = str('count_' + count_col)\n      if 'l2' in i:\n        col_name = 'team_l2_size'\n      else:\n        col_name = 'team_size'\n      grp_cols = [grp1_col, i]\n      df2 = df.groupby(grp_cols)[count_col].nunique().to_frame(name=col_name).reset_index()\n      df = df.merge(df2, how='left')\n    return df\n  \n  def groupby_compare(self, df, grp_cols, grp_fixed_cols, transform_col):\n    for i in grp_cols:\n      all_grp_cols = grp_fixed_cols.copy()\n      if i is not None: all_grp_cols.extend([i])\n      df1 = df.groupby(all_grp_cols)[transform_col].transform('size').sub(1)\n      df2 = df.groupby(all_grp_cols)[transform_col].transform('sum').sub(df[transform_col])\n      if i is not None: new_colname = str('compare_ratio_' + transform_col + '_atlevel_' + i)\n      if i is None: new_colname = str('compare_ratio_' + transform_col + 'atlevel_psg_yearmonth')\n      df[new_colname] = df2/df1\n      df[new_colname] = df[transform_col]/df[new_colname]\n    return df\n  \n  def fillna_df(self, df, fill_cols, mode, grp_col=None, abs_value=None, ref_value_col=None, ref_value_col_frac=None):\n    \"\"\" mode = ['simple_abs', 'simple_ref', 'adv_fill']\n    \"\"\"\n    for i in fill_cols:\n      if mode=='simple_abs':\n        df[i] = df[i].astype(float)\n        df[i].fillna(value=abs_value, inplace=True)\n      elif mode=='simple_ref':\n        df[i].fillna(value=df[ref_value_col], inplace=True)\n      elif mode=='adv_fill':\n        df[i] = df.groupby(grp_col)[i].transform(lambda x: x.ffill())\n        df['flagna'] = df[i].isnull()\n        df[i] = df.groupby(grp_col)[i].transform(lambda x: x.bfill())\n        if ref_value_col is not None: df[i] = np.where(df['flagna']==1, df[i]-(ref_value_col_frac*df[i]), df[i])\n        if abs_value is not None: df[i] = np.where(df['flagna']==1, df[i]-abs_value, df[i])\n        df.drop('flagna', axis=1, inplace=True)\n        df.reset_index(inplace=True, drop=True)\n    return df\n  \n  ###########################################################################################################################\n  \n  def competency_features(self):\n    \"\"\" the major features required have been created at several levels:\n        - personnel number + year\n        - personnel number + year + competency_group\n        - personnel number + year + competency_group + competency_group_type_l1\n        the features being:\n        - sum of manager_rating___numeric_value\n        - mean of manager_rating___numeric_value\n    \"\"\"\n    comp = self.cpy\n    \n    # find below some very rough code to clean, process and create some features for competency\n    x = comp.competency_combined.copy()\n    x.dropna(axis=0, subset=['manager_rating___numeric_value'], inplace=True)\n    x = x[~x['manager_rating___numeric_value'].isin(['not rated', 'not applicable'])]\n    x.manager_rating___numeric_value = x.manager_rating___numeric_value.astype('float')\n    x['competency_group_type_l1'] = x['competency'].str.split(' _ ').str[0]\n    x['competency_group_type_l2'] = x['competency'].str.split(' _ ').str[1]\n    \n    # creating the grouped dfs and merging the new features\n    x = self.group_agg_feats(x, group_cols=['personnel_number', 'year'], agg_col='manager_rating___numeric_value',\n                             new_cols=['personnel_number', 'year', 'pers_year_comp_score_sum', 'pers_year_comp_score_mean'])\n    x = self.group_agg_feats(x, group_cols=['personnel_number', 'competency_group', 'year'], agg_col='manager_rating___numeric_value',\n                             new_cols=['personnel_number', 'competency_group', 'year', 'pers_compgroup_year_comp_score_sum', 'pers_compgroup_year_comp_score_mean'])\n    # commenting out the below feature creation pending good EDA on the comp L2 level sparsity and information\n    #x = self.group_agg_feats(x, group_cols=['personnel_number', 'competency_group', 'competency_group_type_l1', 'year'], agg_col='manager_rating___numeric_value', new_cols=['personnel_number', 'competency_group', 'competency_group_type_l1', 'year', 'pers_compgroupl1_year_comp_score_sum', 'pers_compgroupl1_year_comp_score_mean'])\n    \n    x = x[['personnel_number', 'year', 'competency_group', 'pers_year_comp_score_sum', 'pers_year_comp_score_mean', 'pers_compgroup_year_comp_score_sum', 'pers_compgroup_year_comp_score_mean']]\n    x.drop_duplicates(inplace=True)\n    \n    # post-processing and final competency features added to the instance\n    xx = x.copy()\n    xx = xx[['personnel_number', 'year', 'competency_group', 'pers_compgroup_year_comp_score_sum', 'pers_compgroup_year_comp_score_mean']]\n    xx.drop_duplicates(inplace=True)\n    xx = xx.groupby(['personnel_number', 'year', 'competency_group']).sum().unstack('competency_group').reset_index().my_flatten_cols()\n    x = x.merge(xx, how='left', on=['personnel_number', 'year'])\n    x.drop(['competency_group'], axis=1, inplace=True)\n    x.drop_duplicates(inplace=True, subset=['personnel_number', 'year'])\n    x.rename(columns={'year': 'comp_year', 'personnel_number': 'employee_personnal_number_pa'}, inplace=True)\n\n    x['employee_personnal_number_pa'] = x['employee_personnal_number_pa'].astype(float)\n    x['comp_year'] = x['comp_year'].astype(int)\n    self.comp_features = x\n    return None\n  \n  ###########################################################################################################################\n  \n  def target_features(self):\n    \"\"\" the major features are group aggregation features grouping on:\n            - ['personnel_number']\n            - ['personnel_number', 'year']\n            - ['personnel_number_of_appraiser', 'year']\n        using [net_target] as the aggregation factor and agg functions being [sum, mean]\n    \"\"\"    \n    # find below some very rough code to clean, process and create some features for target\n    x = self.trg.target_combined.copy()\n    x.columns = ['employee_personnal_number_pa', 'personnel_number_of_appraiser', 'net_target' ,'target_year']\n    \n    x = self.group_cummean_feats(x, group_cols=['employee_personnal_number_pa', 'personnel_number_of_appraiser'], cummean_col='net_target', sort_col='target_year')\n    x = x[['employee_personnal_number_pa', 'net_target', 'target_year', 'cummean_employee_personnal_number_pa_net_target', 'cummean_personnel_number_of_appraiser_net_target']]\n    \n    self.target_feats = x\n    return None\n  \n  ###########################################################################################################################\n  \n  def opr_features(self):\n    \"\"\"\n    contains a dictionary for mapping between opr values and their relative scale\n    \"\"\"   \n    x = self.opr.opr_combined.copy()\n    \n    # some preprocessing\n    x.dropna(axis=0, how='any', inplace=True)\n    x['opr_value'] = x['opr_rating_scale'].map(self.opr_dict)\n    x['opr_bucket'] = x['opr_rating_scale'].map(self.opr_dict_bucket)\n    x.year = x.year.astype('int')\n    \n    x.rename(columns={'year': 'opr_year', 'personnel_number': 'employee_personnal_number_pa'}, inplace=True)\n    self.opr_feats = x\n    return None\n  \n  ###########################################################################################################################\n  \n  def movement_features(self):\n    \"\"\" movement is defined as any position/band/location changes\n    features:\n      - mov_rfa_quality features (count, recent_flag)\n      - lateral move (count, recent_flag)\n      - internal restructuring (count, recent_flag)\n    \"\"\"    \n    x = self.mov.movement_combined.copy()\n\n    # some pre-processing\n    x.rename(columns={'pers_no_': 'personnel_number'}, inplace=True)\n    x.drop_duplicates(subset=(['personnel_number', 'position', 'ps_group', 'start_date', 'reason_for_action']), inplace=True)\n    x.dropna(axis=0, subset=['personnel_number', 'start_date'], how='any', inplace=True)\n    x['mov_rfa_good'] = np.where(x['reason_for_action'].isin(self.mov_rfa_good), 1, 0)\n    x['mov_rfa_neutral'] = np.where(x['reason_for_action'].isin(self.mov_rfa_neutral), 1, 0)\n    x['mov_rfa_bad'] = np.where(x['reason_for_action'].isin(self.mov_rfa_bad), 1, 0)\n    x['mov_rfa_score'] = np.where(x['reason_for_action'].isin(self.mov_rfa_good), 2,\n                                 np.where(x['reason_for_action'].isin(self.mov_rfa_neutral), 1, -1))\n    x = self.date_converter(x)\n    x.sort_values(['personnel_number', 'start_date_yearmonth', 'end_date_yearmonth'], ascending=[True, True, True], inplace=True)\n    x.reset_index(drop=True, inplace=True)\n    \n    # creating the cumulative movements (qualitative and quantitative) for each employee+year+month (startdate as reference)\n    x = self.group_cumsum_feats(x, group_cols = ['personnel_number', 'start_date_yearmonth'], sort_col='start_date_yearmonth',\n                                cumsum_cols = ['mov_rfa_good', 'mov_rfa_neutral', 'mov_rfa_bad', 'mov_rfa_score'])\n    # other features\n    # expanding top/worst mov_rfa_score\n    # rolling (window=4) mean mov_rfa_score \n    \n    self.move_features = x\n    return None\n  \n  ###########################################################################################################################\n  \n  def salary_features(self):\n    x1 = self.sal.salbonus_ca.copy()\n    x2 = self.sal.salbonus_naz.copy()\n    \n    # process and return the final combined bonus salary dataset\n    x1.columns = ['employee_personnal_number_pa', 'yearmonth', 'bonus']\n    x1['crcy'] = 'CAD'\n    x2 = x2[['pers_no_', 'for_period', 'amount', 'crcy']]\n    x2.columns = ['employee_personnal_number_pa',  'yearmonth', 'bonus', 'crcy']\n    x_bonus = x1.append(x2, ignore_index=True)\n    x_bonus.sort_values(['employee_personnal_number_pa', 'yearmonth'], inplace=True)\n    x_bonus = x_bonus.apply(lambda x: x.str.strip() if (x.dtype == 'object') else x)\n    x_bonus = x_bonus.apply(lambda x: x.str.replace('-|,', '') if (x.dtype == 'object') else x)\n    x_bonus['bonus'] = x_bonus['bonus'].astype(float)\n    x_bonus = x_bonus.loc[(x_bonus['bonus'] > 0.0) & (x_bonus['yearmonth'] != 0)]\n    x_bonus['employee_personnal_number_pa'] = x_bonus['employee_personnal_number_pa'].astype(float)\n    x_bonus['yearmonth'] = x_bonus['yearmonth'].astype(int)\n    x_bonus['crcy_rate'] = x_bonus['crcy'].map(self.crcy_converter)\n    x_bonus['bonus'] = x_bonus.bonus * x_bonus.crcy_rate\n    x_bonus = x_bonus[['employee_personnal_number_pa', 'yearmonth', 'bonus']]\n    \n    # process and return the final combined main salary dataset\n    x = salary.salary_combined.copy()\n    x['start_date'] = x['start_date'].dt.strftime('%Y%m')\n    x.dropna(inplace=True, axis=0, subset=['persno', 'start_date', 'annual_salary'], how='any')\n    x = x[['persno', 'annual_salary', 'crcy', 'start_date']]\n    x.sort_values(['persno', 'start_date'], inplace=True)\n    x['annual_salary'] = x['annual_salary'].astype(float)\n    x = x.loc[(x['annual_salary'] > 0.0) & (x['start_date'] != 0)]\n    x['start_date'] = np.where(x['start_date'].astype(int) < int(self.min_yearmonth), int(self.min_yearmonth), x['start_date'].astype(int))\n    x.columns = ['employee_personnal_number_pa', 'salary', 'crcy', 'yearmonth']\n    x.drop_duplicates(subset=['employee_personnal_number_pa', 'yearmonth'], keep='last', inplace=True)\n    x['employee_personnal_number_pa'] = x['employee_personnal_number_pa'].astype(float)\n    x['yearmonth'] = x['yearmonth'].astype(int)\n    x['crcy_rate'] = x['crcy'].map(self.crcy_converter)\n    x['salary'] = x.salary * x.crcy_rate\n    x = x[['employee_personnal_number_pa', 'yearmonth', 'salary']]\n    \n    # merge the final bonus and main salary files\n    x = x.merge(x_bonus, how='left', on=['employee_personnal_number_pa', 'yearmonth'])\n    self.sal_features = x\n    return None\n  \n  ###########################################################################################################################\n  \n  def misc_features(self):\n    x = self.misc.demo.copy()\n    x['date_of_birth'] = pd.to_datetime(x['date_of_birth'], infer_datetime_format=True)\n    x['original_hire_date'] = pd.to_datetime(x['original_hire_date'], infer_datetime_format=True)\n    x['date_of_birth'] = np.where(x['date_of_birth'].dt.year.astype(float) > 2000, \n                                    x['date_of_birth'] - pd.DateOffset(years=100), x['date_of_birth'])\n    x['original_hire_date'] = np.where(x['original_hire_date'].dt.year.astype(float) > 2020, \n                                    x['original_hire_date'] - pd.DateOffset(years=100), x['original_hire_date'])\n    x2 = x.groupby(['personnel_number', 'date_of_birth', 'original_hire_date']).size().to_frame(name='count').reset_index()\n    x2.sort_values(['personnel_number', 'count'], ascending=[True, False], inplace=True)\n    x2.drop_duplicates(subset=['personnel_number'], inplace=True)\n    x2.drop(['count'], axis=1, inplace=True)\n    x2.columns = ['employee_personnal_number_pa', 'DOB', 'OHD']\n    \n    self.misc_feats = x2\n    return None\n  \n  ###########################################################################################################################\n  \n  def blueprint_features(self):\n    \"\"\" the biggest of all the classes. contains all the major hierarchy features and also is the mother dataset to all others (each one will be merged into this one after the other)\n    \"\"\"\n    x1 = self.bpt.dict16.copy()\n    x2 = self.bpt.dict17.copy()\n    x3 = self.bpt.dict18.copy()\n    x11 = pd.concat(x1.values(), ignore_index=True)\n    x21 = pd.concat(x2.values(), ignore_index=True)\n    x31 = pd.concat(x3.values(), ignore_index=True)\n    x = x11.append(x21, ignore_index=True)\n    x = x.append(x31, ignore_index=True)\n    \n    # some custom preprocessing for the blueprint files (not modularised because not applicable elsewhere)\n    ## remove people with employment status being = 1\n    x = x[x['employment_status_pa']!=1]\n    x.drop('employment_status_pa', axis=1, inplace=True)\n    ## drop rows with nas (conservative on id columns, liberal on band/group columns)\n    x.dropna(subset=['employee_personnal_number_pa', 'employee_global_id_pa', 'position_id_om'], how='any', inplace=True, axis=0)\n    x.dropna(subset=['pay_grade_group_pa', 'position_band_om'], how='all', inplace=True, axis=0)\n    ## fill nas for the two band/group columns based on each other (cases of both nas are removed prior)\n    x = self.fillna_df(x, fill_cols=['position_band_om'], mode='simple_ref', ref_value_col='pay_grade_group_pa')\n    x = self.fillna_df(x, fill_cols=['pay_grade_group_pa'], mode='simple_ref', ref_value_col='position_band_om')\n    ## filter for banded employees (based on pay group)\n    x = x.loc[x['pay_grade_group_pa'].isin(self.ps_group_list)]\n    ## create the monthly flag column and process the date columns available\n    x['yearmonth'] = x['term_year'].map(str) + x['term_month_temp'].astype(str)\n    x.position_start_date_om = pd.to_datetime(x.position_start_date_om)\n    x = self.helper_functions.nlp_process_columns(x, self.bpt_nlp_cols)\n    x['monthly_file_date'] = pd.to_datetime(x['term_year'].astype(str) + x['term_month_temp'].astype(str), format='%Y%m')\n    ## treat the position start date\n    x = self.fillna_df(x, fill_cols=['position_start_date_om'], mode='adv_fill', grp_col='employee_personnal_number_pa')\n\n    self.min_yearmonth = np.datetime64(x.yearmonth.min())\n    self.min_yearmonthfile = np.datetime64(x.monthly_file_date.min())\n    # calling the miscellaneous and salary features functions\n    self.misc_features()\n    self.salary_features()\n    \n    ## add the misc features to the bp_combined dataset\n    x = x.merge(self.misc_feats, how='left')\n    \n    ####################################################################################################################\n    # MANAGER FEATURES\n    x_m = x[['manager_s_global_id_pa', 'manager_s_position_id_om', 'manager_s_position_name_om', 'employee_global_id_pa',\n             'employee_personnal_number_pa', 'pay_grade_group_pa', 'org_unit_id_of_position_om', 'position_band_om',\n            'DOB', 'OHD', 'monthly_file_date', 'position_start_date_om', 'position_work_location_code_om']]\n    man_list = x.manager_s_global_id_pa.astype(int).unique()\n    x_m = x_m.loc[x_m['employee_global_id_pa'].isin(man_list)]\n    x_m.columns = ['manager_s_global_id_pa_l2', 'manager_s_position_id_om_l2', 'manager_s_position_name_om_l2', 'manager_s_global_id_pa', 'manager_s_personnal_number_pa', 'manager_s_pay_grade_group_pa', 'manager_s_org_unit_id_of_position_om', 'manager_s_position_band_om',\n                  'manager_s_DOB', 'manager_s_OHD', 'monthly_file_date', 'manager_s_position_start_date_om', 'manager_s_position_work_location_code_om']\n    x_m.drop_duplicates(inplace=True)\n    x = x.merge(x_m, how='left')\n    \n    # features applicable for both employee and manager\n    ## create the position tenure features\n    x['position_start_date_om'] = np.where(x['position_start_date_om'] > x['monthly_file_date'], self.min_yearmonthfile, x['position_start_date_om'])\n    x['manager_s_position_start_date_om'] = np.where(x['manager_s_position_start_date_om'] > x['monthly_file_date'], self.min_yearmonthfile, x['manager_s_position_start_date_om'])\n    x['position_tenure'] = (x.monthly_file_date - x.position_start_date_om).astype('timedelta64[D]')\n    x['manager_s_position_tenure'] = (x.monthly_file_date - x.manager_s_position_start_date_om).astype('timedelta64[D]')\n    x['position_tenure_diff'] = (x.manager_s_position_tenure - x.position_tenure).astype(float)\n    ## create the remote location flag\n    x['remote_flag'] = np.where(x['position_work_location_code_om'].str.contains('remote', case=False, na=False), 1, 0)\n    x['manager_s_remote_flag'] = np.where(x['manager_s_position_work_location_code_om'].str.contains('remote', case=False, na=False), 1, 0)\n    ## create the age features\n    x['employee_age'] = (x.monthly_file_date - x.DOB).astype('timedelta64[D]')\n    x['manager_s_age'] = (x.monthly_file_date - x.manager_s_DOB).astype('timedelta64[D]')\n    x['age_diff'] = (x.manager_s_age - x.employee_age).astype(float)\n    ## create the tenure features\n    x['employee_tenure'] = (x.monthly_file_date - x.OHD).astype('timedelta64[D]')\n    x['manager_s_tenure'] = (x.monthly_file_date - x.manager_s_OHD).astype('timedelta64[D]')\n    # treat the negative cases that came in due to OHD data issues\n    x['employee_tenure'] = np.where(x['employee_tenure'] < 0, x['position_tenure'], x['employee_tenure'])\n    x['manager_s_tenure'] = np.where(x['manager_s_tenure'] < 0, x['position_tenure'], x['manager_s_tenure'])\n    x['tenure_diff'] = (x.manager_s_tenure - x.employee_tenure).astype(float)\n    ## create the org unit id comparison binary flag\n    x['orgunitid_employee_isdifferentfrom_manager'] = np.where(x['org_unit_id_of_position_om'] == x['manager_s_org_unit_id_of_position_om'], 0, 1)\n    \n    ####################################################################################################################\n    \n    # TEAM FEATURES\n    x = self.groupby_count(x, grp1_col='yearmonth', grp2_cols=['manager_s_global_id_pa', 'manager_s_global_id_pa_l2', 'org_unit_id_of_position_om',\n                                                         'pay_grade_group_pa', 'cost_center_id_om', 'local_entity_code'], \n                      count_col = 'employee_personnal_number_pa')\n    \n    ####################################################################################################################\n    \n    # adding the salary features\n    x['employee_personnal_number_pa'] = x['employee_personnal_number_pa'].astype(float)\n    x['yearmonth'] = x['yearmonth'].astype(int)\n    x = x.merge(self.sal_features, how='left', on=['employee_personnal_number_pa', 'yearmonth'])\n    \n    x = self.fillna_df(x, fill_cols=['salary'], mode='adv_fill', grp_col='employee_personnal_number_pa', ref_value_col_frac=0.1)\n    x = self.fillna_df(x, fill_cols=['bonus'], mode='simple_abs', abs_value=0)\n    \n    x = self.groupby_compare(x, grp_cols=['org_unit_id_of_position_om', 'macro_entity_level_4_om', 'ab__inbev_entity_level_3_om', 'physical_work_location_city_pa', 'global_job_om', 'job_family_om', 'functional_area_om', 'cost_center_id_om', 'local_entity_code', None], grp_fixed_cols=['yearmonth', 'pay_grade_group_pa'], transform_col='salary')\n    x = self.groupby_compare(x, grp_cols=['org_unit_id_of_position_om', 'macro_entity_level_4_om', 'ab__inbev_entity_level_3_om', 'physical_work_location_city_pa', 'global_job_om', 'job_family_om', 'functional_area_om', 'cost_center_id_om', 'local_entity_code', None], grp_fixed_cols=['yearmonth', 'pay_grade_group_pa'], transform_col='bonus')\n    \n    ####################################################################################################################\n    \n    # adding the movement features\n    bp_move = self.move_features\n    bp_move = bp_move[['personnel_number', 'start_date_yearmonth', 'mov_rfa_good', 'mov_rfa_neutral', 'mov_rfa_bad', 'mov_rfa_score',\n                      'cumsum_mov_rfa_good', 'cumsum_mov_rfa_neutral', 'cumsum_mov_rfa_bad', 'cumsum_mov_rfa_score']]\n    bp_move.columns = ['employee_personnal_number_pa', 'yearmonth', 'mov_rfa_good', 'mov_rfa_neutral', 'mov_rfa_bad', 'mov_rfa_score',\n                      'cumsum_mov_rfa_good', 'cumsum_mov_rfa_neutral', 'cumsum_mov_rfa_bad', 'cumsum_mov_rfa_score']\n    bp_move.drop_duplicates(subset=['employee_personnal_number_pa', 'yearmonth'], inplace=True, keep='first')\n    bp_move['employee_personnal_number_pa'] = bp_move['employee_personnal_number_pa'].astype(float)\n    bp_move['yearmonth'] = bp_move['yearmonth'].astype(int)\n    # corresponding features for the manager\n    bp_move_manager = bp_move[['employee_personnal_number_pa', 'yearmonth', 'mov_rfa_score', 'cumsum_mov_rfa_score']].copy()\n    bp_move_manager.columns = ['manager_' + x for x in bp_move_manager.columns]\n    bp_move_manager.rename(columns={'manager_employee_personnal_number_pa': 'manager_s_personnal_number_pa',\n                                   'manager_yearmonth': 'yearmonth'}, inplace=True)\n    \n    # merge the employee level features and manager level features\n    x = x.merge(bp_move, how='left', on=['employee_personnal_number_pa', 'yearmonth'])\n    x = x.merge(bp_move_manager, how='left', on=['manager_s_personnal_number_pa', 'yearmonth'])\n    \n    x = self.fillna_df(x, fill_cols=['mov_rfa_good', 'mov_rfa_neutral', 'mov_rfa_bad', 'mov_rfa_score', 'manager_mov_rfa_score'], mode='simple_abs', abs_value=0)\n    x = self.fillna_df(x, fill_cols=['cumsum_mov_rfa_good', 'cumsum_mov_rfa_neutral', 'cumsum_mov_rfa_bad', 'cumsum_mov_rfa_score', 'manager_cumsum_mov_rfa_score'], mode='adv_fill', grp_col=['employee_personnal_number_pa'], abs_value=1)\n    \n    ####################################################################################################################\n    \n    # adding the opr features\n    x['opr_year'] = np.where(x['term_month_temp'].astype(int) < 9, x['term_year']-1, x['term_year'])\n    x = x.merge(self.opr_feats, how='left', on=['employee_personnal_number_pa', 'opr_year'])\n    # adding the manager opr features\n    manager_opr = self.opr_feats\n    manager_opr.columns = ['manager_' + x for x in manager_opr.columns]\n    manager_opr.rename(columns={'manager_employee_personnal_number_pa': 'manager_s_personnal_number_pa',\n                                   'manager_opr_year': 'opr_year'}, inplace=True)\n    x = x.merge(manager_opr, how='left', on=['manager_s_personnal_number_pa', 'opr_year'])\n    \n    ####################################################################################################################\n    \n    # adding the target features\n    x['target_year'] = np.where(x['term_month_temp'].astype(int) < 4, x['term_year']-2, x['term_year']-1)\n    x = x.merge(self.target_feats, how='left', on=['employee_personnal_number_pa', 'target_year'])\n    # adding the manager target features\n    manager_target = self.target_feats\n    manager_target.columns = ['manager_' + x for x in manager_target.columns]\n    manager_target.rename(columns={'manager_employee_personnal_number_pa': 'manager_s_personnal_number_pa',\n                                   'manager_target_year': 'target_year'}, inplace=True)\n    x = x.merge(manager_target, how='left', on=['manager_s_personnal_number_pa', 'target_year'])\n    \n    ####################################################################################################################\n    \n    # adding the competency features\n    x['comp_year'] = np.where(x['term_month_temp'].astype(int) < 7, x['term_year']-1, x['term_year'])\n    x = x.merge(self.comp_features, how='left', on=['employee_personnal_number_pa', 'comp_year'])\n    # adding the manager comp features\n    manager_comp = self.comp_features\n    manager_comp.columns = ['manager_' + x for x in manager_comp.columns]\n    manager_comp.rename(columns={'manager_employee_personnal_number_pa': 'manager_s_personnal_number_pa',\n                                   'manager_comp_year': 'comp_year'}, inplace=True)\n    x = x.merge(manager_comp, how='left', on=['manager_s_personnal_number_pa', 'comp_year'])\n    \n    self.bp_features = x\n    return None\n  ###########################################################################################################################\n  \n  def turnover(self):\n    # write the turnover file\n    to_df = self.tvr.to_combined.copy()\n    to_df = self.date_converter(to_df)\n    self.to_df = to_df\n    \n    to_df.to_csv('/dbfs/mnt/datalake/OUTPUT/turnover_labels_df.csv', index=False)\n    return None\n    \n  ###########################################################################################################################\n    \n  def main(self):\n    ads = self.bp_features\n    ads.drop(['org_unit_id_of_position_om', 'position_id_om', 'ab__inbev_entity_level_4_om', 'position_start_date_om',\n             'manager_s_global_id_pa', 'manager_s_position_id_om', 'month_file', 'term_year', 'term_month_temp', 'monthly_file_date',\n             'manager_s_global_id_pa_l2', 'manager_s_position_id_om_l2', 'manager_s_personnal_number_pa', 'manager_s_org_unit_id_of_position_om',\n             'manager_s_DOB', 'DOB', 'opr_year', 'target_year', 'comp_year'], axis=1, inplace=True)\n    ads.dropna(subset=['employee_personnal_number_pa', 'pay_grade_group_pa'], inplace=True, how='any')\n    ads.dropna(subset=['opr_value', 'net_target', 'pers_year_comp_score_sum'], inplace=True, how='all')\n    # add filter to remove people/records with 1B OPR value\n    ads = ads[ads['opr_rating_scale']!='1b']\n    self.ads = ads\n    ads.set_index('yearmonth', inplace=True)\n    ads_dict = {x: ads.loc[x] for x in ads.index.unique()}\n\n    # write the individual monthly datasets into the output folder\n    for i in ads_dict.keys():\n      ads_temp = ads_dict[i]\n      ads_temp.to_csv(str('/dbfs/mnt/datalake/OUTPUT/MONTHLY_DATASETS/' + str(i) + '.csv'), index=False)\n    return None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":49},{"cell_type":"code","source":["all = all_files(blueprint=bp, competency=comp, opr=opr, target=target, movement=movement, salary=salary, turnover=turnover, misc=misc,\n               bp_nlp_cols=['position_name_om', 'physical_work_location_city_pa', 'position_work_location_code_om', 'manager_s_position_name_om',\n                           'physical_work_location_description_pa'], helpers = helpers)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["#opr_df=opr.opr_combined.copy()\n#opr_df.to_csv('/dbfs/mnt/datalake/OUTPUT/oprdf.csv', index=False)\nbpp_df=all.ads.copy()\nbpp_df.to_csv('/dbfs/mnt/datalake/WORKING/bppdf.csv', index=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["%whos blueprint competency opr_class target_class movement_class salary_class to_class misc_class"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Variable   Type              Data/Info\n--------------------------------------\nbp         blueprint         &lt;__main__.blueprint object at 0x7f82f5e52ac8&gt;\ncomp       competency        &lt;__main__.competency object at 0x7f828b0dcba8&gt;\nmisc       misc_class        &lt;__main__.misc_class object at 0x7f8282852f28&gt;\nmovement   movement_class    &lt;__main__.movement_class &lt;...&gt;object at 0x7f829064b898&gt;\nopr        opr_class         &lt;__main__.opr_class object at 0x7f828c2de550&gt;\nsalary     salary_class      &lt;__main__.salary_class object at 0x7f82887e4748&gt;\ntarget     target_class      &lt;__main__.target_class object at 0x7f82887e4b00&gt;\nturnover   to_class          &lt;__main__.to_class object at 0x7f8289a376d8&gt;\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["# '2016 blueprint files', all.bpt.dict16.keys()\n# '2017 blueprint files', all.bpt.dict17.keys()\n# 'competency files', all.cpy.competency.keys()\n# 'competency combined', all.cpy.competency_combined.keys()\n# 'demographics', all.misc.demo.keys()\n# 'movement files', all.mov.movement.keys()\n# 'movement combined', all.mov.movement_combined.keys()\n# 'opr combined', all.opr.opr_processed.keys()\n# 'salary main combined', all.sal.salary_combined.keys()\n# 'salary bonus combined', all.sal.salary_bonus_combined.keys()\n# 'target combined', all.trg.target_combined.keys()\n# 'turnover combined', all.tvr.to_combined.keys()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":"2"},"version":"2.7.14","nbconvert_exporter":"python","file_extension":".py"},"name":"1. Raw_ADS_preparation","notebookId":2367262829641267},"nbformat":4,"nbformat_minor":0}
