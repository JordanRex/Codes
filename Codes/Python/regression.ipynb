{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION QUESTION BANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What family of supervised models do regression models come under? What are the major statistical models in that family?\n",
    "    - GLMs (generalized linear models) is the family\n",
    "    - Major models\n",
    "        1. Linear regression\n",
    "            - regularized ones are ridge and lasso\n",
    "        2. Logistic regression\n",
    "        3. Poisson regression\n",
    "        4. Bayesian regression\n",
    "        5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- What are the basic assumptions made for linear regression analysis?\n",
    "    1. linear relationship between dependant variable and the independant features\n",
    "    2. Multivariate normality (not so important)\n",
    "    3. No or little multicollinearity (correlated features)\n",
    "    4. No auto-correlation (lagged form of features)\n",
    "    5. Homoscedasticity (low variance in dependant variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- Talk about the linear regression modelling functions in scikit-learn.\n",
    "- what is the module in sklearn, commonly used models in them, major parameters used while fitting the model?\n",
    "    1. all part of the sklearn.linear_model module\n",
    "    2. common algos\n",
    "        - linear regression\n",
    "        - logistic regression\n",
    "        - lasso/ridge\n",
    "    3. parameters = trick question\n",
    "        - no user tunable parameters\n",
    "        - only one argument to specify if the intercept needs to be fit or not, and another to specify if to normalize\n",
    "            - usually assumed that the user has centered the data already (normalize=False) and intercept needs to be fit (fit_intercept=True)\n",
    "        - very few to no contexts exist where the intercept shouldnt be fit (follow up question with regards to this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "_Difficulty - Medium_\n",
    "\n",
    "- Why is Logistic regression considered part of regression (isnt it classification?). How is it used as a classifier then?\n",
    "    - logistic regression is not truly a classifier\n",
    "    - it does risk estimation, regressing probabilities, not actual class labels\n",
    "    - it is a manual task of assigning a decision threshold (heuristic based) to the probabilities to get class labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "_Difficulty - Hard_\n",
    "\n",
    "- Are regression techniques used only to model a continuous dependant variable? If not what are the other tasks under regression?\n",
    "    - ranking (poisson)\n",
    "    - classification (logistic, sgd=stochastic gradient descent)\n",
    "    - forecasting (time series based methods like ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "_Difficulty - Hard_\n",
    "\n",
    "- What are the different estimation methods used for estimating the beta coefficients in linear regression?\n",
    "    1. Least squares (like ordinary least squares - OLS)\n",
    "    2. Maximum likehihood methods (MLE, ridge, lasso)\n",
    "    3. Mixed effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- Explain regularization in linear regression in the form of its basic components.\n",
    "- How does the equation for the loss function look like in linear regression with regularization and an OLS estimator? (what the model tries to minimize)\n",
    "    - model => loss(_to be minimized_) = OLS + Lnorm(L1 or L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "_Difficulty - Hard_\n",
    "\n",
    "- What are the regularization techniques in regression?\n",
    "    1. lasso (L1 norm) -> lambda * sum(absolute of coefficient) -> __(λ* E(|β|))__\n",
    "        - penalizes only the high coefficients\n",
    "        - does feature selection by making low value predictors' coefficients zero\n",
    "    2. ridge (L2 norm) -> lambda * sum(squared coefficient) -> __(λ * E[(β)]&sup2;)__\n",
    "        - not robust to scale, hence features need to be scaled\n",
    "        - reduces coefficients' values to close to zero, but not exactly zero\n",
    "            - hence retains all features (cannot be used for feature selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "_Difficulty - Hard_\n",
    "\n",
    "1. Basic explanation for R&sup2;\n",
    "    - it is the coefficient of determination used to validate performance of a regression model\n",
    "    - it is the proportion of the variance in the dependent variable that is predictable from the independent variables\n",
    "2. What does it mean to have negative R&sup2;? What is the analogy in the classification context>\n",
    "    - when the model is doing worse than if you simply predicted all values as the mean value of the dependant variable\n",
    "    - similar to classification when AUC < 0.5, i.e. the model being worse than a 50% probability random classifier\n",
    "3. Is it a good idea to use R&sup2; when you have a lotof predictors? What is the alternative?\n",
    "    - It is not. R&sup2; increases with increasing predictors\n",
    "    - Should alternatively used adjusted R&sup2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "- Between the following, which is the most informative statistic to determine the degree of the relationship between a feature and the dependant variable\n",
    "    1. correlation (pearsons)\n",
    "    2. p-value for the null hypothesis that the ß=0\n",
    "    3. t-statistics for the null hypothesis that the ß=0\n",
    "\n",
    "__correlation. the remaining can give a significant value even if it is only a weak effect since it is only statistical significane. correlation talks about the absolute quanta of relationship (0.9 value usually translates to the feature being powerful alone in predicting the dependant variable)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "1. Which is more prone to overfitting of the two -> Linear and Polynomial regression? \n",
    "    - Polynomial regression\n",
    "    - since it can curve to fit better on tha training data (which the straight line in linear regression cannot) it can easily end up overfitting to the train data\n",
    "    \n",
    "2. Why and how to control it in the more overfitting type of regression?\n",
    "    - You can reduce the number of degrees in the polynomial expression so the curve becomes more linear, and reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "The accuracy metrics used in regression are:\n",
    "- mean absolute error\n",
    "- mean squared error\n",
    "- mean squared log error\n",
    "- r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "What is elastic net regression and how is it related to the regularized regression techniques\n",
    "- Elastic net is the hybrid of lasso and ridge, incorporating both L1 and L2 regularization\n",
    "- Elastic-net is useful when there are multiple features which are correlated. \n",
    "    - Lasso is likely to pick one of these at random, while elastic-net is likely to pick both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "How robust are the general linear models to outliers?\n",
    "- Linear regression, and its related variants lasso and ridge are also sensitive to outliers (basically all OLS estimator based methods)\n",
    "- regression models that use absolute error instead of squared error is marginally better at handling outliers (absolute error or quadratic error based estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 15\n",
    "\n",
    "_Difficulty - Hard_\n",
    "\n",
    "1. Does time series forecasting when you have independant variables as well involve regression? If so what is it called?\n",
    "    - Yes it does. Most time series forecasting contexts have several independant variables besides your time series alone.\n",
    "    - It is called ARIMAX or Regression with ARIMA when you have independant variables (whhich are called regressors)\n",
    "2. Is ARMAX and Regression with ARMA errors the same in time-series forecasting?\n",
    "    - No they are not exactly the same (the final equation however is similar despite the process to derive them being different)\n",
    "    - ARMAX tries to model the regression part and the time series components at the same time\n",
    "    - Regression with ARMA errors models the regression component first and fits the time series components on the residuals\n",
    "3. What perspective does the forecast package in R (the defacto package for time series modelling in R) use?\n",
    "    - The forecast package arima with regressors uses the second perspective (regression with ARMA errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16\n",
    "\n",
    "_Difficulty - Easy_\n",
    "\n",
    "What is the sum of residuals in a simple linear regression? Why?\n",
    "- 0 (zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
