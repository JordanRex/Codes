{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## base functions for xgboost, catboost and lightgbm are to be added\n",
    "## has tuning using bayesian framework\n",
    "## also will follow up with LIME/tree interpreters for the same\n",
    "\n",
    "## sample data used is FLight delays dataset ##\n",
    "\n",
    "# lightgbm done\n",
    "# xgboost needs to be setup\n",
    "# start on catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'ABI_modelling_snippets.ipynb', 'airlines.csv', 'airports.csv', 'flights.csv', 'flights_sample.csv', 'gbm_trials.csv', 'hyperparameter-optimization-master']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## importing the various packages\n",
    "\n",
    "# clear the workspace\n",
    "%reset -f\n",
    "\n",
    "# print list of files in directory\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "# print/display all plots inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# the base packages\n",
    "import collections # for the Counter function\n",
    "import csv # for reading/writing csv files\n",
    "import pandas as pd, numpy as np, time, gc\n",
    "import ast # for the AST segment, parsing json to table and so on\n",
    "\n",
    "# the various packages/modules used across processing (sklearn), modelling (lightgbm) and bayesian optimization (hyperopt, bayes_opt)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, StratifiedShuffleSplit\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from hyperopt import hp, tpe, STATUS_OK, fmin, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "# Evaluation of the model\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define the global variables used later\n",
    "MAX_EVALS = 10 # number of iterations/parameter sets created towards tuning\n",
    "N_FOLDS = 5 # number of cv folds\n",
    "random_seed = 1 # the value for the random state used at various points in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## function to get frequency count of elements in a vector/list\n",
    "def freq_count (input_vector):\n",
    "    return collections.Counter(input_vector)\n",
    "\n",
    "## function to create \n",
    "def prepare_data(input_file_path = 'flights_sample.csv', response = 'ARRIVAL_DELAY'):\n",
    "    train = pd.read_csv(input_file_path)\n",
    "    train = train.sample(frac = 0.1, random_state = random_seed)\n",
    "    train = train[[\"MONTH\",\"DAY\",\"DAY_OF_WEEK\",\"AIRLINE\",\"FLIGHT_NUMBER\",\"DESTINATION_AIRPORT\",\n",
    "                 \"ORIGIN_AIRPORT\",\"AIR_TIME\", \"DEPARTURE_TIME\",\"DISTANCE\",\"ARRIVAL_DELAY\"]]\n",
    "    train.dropna(inplace = True)\n",
    "    print(train.shape)\n",
    "    \n",
    "    categorical_columns = train.select_dtypes(include=['object']).columns\n",
    "    for column in tqdm(categorical_columns):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        train[column] = le.fit_transform(train[column].astype(str))\n",
    "    \n",
    "    train[response] = (train[response] > 25)*1\n",
    "    print(freq_count(train[response]))\n",
    "\n",
    "    y = train[response].values\n",
    "    X = train.drop([response], axis = 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = random_seed)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5707, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 175.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 4997, 1: 710})\n"
     ]
    }
   ],
   "source": [
    "## preparing the different train/test features/labels datasets\n",
    "X_train, X_test, y_train, y_test = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 8,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        learning_rate=0.1, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with default hyperparameters\n",
    "model = lgb.LGBMClassifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline score on the test set is 0.6792.\n",
      "The baseline training time is 0.1197 seconds\n"
     ]
    }
   ],
   "source": [
    "# baseline model with default hyperparameters\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "model.fit(X_train, y_train)\n",
    "train_time = timer() - start\n",
    "\n",
    "predictions = model.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print('The baseline score on the test set is {:.4f}.'.format(auc))\n",
    "print('The baseline training time is {:.4f} seconds'.format(train_time))\n",
    "\n",
    "# Create a lgb dataset\n",
    "train_set = lgb.Dataset(X_train, label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## RANDOM SEARCH TUNING FOR LIGHTGBM ##\n",
    "\n",
    "## start of random tuning module ##\n",
    "\n",
    "import random\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
    "    'num_leaves': list(range(50, 300)),\n",
    "    'learning_rate': list(np.logspace(np.log(0.005), np.log(0.2), base = np.exp(1), num = 1000)),\n",
    "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
    "    'min_child_samples': list(range(20, 500, 5)),\n",
    "    'reg_alpha': list(np.linspace(0, 1)),\n",
    "    'reg_lambda': list(np.linspace(0, 1)),\n",
    "    'colsample_bytree': list(np.linspace(0.6, 1, 10))\n",
    "}\n",
    "\n",
    "# Subsampling (only applicable with 'goss')\n",
    "subsample_dist = list(np.linspace(0.5, 1, 100))\n",
    "\n",
    "# # learning rate distribution\n",
    "# # graph plotted for the same\n",
    "# plt.hist(param_grid['learning_rate'], color = 'r', edgecolor = 'k');\n",
    "# plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);\n",
    "\n",
    "# # number of leaves distribution\n",
    "# # graph plotted for the same\n",
    "# plt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\n",
    "# plt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);\n",
    "\n",
    "# ## sample prediction with random params\n",
    "#####################################################################################################\n",
    "# # Randomly sample parameters for gbm\n",
    "# # sample snippet to return a sample set of parameters\n",
    "# params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "# params['subsample'] = random.sample(subsample_dist, 1)[0] if params['boosting_type'] != 'goss' else 1.0\n",
    "# print(params)\n",
    "\n",
    "# # Perform cross validation with N folds\n",
    "# r = lgb.cv(params, train_set, num_boost_round = 10000, nfold = N_FOLDS, metrics = 'auc', \n",
    "#            early_stopping_rounds = 100, verbose_eval = False, seed = random_seed)\n",
    "\n",
    "# # Highest score\n",
    "# r_best = np.max(r['auc-mean'])\n",
    "\n",
    "# # Standard deviation of best score\n",
    "# r_best_std = r['auc-stdv'][np.argmax(r['auc-mean'])]\n",
    "\n",
    "# print('The maximium ROC AUC on the validation set was {:.5f} with std of {:.5f}.'.format(r_best, r_best_std))\n",
    "# print('The ideal number of iterations was {}.'.format(np.argmax(r['auc-mean']) + 1))\n",
    "######################################################################################################\n",
    "\n",
    "# Dataframe to hold cv results\n",
    "random_results = pd.DataFrame(columns = ['loss', 'params', 'iteration', 'estimators', 'time'],\n",
    "                       index = list(range(MAX_EVALS)))\n",
    "\n",
    "# the objective function that returns a minimization value computed by taking the difference of 1 and roc-auc\n",
    "def random_objective(params, iteration, n_folds = N_FOLDS):\n",
    "    \"\"\"Random search objective function. Takes in hyperparameters\n",
    "       and returns a list of results to be saved.\"\"\"\n",
    "\n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = random_seed)\n",
    "    end = timer()\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "    \n",
    "    # Return list of results\n",
    "    return [loss, params, iteration, n_estimators, end - start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Iterate through the specified number of evaluations\n",
    "for i in range(MAX_EVALS):\n",
    "    \n",
    "    # Randomly sample parameters for gbm\n",
    "    params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}\n",
    "    print(params)\n",
    "    \n",
    "    if params['boosting_type'] == 'goss':\n",
    "        # Cannot subsample with goss\n",
    "        params['subsample'] = 1.0\n",
    "    else:\n",
    "        # Subsample supported for gdbt and dart\n",
    "        params['subsample'] = random.sample(subsample_dist, 1)[0]\n",
    "        \n",
    "    results_list = random_objective(params, i)\n",
    "    # Add results to next row in dataframe\n",
    "    random_results.loc[i, :] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'boosting_type': 'dart', 'num_leaves': 105, 'learning_rate': 0.19489663914444297, 'subsample_for_bin': 140000, 'min_child_samples': 480, 'reg_alpha': 0.02040816326530612, 'reg_lambda': 0.673469387755102, 'colsample_bytree': 0.7333333333333333, 'subsample': 0.98989898989899, 'metric': 'auc', 'verbose': 1}\n",
      "The best model from random search scores 0.6738 on the test data.\n",
      "This was achieved using 4 search iterations.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "      <th>estimators</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303164</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>4</td>\n",
       "      <td>388</td>\n",
       "      <td>4.16774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.304249</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>6</td>\n",
       "      <td>242</td>\n",
       "      <td>0.84975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307988</td>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'dart'...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309646</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>1.11357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311354</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.473828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params iteration  \\\n",
       "0  0.303164  {'class_weight': 'balanced', 'boosting_type': ...         4   \n",
       "1  0.304249  {'class_weight': 'balanced', 'boosting_type': ...         6   \n",
       "2  0.307988  {'class_weight': None, 'boosting_type': 'dart'...         0   \n",
       "3  0.309646  {'class_weight': 'balanced', 'boosting_type': ...         8   \n",
       "4  0.311354  {'class_weight': 'balanced', 'boosting_type': ...         1   \n",
       "\n",
       "  estimators      time  \n",
       "0        388   4.16774  \n",
       "1        242   0.84975  \n",
       "2         14       1.4  \n",
       "3         79   1.11357  \n",
       "4         47  0.473828  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort results by best validation score\n",
    "random_results.sort_values('loss', ascending = True, inplace = True)\n",
    "random_results.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# print the best set of hyper-parameters generated\n",
    "print(random_results.loc[0, 'params'])\n",
    "\n",
    "# Find the best parameters and number of estimators\n",
    "best_random_params = random_results.loc[0, 'params'].copy()\n",
    "best_random_estimators = int(random_results.loc[0, 'estimators'])\n",
    "best_random_model = lgb.LGBMClassifier(n_estimators=best_random_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', **best_random_params, random_state = random_seed)\n",
    "\n",
    "# Fit on the training data\n",
    "best_random_model.fit(X_train, y_train)\n",
    "\n",
    "# Make test predictions\n",
    "predictions = best_random_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print('The best model from random search scores {:.4f} on the test data.'.format(roc_auc_score(y_test, predictions)))\n",
    "print('This was achieved using {} search iterations.'.format(random_results.loc[0, 'iteration']))\n",
    "\n",
    "from IPython.display import display\n",
    "display(random_results.head(5))\n",
    "## the best 5 parameters are displayed below\n",
    "\n",
    "## end of random tuning module ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "## Bayesian Hyperparameter Optimization using Hyperopt For LIGHTGBM ##\n",
    "\n",
    "## start of random tuning module ##\n",
    "\n",
    "## creating the objective function (to return the minimization value)\n",
    "def objective(params, n_folds = N_FOLDS):\n",
    "    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n",
    "    \n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    # Retrieve the subsample if present otherwise set to 1.0\n",
    "    subsample = params['boosting_type'].get('subsample', 1.0)\n",
    "    \n",
    "    # Extract the boosting type\n",
    "    params['boosting_type'] = params['boosting_type']['boosting_type']\n",
    "    params['subsample'] = subsample\n",
    "    \n",
    "    # Make sure parameters that need to be integers are integers\n",
    "    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "        params[parameter_name] = int(params[parameter_name])\n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform n_folds cross validation\n",
    "    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = N_FOLDS, \n",
    "                        early_stopping_rounds = 100, metrics = 'auc', seed = random_seed)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    # Extract the best score\n",
    "    best_score = np.max(cv_results['auc-mean'])\n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1 - best_score\n",
    "    \n",
    "    # Boosting rounds that returned the highest cv score\n",
    "    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n",
    "\n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a', newline='')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'estimators': n_estimators, \n",
    "            'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "## defining the domain space ##\n",
    "# Define the search space (the hyper-parameter space to search for with respect to each parameter)\n",
    "space = {\n",
    "    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n",
    "    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.4, 1)}, \n",
    "                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.4, 1)},\n",
    "                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 500, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 20, 100, 5),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 0.9)\n",
    "}\n",
    "\n",
    "# optimization algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Keep track of results\n",
    "bayes_trials = Trials()\n",
    "\n",
    "# File to save first results\n",
    "out_file = 'gbm_trials.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration', 'estimators', 'train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Global variable\n",
    "global  ITERATION\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "# Run optimization\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, rstate = np.random.RandomState(random_seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "      <th>estimators</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301069</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': 'bal...</td>\n",
       "      <td>7</td>\n",
       "      <td>338</td>\n",
       "      <td>2.334882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.301278</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': 'bal...</td>\n",
       "      <td>1</td>\n",
       "      <td>292</td>\n",
       "      <td>2.265558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.303245</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': 'bal...</td>\n",
       "      <td>10</td>\n",
       "      <td>210</td>\n",
       "      <td>2.032111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.304062</td>\n",
       "      <td>{'boosting_type': 'dart', 'class_weight': 'bal...</td>\n",
       "      <td>9</td>\n",
       "      <td>193</td>\n",
       "      <td>4.221925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.304086</td>\n",
       "      <td>{'boosting_type': 'gbdt', 'class_weight': 'bal...</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>1.056406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params  iteration  \\\n",
       "0  0.301069  {'boosting_type': 'gbdt', 'class_weight': 'bal...          7   \n",
       "1  0.301278  {'boosting_type': 'gbdt', 'class_weight': 'bal...          1   \n",
       "2  0.303245  {'boosting_type': 'gbdt', 'class_weight': 'bal...         10   \n",
       "3  0.304062  {'boosting_type': 'dart', 'class_weight': 'bal...          9   \n",
       "4  0.304086  {'boosting_type': 'gbdt', 'class_weight': 'bal...          4   \n",
       "\n",
       "   estimators  train_time  \n",
       "0         338    2.334882  \n",
       "1         292    2.265558  \n",
       "2         210    2.032111  \n",
       "3         193    4.221925  \n",
       "4          45    1.056406  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort with best scores on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n",
       "        colsample_bytree=0.6914076463501079,\n",
       "        learning_rate=0.0010504530948152394, max_depth=-1, metric='auc',\n",
       "        min_child_samples=70, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=338, n_jobs=-1, num_leaves=123, objective='binary',\n",
       "        random_state=50, reg_alpha=0.5978444063317354,\n",
       "        reg_lambda=0.43145877949439737, silent=True,\n",
       "        subsample=0.7316723211407649, subsample_for_bin=300000,\n",
       "        subsample_freq=0, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from Bayes optimization scores 0.67834 AUC ROC on the test set.\n",
      "This was achieved after 7 search iterations\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(X_test)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(y_test, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training\n",
    "ITERATION = MAX_EVALS + 1\n",
    "\n",
    "# Set more evaluations\n",
    "MAX_EVALS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Use the same trials object to keep training\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, \n",
    "            max_evals = MAX_EVALS, trials = bayes_trials, verbose = 1, rstate = np.random.RandomState(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model from Bayes optimization scores 0.68398 AUC ROC on the test set.\n",
      "This was achieved after 80 search iterations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>params</th>\n",
       "      <th>iteration</th>\n",
       "      <th>estimators</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303164</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>4</td>\n",
       "      <td>388</td>\n",
       "      <td>4.16774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.304249</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>6</td>\n",
       "      <td>242</td>\n",
       "      <td>0.84975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.307988</td>\n",
       "      <td>{'class_weight': None, 'boosting_type': 'dart'...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309646</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>1.11357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311354</td>\n",
       "      <td>{'class_weight': 'balanced', 'boosting_type': ...</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.473828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss                                             params iteration  \\\n",
       "0  0.303164  {'class_weight': 'balanced', 'boosting_type': ...         4   \n",
       "1  0.304249  {'class_weight': 'balanced', 'boosting_type': ...         6   \n",
       "2  0.307988  {'class_weight': None, 'boosting_type': 'dart'...         0   \n",
       "3  0.309646  {'class_weight': 'balanced', 'boosting_type': ...         8   \n",
       "4  0.311354  {'class_weight': 'balanced', 'boosting_type': ...         1   \n",
       "\n",
       "  estimators      time  \n",
       "0        388   4.16774  \n",
       "1        242   0.84975  \n",
       "2         14       1.4  \n",
       "3         79   1.11357  \n",
       "4         47  0.473828  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pd.read_csv('gbm_trials.csv')\n",
    "\n",
    "# Sort values with best on top and reset index for slicing\n",
    "results.sort_values('loss', ascending = True, inplace = True)\n",
    "results.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Extract the ideal number of estimators and hyperparameters\n",
    "best_bayes_estimators = int(results.loc[0, 'estimators'])\n",
    "best_bayes_params = ast.literal_eval(results.loc[0, 'params']).copy()\n",
    "\n",
    "# Re-create the best model and train on the training data\n",
    "best_bayes_model = lgb.LGBMClassifier(n_estimators=best_bayes_estimators, n_jobs = -1, \n",
    "                                       objective = 'binary', random_state = 50, **best_bayes_params)\n",
    "best_bayes_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the testing data \n",
    "preds = best_bayes_model.predict_proba(X_test)[:, 1]\n",
    "print('The best model from Bayes optimization scores {:.5f} AUC ROC on the test set.'.format(roc_auc_score(y_test, preds)))\n",
    "print('This was achieved after {} search iterations'.format(results.loc[0, 'iteration']))\n",
    "\n",
    "from IPython.display import display\n",
    "display(random_results.head(5))\n",
    "## the best 5 parameters are displayed below"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
